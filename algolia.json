[{"categories":["其他技術分享"],"content":"在技術分享的過程中，我發現很多人想要入門練習 VM、或是想要找一個免費的雲端主機來部署程式，但是卻不知道要去哪裡找這種資源。\n而剛好我已經用 Google Cloud 的免費服務很久了，使用體感還滿不錯的！！所以這篇文章就來跟大家分享一下，要如何免費使用 Google Cloud 所提供的 VM 服務，免費部署你所寫的程式。\n目錄 什麼是 Google Cloud 的終身免費方案？ 如何創建免費的 Compute Engine？ 如何登入進 Compute Engine 的 VM 裡？ 如何將程式上傳到此 VM 中？ Google Cloud 的終身免費方案總結 結語 什麼是 Google Cloud 的終身免費方案？ # 其實各大雲端服務廠商如 GCP（Google Cloud Platform）、AWS（Amazon Web Service）、阿里雲\u0026hellip;等等，通常都會提供 12 個月的免費方案，目的是讓你在這一年期間試玩雲端上的服務，進一步轉化成他們的付費用戶。\n不過我們今天要用的不是這個！而是真・永久免費方案，即是 Google Cloud 提供的 Free Tier 方案，其中就有一項是針對 Compute Engine 的規範，提供我們每個月可以使用：\n一個 e2-micro 的 VM（地點必須選擇 us-west1、us-central1、us-east1） 30 GB 的標準硬碟 HDD 對外網路量 1GB 所以簡單來說，只要我們在創建 VM 時，選擇最小的 e2-micro 來創建，並且乖乖的將 VM 創建在美國內部、而且不使用高速的 SSD 硬碟，這樣就可以永久免費享用那台 VM 的服務。\n以我個人的使用經驗來說，我曾經用這台 VM 開發過 LINE Bot、Discord Bot 的程式，除了要連線進去 VM 會有一點卡頓感之外，在程式運作期間的 response time 其實都滿快的！！\n所以如果是要做為 VM 的開發練習、部署 side project 的雲端主機、部署個人網站\u0026hellip;等等，e2-micro 應該都還滿夠用，但如果要做更大型的開發可能就不太適合這樣（但想想也是啦，畢竟是很初階的 VM，能夠使用就要感謝了XD，如果要做大型的開發還是得乖乖付費）。\n如何創建免費的 Compute Engine？ # 補充：經實測，現在要綁定信用卡才能使用 Compute Engine 的服務，如果大家要繼續往下操作的話，會需要先準備好一張信用卡 \u0026amp; 個人資訊，如果暫時不想提供的話，那就先看看這篇文章，了解一下流程就好～。\n了解了 Google Cloud 所提供的免費資源額度之後，接下來我們就實際到 Google Cloud 中，來創建一個免費的 Compute Engine（即是 VM 服務）出來吧！\n首先要先打開 https://console.cloud.google.com/ 進到 Google Cloud 的控制台，接著點擊左上角的「選取專案」，然後選擇「新增專案」。\n接著專案名稱可以隨意填，好了之後就按下「建立」，這樣子就可以在 Google Cloud 中建立一個專案，後續就可以在裡面使用 Compute Engine 的服務創建 VM。\n創建好專案之後，接著要再點擊一次上面的「選取專案」，然後選擇剛剛所創建出來的 Project。\n接著展開左邊的側邊欄，然後選擇「Compute Engine」裡面的「VM 執行個體」。\n此時會打開 Compute Engine API 的啟用畫面，就點擊「啟用」，然後等他跑一下。\n補充：如果你是第一次使用 Google Cloud 的服務的話，這裡就會顯示「必須啟用計費功能」，然後會把你導去綁定信用卡 \u0026amp; 強制啟用 300 美元的試用\u0026hellip;.，我自己是已經有綁過信用卡 + 用過 300 美元的試用，所以現在不會再問我了，但第一次使用可能會需要填滿多資料 + 滿多步驟，這裡大家可以自由斟酌是否要填寫相關的資訊給 Google（不過如果不填寫的話後面也沒辦法做就是了😂）。\n等他跑完之後，理論上大家就會被跳轉到下面這個「VM 執行個體」的頁面，如果沒有被跳轉過來的話，也可以跟前面的步驟一樣，先展開左邊的標籤，然後選擇「Compute Engine」裡面的「VM 執行個體」，就一樣可以進到這個畫面。\n接著點擊上方的「建立執行個體」，就可以開始創建一個 VM。\n然後重點來了！！！接下來的每個步驟都要小心選擇，如果沒有好好選的話，可能就會不小心選用到高價的服務，月底帳單就要哭了🥹。\n首先 instance 的名稱可以隨意填，這就是到時候被創建出來的 VM 的名稱，區域只能選擇「us-west1、us-central1、us-east1」其中一個，建議選擇 us-west1 的地區，這在美國西岸，離台灣比較近，網路延遲會小一點。\n接著往下拉一點，然後將「機型」調整為 e2-micro，這一步非常重要！！！VM 的機型可以說是影響價格最大的關鍵因素，所以一定要選 e2-micro（微型）才對！！\n接著點擊左側的「OS 和儲存空間」，然後選擇「變更」，將我們要使用的硬碟類型變更一下。\n這裡一定要把「開機硬碟類型」改成「標準永久磁碟」（其實就是俗稱的 HDD 傳統硬碟），另外大小雖然 Google Cloud 上限是提供到 30 GB，不過經個人實測其實填 20 GB 就很夠用了。\n所以調整完硬碟之後的最終狀態會是下面這樣（累了嗎？還沒完😂，再努力一下XD）。\n接著再點擊左側的「網路」，然後勾選「允許 HTTP 流量」和「允許 HTTPS 流量」，他會自動在下面的網路標記添加標籤，這部分不用特別動他，讓他自動添加即可。\n接著往下拉一點，然後點擊「網路介面」中的「default」。\n並且在「網路服務級別」的地方改成勾選「標準級（us-west1）」，改好之後記得按完成。\n這樣子就全部設定好了！！所以就可以按下建立，創建出這個 VM 出來了！！（此時在右邊的預估每月費用中，費用會顯示 6.91 美元左右，這個不用怕，他到時在月底帳單會自己折抵，所以月底帳單收到的費用會是 0 元，如果後續有發現被超收的情況的話，一定要趕快找 Google 申訴！）\n如何登入進 Compute Engine 的 VM 裡？ # 等他跑一段時間創建好 VM 之後，這時候就可以看到該 VM 的內部 IP、外部 IP，以及其他的相關資訊。\n如果想要登入進那台 VM 玩耍的話，只要點擊右邊的 SSH，這時 Google Cloud 就會開啟一個新的視窗，創建一個新的 SSH 連線給我們。\n因此只要在新視窗中點擊「Authorize」：\n就可以直接透過此視窗開啟 SSH 連線，登入到 VM 內部了！因此後續大家就可以在這個 VM 裡面執行任何你想要練習的指令、或是運行你的程式了！（在此視窗中輸入指令時會有一個微妙的卡頓感，這是正常的現象，因為此 VM 建立在美國西岸，所以多多少少會有一點網路延遲）\n如何將程式上傳到此 VM 中？ # 雖然在這個 SSH 的視窗中有提供「上傳檔案」的功能，不過我通常都是會先把程式上傳到 GitHub（會順便將該 repo 設定成 private 不公開），接著在此 VM 中直接透過 Git 來下載，這樣子不僅所有改動都可以儲存在 GitHub 上，而且也不用去研究檔案傳輸的問題，個人覺得非常讚！\n至於在運行程式時，可能會遇到某某套件不存在的情況，這就是要靠自己去 debug 以及安裝相關的環境了。\n其實我還是滿推薦大家可以實際的創一台 VM 來玩的啦，只有當你真的靠自己去完成某些環境的","date":"2025-06-03","objectID":"79e74e49cb7f0c5690d9604038fc2553","title":"終身免費的 VM 服務！Google Cloud 免費方案分享","url":"https://kucw.io/blog/gcp-free-tier/"},{"categories":["Spring Boot"],"content":"本文介紹要如何使用 Dockerfile 的 Multi-stage build 寫法，為 Spring Boot 生成更精簡的 Docker image。\n目錄 什麼是 Multi-stage build？ 在 Spring Boot 中使用 Multi-stage build 來生成 image 如何知道要把 src 複製到 /root/src 底下？ 結語 什麼是 Multi-stage build？ # 在生成 docker image 時，通常會透過撰寫 Dockerfile 來生成，而 Dockerfile 有兩種寫法可以用，一種就是用基本的 Dockerfile 寫法，另一種則是更進階的 Multi-stage build 寫法。\n針對這種需要編譯的程式語言（ex: Java、C++），用 Multi-stage build 會更好，因為 Multi-stage build 的概念是「依序 build 多個 image，但是只取最終的 image 來生成」。\n所以就可以先在前面的 Build stage 中先編譯 Spring Boot 檔案，此時在這個 image 裡面會有許多跟「執行」無關的檔案，像是 src、pom.xml、target 資料夾中的一堆編譯檔\u0026hellip;等等，這些檔案其實跟執行無關，跟執行真正有關的就只有 .jar 檔而已。\n而在 Build stage 執行完之後，就可以馬上把這個 .jar 檔拉到 Package stage 裡面，並且最終只生成 Package stage 的 image 出來，這樣子中間那些編譯檔就不用也被包進 image 裡面，徒增 image 大小了，讚！！\n另外補充一下，其實 Build、Package 這些名稱完全就是自己想叫什麼就叫什麼，沒有任何規範，只是一個概念而已，實際上可以有多個 stage，並且每個 stage 的名字可以自由取名。\n在 Spring Boot 中使用 Multi-stage build 來生成 image # 要在 Spring Boot 使用 Dockerfile 來 build image 的話，首先要先在 Spring Boot 的資料夾底下創建一個 Dockerfile （注意是要放在和 src 平行的層級，不是放在 src 裡面）。\n然後就可以在 Dockerfile 中撰寫 Multi-stage build 的程式：\n# -- build stage -- # 在 build 階段可以挑選 maven:3.9.0-eclipse-temurin-17 當作基底 image，他裡面已經預裝好 Maven 和 Java 17，讚！（所以挑 image 真的也是一門藝術，挑到對的 image 就可以省去很多自己安裝的步驟） # 記得最後面一定要加 AS xxx，這樣才能使用 Multi-stage build 的功能 FROM maven:3.9.0-eclipse-temurin-17 AS build # 拉取必要的 source code 和 pom.xml 進到 image 裡面（即是把 src 底下的所有程式複製到 image 的 /root/src 中，把 pom.xml 複製到 image 的 /root 裡） # 至於如何知道要放在 /root 下而不是放在 /usr 下，下面會講 COPY src /root/src COPY pom.xml /root # cd 到 /root 資料夾裡面，並且執行 mvn build Spring Boot WORKDIR /root RUN mvn clean package -Dmaven.test.skip=true # -- package stage -- # 因為在執行階段其實只需要 jre 就好，不需要整個 jdk 都拉進來，所以這裡採用的是 eclipse-temurin:17-jre，可以讓 image size 變得更小，選 image 的藝術 again FROM eclipse-temurin:17-jre # 偷 build 階段生成好的 .jar 檔進到此 image 裡面，所以就是複製 build 階段的 /root/target/*.jar 過來，並且將他改名成 app.jar COPY --from=build /root/target/*.jar app.jar # 聲明要開啟 8080 port EXPOSE 8080 # 指定運行此 package image 的指令 CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] 寫好之後就可以在 Spring Boot 中開啟 terminal，然後執行 docker build -t my/springboot:1.0.0 .，表示要運行當前的 Dockerfile，生成此 Spring Boot 程式的 image 出來（注意此處只會生成最終的 package image 出來，前面的 build image 就會被丟掉了）。\n此時就會在本機中生成一個 my/springboot:1.0.0 的 image 出來，接著只要執行 docker run -p 8080:8080 my/springboot:1.0.0，就可以用 docker 執行 Spring Boot 程式了，讚！\n如何知道要把 src 複製到 /root/src 底下？ # 在撰寫 Dockefile image 時，最常見的困擾就是「不知道要把檔案複製到哪裡去」。\n舉例來說，像是在上面的 build 階段時，就必須要把 Spring Boot 的 src 複製到 build image 中的 /root/src 底下，然後要把 pom.xml 複製到 /root 底下：\n# 使用 maven:3.9.0-eclipse-temurin-17 當做基底 image FROM maven:3.9.0-eclipse-temurin-17 # 複製 src 底下的程式到 image 的 /root/src 中，複製 pom.xml 複製到 image 的 /root 中 COPY src /root/src COPY pom.xml /root 之所以會知道要複製到這些位置，除了靠經驗傳承之外，其實也可以透過爬 Docker Hub 上的 image 介紹得知。\n以 maven:eclipse-temurin 為例，在 maven image 中的第 27、28 行就有定義 ARG USER_HOME_DIR=/root 和 ENV MAVEN_CONFIG=/root/.m2，所以其實這個 image 預設的 user home 就是 /root，而不是 /usr，因此才會把 src 程式複製到 /root 底下。\n所以如果想要客製化自己的 settings.xml 的話，那就要把 setting.xml 複製到 /root/.m2/setting.xml 的位置，這樣才會真的生效！\n因此在用任何 image 時，建議也可以來 Docker Hub 中查看 image 的介紹，可以更透徹的了解這個 image 中的系統結構。\n結語 # 這篇文章我們介紹了如何使用 Dockerfile 去生成 Spring Boot 的 image，並且採用了 Multi-image build 的技術，減少最終生成的 image 大小。\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端","date":"2025-05-22","objectID":"afdc317a4bd1909d84c3d0eefa7e58b4","title":"為 Spring Boot 生成 Docker image（Multi-stage build）","url":"https://kucw.io/blog/springboot-docker-image/"},{"categories":["其他技術分享"],"content":"初入門 Docker 時，可能會對 Docker 中有哪些指令感到眼花撩亂，這篇文章會介紹如何運行你的第一個 Docker 程式，並且介紹 Docker 的常用指令有哪些。\n如果只想查看 Docker 常用指令的介紹，可以直接點擊 Docker 常用指令 跳轉到下方。\n目錄 Docker 中的 Image、Container、Registry 的用途 運行第一個 Docker Container 安裝 Docker Desktop 下載 Docker Image Docker 常用指令介紹 pull 指令（下載 image） run 指令（創建並運行 container） start/stop 指令（運行/結束 container） ps -a 指令（查看所有 container 的運行狀態） exec 指令（對正在運行的 container 下達指令） logs 指令（查看該 container 輸出的 log） inspect 指令（查看該 container 的詳細資訊） rm 指令、rmi 指令、images 指令 Docker 常用指令總結 結語 Docker 中的 Image、Container、Registry 的用途 # 在 Docker 中，Image、Container、Registry 是很常出現的三個名詞：\n1. Image（映像檔）\n大家可以把 image 想像成是任何一段程式（或是一個 .jar 檔），所以當我們作為 Developer 寫完程式之後，就可以將這段程式封存成一個 image，並且打上一個 tag 的編號 x.x.x（這個效果就等同於 Maven 的版號）。\n2. Container（容器）\nContainer 的概念類似於「買一台新電腦，然後在裡面運行 image 裡的程式」，在 Docker 中可以運行許多個 Container，每一個 Container 都是一個獨立的環境。\n3. Registry（倉庫）\n可以把 Registry 想像成一個雲端倉庫，裡面會存放所有 image 們（概念等同於 Maven 需要有一個地方存放 .jar 檔），所以不管是要 push image 上去、還是要 pull image 下來，都是在跟 Registry 倉庫溝通。\nDocker 官方的 Registry 叫做 Docker Hub，預設就是會從這個地方下載 image 下來，或是公司內部也很常見會建置私人的 Registry，存放公司內部的 image 們。\n運行第一個 Docker Container # 安裝 Docker Desktop # 要在電腦上運行 Docker 的話，首先要先安裝 Docker Desktop 這個軟體，因此大家可以先進到 Docker 官網，然後根據自己的電腦系統下載對應的 Docker Desktop（建議大家也可以順便辦一下 Docker 的帳號，等一下可以登入進 Docker Desktop 中）。\n下載好 Docker Desktop 之後，接著就可以在 terminal 中輸入指令，從 Registry（倉庫）中拉取想要的 image 下來，並且使用該 image 生成 container 來運行。\n補充：安裝好 Docker Desktop 之後，記得要執行他才可以，他一定要在背景運行著，這樣等等我們輸入的指令才會生效。\n所以如果等等在執行下面的 docker 指令時看到 Cannot connect to the Docker daemon at unix:///Users/kujudy/.docker/run/docker.sock. Is the docker daemon running? 的錯誤，就表示你忘記執行 Docker Desktop 軟體了。\n下載 Docker Image # 在 Docker 官方的 Registry Docker Hub 中，裡面提供了各式各樣的 image 供大家下載。\n舉例來說，在 Docker Hub 中有一個 image 叫做 hello-world（沒錯就叫這個名字），此時點擊他右邊的 Copy 就可以複製拉取這個 image 的指令。\n只要在 terminal 中貼上這段指令（或是自己手動輸入 docker pull hello-world），就可以下載 hello-world 這個 image 下來。\n此時如果回到剛剛安裝的 Docker Desktop 查看一下的話，就可以看到在 Images 中出現了 hello-world 這個 image 了，這就表示我們下載了 hello-world 這個 image 映像檔到我們的電腦中。\n而要運行這個 image 的話，只需要在 terminal 中再輸入 docker run hello-world，就可以用 hello-world image 去生成一個 container 出來，並且 Docker 就會去執行這個 container 中的程式，因此最終就會輸出如下的資訊在 console 上。\n這個 image 比較簡單，就只是輸出文字在 console 上而已，其他的 image 有各式各樣的功能可以使用，基本上熱門的服務如 nginx、redis…等，都有 image 可以用。\n順便一提，此時如果回到 Docker Desktop 上查看的話，就可以看到在 Containers 區中多了一個容器出來，他上面就會寫這個 container 的 id 是多少（即是 f10ff7cdfb54）、以及這個 container 是從哪個 image 生成出來的（即是 hello-world image）。\n所以其實安裝了 Docker Desktop 之後，我們就能透過 Docker Desktop 的好用介面，一目瞭然現在到底下載了哪些 image、以及有哪些 container 是正在運行著的，算是方便我們管理這樣（不過這些功能也完全可以用 terminal 指令來查看，就看大家喜歡哪一種）。\nDocker 常用指令介紹 # 成功運行起第一個 Docker 程式之後，接下來也來介紹一下 Docker 中常用的指令有哪些。\npull 指令（下載 image） # docker pull [image name]:[image tag]：從 Registry 中下載 image 到本機電腦。\n如果想要從遠端倉庫中下載某個 image 時，就要用 docker pull 來下載，像是 docker pull nginx 就可以下載 nginx 這個 image 下來。\n另外在下載 image 時，也可以特別指定要下載的 image 是哪個版本（版本在 docker 中稱為 tag）。\n舉例來說，在下載 nginx 時就可以輸入 docker pull nginx:1.28，表示要下載 tag 為 1.28 的 nginx image 下來，這裡的 1.28 就等同於是版本號。\n如果沒有指定版本的話，預設就是下載 latest 這個 tag 下來（表示當前的最新版），所以 docker pull nginx 和 docker pull nginx:latest 是完全一樣的。\n# 這兩個下載的 image 一模一樣 docker pull nginx docker pull nginx:latest 不過在實務上，建議大家在下載和使用 image 時，一定要特別指定要使用的是哪個 tag，不要用無腦用 latest 最新版，這樣很容易會遇到版本升級導致的問題…這是特別要注意的地方！！\nrun 指","date":"2025-05-20","objectID":"4e6342b9b8802362f355ae31117b400c","title":"Docker 常用指令介紹 + 第一個 Docker 程式","url":"https://kucw.io/blog/docker-command/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，這篇文章是每月一次的自媒體月報，主要分享我經營《古古的後端筆記》個人品牌的幕後秘辛，如果你也對於「自媒體創業」有興趣的話，歡迎繼續閱讀本文～\n補充：如果想了解《古古的後端筆記》電子報的起源，也可以先查看創刊號的文章。\n2025.3～4 月粉絲追蹤數、電子報訂閱人數 # 這兩個月的粉絲成長人數如下：\n2025/2/25 人數 2025/4/29 人數 3 月份總成長人數 4 月份總成長人數 Facebook 粉專追蹤數 4607 4674 +37 +30 電子報訂閱人數 3079 3348 +155 +114 Threads 粉絲追蹤數 8135 8732 +391 +206 IG 粉絲追蹤數 688 747 +34 +25 本月撰寫的電子報主題、文章 # 這邊記錄了我這兩個月撰寫了哪些電子報和文章，大家如果對於其中的內容有興趣的話，也可以前往查看：\nCron 是什麼？定時任務的語法怎麼寫？ CDN 是什麼？一次搞懂 CDN 的用途和三大好處 Elasticsearch 是什麼？認識地表最強的全文搜尋工具！ Elasticsearch 進階用法，如何透過 function_score 自定義排序結果？ 近況更新 # 雖然自媒體月報已經有點快被我拿來當成個人生活的抒發😂，但是還是有一個消息想跟大家更新一下，就是我已經找到工作啦～\n其實更準確的說是我已經入職一陣子了，只是想說剛好趁著月報的機會和大家更新一下近況這樣，很感謝大家一路以來的關心，不管是 FB 私訊還是電子報的回信我都有收到，也特別感謝中間有幫忙牽線分享面試機會的古粉們🥹（揪甘心），一路上遇到很多貴人相助，心中只有萬分感謝😭（但拜偷別私訊我問我在哪上班，暫時不方便透露🥹）。\n是說我本來有打算和大家分享這次找工作被問到的問題有哪些，但是考慮到各種因素最後還是作罷（怕牽扯到法律問題的話會有點麻煩，所以只能跟大家說聲抱歉了\u0026hellip;.）。\n然後也跟大家分享一下這份新工作吸引我的地方在哪吧！這份新工作的內容其實會有點偏 DevOps（還沒看 CI/CD 那篇電子報的快去看🤣），有很多機會可以碰 Docker + Kubernetes 的場合，覺得很有挑戰性，是以前完全沒有機會碰到的領域！而且 DevOps 的角色定義也很吸引我，可以當 Developer 和 Operations 的橋樑感覺很酷XD。\n另外我同時也在想，如果我多懂一點 Operations 的東西，將來是不是就可以將一些簡單常用的技巧分享給電子報的大家（也就是 Developer），看能不能提供一些「站在 Developer 角度必學的 Operations 知識」，讓 Developer 和 Operations 之間的隔閡不要再那麼大之類的？不知道XD，可能還需要更多時間來思考一下這個方向。\n總之，這份後端電子報仍舊會持續下去的！！未來的內容主要還是會圍繞在後端的主題上，頂多偶爾會講一點點 Operations 的東西而已，不會真的寫很硬的 Operations 知識，我還是希望能藉由這份電子報，和大家一起精進系統設計的相關技術💪。\n新的一年挑戰新的事物，Sharing is Learning，無限進步🚀！！\n2025.3～4 月報總結 # 以上就是這兩個月的自媒體月報了！如果有什麼想了解的也歡迎隨時回信給我，每一封信我都會看的，那我們就下個月再見啦！\n","date":"2025-05-06","objectID":"04e535d2763e740cac552d4e667169dc","title":"軟體工程師的自媒體之路 - 2025.3～4 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202503-04/"},{"categories":["其他技術分享"],"content":"大家在工作上可能常常會聽到 CI/CD、持續集成、持續部署\u0026hellip;等等的名詞，但是卻不清楚這些名詞背後所代表的意義是什麼，因此這篇文章我們就來介紹一下，到底什麼是 CI/CD，以及他們之間的區別吧！\n目錄 什麼是 CI/CD？ CI（持續集成） CD（持續交付） 補充：Continuous Delivery 和 Continuous Deployment 的差別 CI/CD 總結 補充：什麼是 DevOps？ 結語 什麼是 CI/CD？ # 所謂的 CI/CD，他其實要拆分成兩個名詞來看，分別是 CI 和 CD：\nCI（Continuous Integration，持續集成） CD（Continuous Delivery，持續交付） 所以雖然 CI 和 CD 長得很像，但是他們實際上所負責的是完全不同的流程！所以接下來我們就分別來看一下，CI 到底是負責什麼部分、而 CD 又是負責哪些部分。\nCI（持續集成） # 所謂的 CI，就是指「計畫需求、實作程式、建置程式、測試程式」這四個步驟的統稱，而 CI 的終極目標，就是要讓這四件事可以被「自動化」執行，重點在「自動化」。\n舉例來說，不管你是前端工程師、後端工程師、還是 Android/iOS 工程師，只要你的工作內容是「寫程式完成客戶的需求」（客戶可以是企業用戶，也可以是一般使用者），那麼你就是屬於開發者（Developer），而 Developer 的日常任務，其實就是大家常常在做的釐清需求、寫程式、測試程式而已。\n所以 CI 並不是一個什麼新的流程，他只是希望可以把 Developer 日常的工作整合在一起，讓大家在「釐清需求 → 寫程式 → build code → 測試程式」這段路可以做得更順暢一點而已。\n因此在 CI 的流程中：\n首先會先從 plan（計畫）開始，也就是先釐清需求為何 釐清好需求之後，接著就進到 code 的環節，開始去實作程式 等到程式實作完之後，這時候就可以進到 build 環節來 build code（建置程式），通常前端會用 webpack 來 build code、後端則是用 Maven 和或是 Gradle 來 build build 完程式之後就會進到 test 環節，此時就可以運行單元測試、或是運行自動化測試，確保我們所 build 出來的 code 是能正常運行的 因此大家也可以觀察一下，當你 push code 到你們公司的 GitHub、GitLab、BitBucket 等…雲端時，常常就會觸發一些 job 去 build code、去執行單元測試，而這些自動化執行的 job，其實都是 CI 的一環！！\n也因為有了這些自動化 build code、自動化執行單元測試的 job，所以我們作為 Developer 不僅可以省去反覆手動操作的時間，也可以確保我們所撰寫的程式是已經被驗證過的，不會 merge 一份有問題的程式回 master branch。\n所以到這裡，CI 的任務就完成了，因此簡單來說，CI 只要好好守護 Developer 們能夠把程式 build 完、把測試程式 run 完，CI 就功成身退了，接下來就是要進到 CD 的戰場了！\nCD（持續交付） # 在我們進入 CD 的介紹前，大家可以先回想一下，你作為 Developer（前後端均可），是否還記得當你的 Pull Request 被 merge 之後，到底發生了什麼事嗎？如果你沒辦法很明確的說出後續每一個步驟在幹嘛，其實是非常正常的，因為「部署程式」這件事情，在職責上確實不歸 Developer 管。\n一般在分工比較細的大公司來說，會將工程師區分成兩種人：Developer（開發工程師）和 Operations（維運工程師），其中 Developer 就是負責寫程式，達成 PM 的需求，而 Operations 則是負責部署程式，將 Developer 寫的程式部署到 server 上，讓使用者可以真的用到這個程式。\n所以所謂的 CD，就是指 Operations 中的「發佈程式、部署程式、維運程式、監控程式運行狀態」這四個步驟的統稱，而 CD 的終極目標，一樣是要讓這四件事可以被「自動化」運行（這裡的自動化有一點微妙的差別，等等會介紹）。\n因此在 CD 的流程中：\n首先會從 release 開始，也就是當 Developer 的程式被 merge 之後，維運工程師就可以開始來處理發版的部分 發佈完成之後，接著就會進到 deploy 的環節，將這份程式部署到 server 上 deploy 完成之後會進到 operate 環節，管理程式是否有正常運行 最終則是 monitor 環節，持續監控程式的運行狀態，時時刻刻確保服務在線，不會中斷 所以在 CD 的流程中，可以看到幾乎全部都是在做部署程式、維護 server 的相關操作，而這裡擴展下去就可以講非常非常廣了。\n舉例來說，如果今天 server 選擇部署在 AWS 上，那麼如何更好的管理和運用 AWS 上的服務，就是 CD 要處理的部分，如果今天換成使用另一個雲端服務 GCP，那麼 CD 又會需要跟著改。\n又或是說，假設今天是使用 Docker + Kubernetes 來部署程式，那麼如何打包 image、如何設定環境變數、如何使用 Kubernetes 調度 pod…等等，這些也是 CD 要處理的部分。\n因此 CD 在玩法上可以說是更加的多樣化，追求的已經不僅僅是「把程式弄到 server 上去 run」這麼簡單而已，而是要追求「又快、又好擴展、又安全、又省錢」這種巔峰造極的境界了。\n也因為如此，近幾年的部署流程其實也是各種飛速發展，其中最熱門的大概就是 Docker + Kubernetes 這類的容器化部署了，大家如果對這方面有興趣的話，可以參考我之前寫的 Docker 介紹，或是搜尋網路上各位大大們的 Docker 教學。\n反正總而言之，CD 的任務就是要守護 Operations（維運工程師），讓維運工程師可以用最有效率的方式去部署和管理 server，同時也努力幫老闆省下最多的成本就對了！\n補充：Continuous Delivery 和 Continuous Deployment 的差別 # 了解了 CD 的概念之後，這裡也補充一下前面提到的「有點微妙的自動化」的部分。\n在 CD 中，又可以細分成 Continuous Delivery（持續交付）和 Continuous Deployment（持續部署），他們兩個之間的差別就在於「部署到 production 正式環境這一步，是否需要人工手動操作」。\nContinuous Delivery（持續交付）：部署到 production 環境需要人工手動操作 Continuous Deployment（持續部署）：全自動化，不需人工介入 像是在前面的 CI 和 CD 的介紹中，一直有提到 CI 和 CD 的終極目標是「自動化」執行，但是會不會自動化過了頭，反而導致我們所交付的程式有問題呢？\n舉例來說，假設當你所寫的程式被 merge 回 master branch 之後，假設就這樣自動化一路部署到 production 環境，中間完全沒有任何人再對他進行壓力測試、或是沒有進行服務的整合測試，可能就會出現意料之外的問題。\n還有一個問題是，有時候功能的發佈是會需要搭配商業策略的，譬如說 iOS 的新功能常常會在 iPhone 發表會上宣布，這就是一種為了商業策略而延遲發版的行為。\n因此一般在實務上，Continuous Delivery（持續交付）會是比較常見的做法（即是部署到 production","date":"2025-05-05","objectID":"6dbbf7f0cd01c961638d03e145905fc2","title":"CI/CD 是什麼？他們之間的差別在哪裡？","url":"https://kucw.io/blog/cicd-intro/"},{"categories":["Elastic Search"],"content":"Elasticsearch 除了可以用來實作一般的全文搜尋之外，如果想要實作「推薦排序」，針對多個維度綜合比較，Elasticsearch 也是支援這類的用法的！\n因此這篇文章我們就來介紹一下，要如何透過 Elasticsearch 中的 function_score，來實作自定義排序的結果吧！\n補充：如果對 Elasticsearch 不太熟悉，也可以先回頭參考 Elasticsearch 的基本介紹 Elasticsearch 是什麼？認識地表最強的全文搜尋工具！。\n目錄 回顧：什麼是 Elasticsearch？ 例子：美食地圖實作 Elasticsearch 中的推薦排序實作（使用 function_score） Elasticsearch 中的 function_score 總結 結語 回顧：什麼是 Elasticsearch？ # 所謂的 Elasticsearch，他是一個強大的「全文搜尋」（Full-Text Search）工具，讓我們可以從大量的數據中，快速找到想要的資料在哪裡。\n也因為 Elasticsearch 的專長是「全文搜尋」，所以一般通常會將他用來「架設搜尋引擎」，因此像是旅遊攻略、評論、社群貼文、電商產品….等等諸如此類的查詢，背後都很適合使用 Elasticsearch 來實作！\n不過，當大家將 Elasticsearch 應用在旅遊攻略、評論、社群貼文…等等的功能實作時，這時候就會遇到一個非常重要的問題，那就是「什麼樣內容的文章應該排在最前面？」，因此「如何調整文章的排序結果」，這就是這週我們要來探討的主題了！\n例子：美食地圖實作 # 想像一下，假設現在你要實作一個美食地圖的查詢功能，在這個美食地圖中，要提供使用者「距離搜尋」、「關鍵字搜尋」、「依照評價排序」…等相關功能，具體大家可以想像成 Google Map 中的搜尋餐廳或是 Uber Eats 中的尋找餐廳的類似功能。\n假設現在使用者查詢了「小籠包」這個關鍵字，並且想要根據「評價由大到小」來排序的話，那這個在 Elasticsearch 中很好實作，就是直接使用 order 來排序就好（邏輯類似於 SQL 中的 ORDER BY）。\n又或是使用者查詢了「小籠包」這個關鍵字，並且想要查詢他「方圓 2 公里以內」的所有小籠包餐廳，那這個在 Elasticsearch 中也很好實作，即是使用 geo_point 格式來查詢即可（詳細用法可以參考 Elasticsearch 官網，這裡先不詳細展開介紹）。\n但是！！問題來了！！！假設今天使用者選擇了「推薦排序」，那麼作為後端的我們，到底應該是把「評價較優」的店家放在前面，還是要把「距離較近」的店家放在前面呢？這真的是個好問題對吧，畢竟誰都想要排在比較前面的位置，這樣子曝光度會比較高，生意也會比其他店家更好。\n所以這時 Elasticsearch 就開始思考，有沒有辦法提供一個「多維度的評分機制」，讓我們可以根據多個維度，來決定這個店家的總分為何呢？ 答案是有的！我們可以透過 Elasticsearch 中的 function_score，對多個維度的值進行權重加總，最終計算出每一個店家的總分，因此就可以實作出「推薦排序」的功能了！\nElasticsearch 中的推薦排序實作（使用 function_score） # 補充：以下內容較為複雜，建議大家對 function_score 有個概念即可，等到後續真正用到時再回來查相關實作細節。\n在 Elasticsearch 中，有一個特別的地方，就是他會針對「每一個文件給出一個 _score 的分數」，這個 _score 就是指「該文件的匹配程度」，因此簡單來說，_score 的值越高的文件，他在搜尋的結果就會被排在越前面，這個就是 Elasticsearch 的運作機制。\n所以如果我們想要實作一個「多維度的評分機制」，根據店家距離、評價數、價格…等等的多個維度去決定一個店家的總分，那麼就是要透過 function_score 的用法，手動的去改變 Elasticsearch 的 _score 的計算方式，這樣子最終的排序結果，才會是我們期望的「推薦排序」。\n舉例來說，一個基本的 function_score 模板如下圖所示：\nGET /_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { // 主查詢，查詢完後這裡自己會有一個評分，就是 old_score \u0026#34;query\u0026#34;: {.....}, // 這裡可以寫上多個加強函數 // 每一個加強函數會產生一個 boost_score（加強 score），因此有多個 functions 就會有多個 boost_score \u0026#34;functions\u0026#34;: [ { \u0026#34;field_value_factor\u0026#34;: ... }, { \u0026#34;gauss\u0026#34;: ... }, { \u0026#34;filter\u0026#34;: {...}, \u0026#34;weight\u0026#34;: ... } ], // 決定多個 boost_score 們要怎麼合併成一個 total_boost_score \u0026#34;score_mode\u0026#34;: \u0026#34;sum\u0026#34;, // 決定 total_boost_score 怎麼和原始的 old_score 合併 \u0026#34;boost_mode\u0026#34;: \u0026#34;sum\u0026#34; } } } 如果拆解一下這個模板的話，基本上就是在 functions 的地方，可以去添加各種不同維度的評分，最終再透過 score_mode 和 boost_mode，將這些不同維度的評分給加總（或是相乘）起來，最終得到每一個店家的總分。\n所以以上面的美食地圖的功能為例，我們就可以在主 query 中寫上「小籠包」的查詢，並且在 functions 裡面，寫上我們想要根據「評價數」和「距離」分別去評分，最終再透過 score_mode 和 boost_mode，將這些分數全部加總起來，就是這個店家的最終得分 _score 了。\n因此透過 function_score 的用法，我們就可以成功的在 Elasticsearch 中實作出「推薦排序」的功能，進而去針對多個維度，去進行綜合性的比較了！\n補充：如果大家想了解更多 function_score 的具體實作，也可以參考我之前寫的 function_score 系列文章。\nElasticsearch 中的 function_score 總結 # 所以總結上述的介紹的話，如果將來我們想要在 Elasticsearch 中實作「推薦排序」這類較為複雜的排序，需要同時針對多個維度的方向來排序時，那麼就可以透過 function_score 的用法，手動的改變 Elasticsearch 中的 _score 的評分機制，最終得到我們想要的排序結果。\n並且因為 Elasticsearch 的 function_score 是可以一直往上加的，所以不管是要一個維度、還是要 N 個維度，都可以根據當下的需求來擴展，因此擴展性可以說是非常的高！\n不過，也因為 function_score 改變的是「排序的結果」，所以這個其實沒有絕對的對錯，畢竟青菜蘿蔔各有所好，你認為好吃的店家，其他人可能不覺得好吃，所以要如何調出一個「完美的 function_score 參數」，反而才是最困難的部分。\n另外也隨著 AI 的興起，使得「推薦排序」的實作不一定要透過 Elasticsearch 來實作，而是可以考慮改成 AI 的演算法來實時運算","date":"2025-04-22","objectID":"25bf5783d168cf20c0d27ae570169ead","title":"Elasticsearch 進階用法，如何透過 function_score 自定義排序結果？","url":"https://kucw.io/blog/elasticsearch-function-score-intro/"},{"categories":["Elastic Search"],"content":"Elasticsearch 可以說是長年霸佔「全文搜尋」排行榜的第一名，不管是在使用者的產品開發上、還是在內部的 log 系統中，也都常常能看見 Elasticsearch 的身影，因此這篇文章我們就來介紹一下，Elasticsearch 到底是什麼吧！\n目錄 什麼是 Elasticsearch？ 在以前的 SQL 資料庫時代 使用 Elasticsearch 之後 什麼是反向索引（Inverted Index）？ Elasticsearch 介紹 Elasticsearch 總結 補充：反向索引的實戰經驗分享 結語 什麼是 Elasticsearch？ # 首先 Elasticsearch 其實是由兩個單字 Elastic 和 Search 所組成，因此念法上就會唸成 Elastic Search，只不過官方的產品名稱是全部縮寫在一起而已。\n而 Elasticsearch 本身是一個強大的「全文搜尋」（Full-Text Search）工具，也就是一個「能夠讓你自己架設搜尋引擎」的工具。所以簡單的說，Elasticsearch 的目標就是「讓使用者能夠快速的從一堆數據中，找到他想要的資料在哪裡」，也就是搜尋引擎最重要的作用了。\n不過在我們開始介紹 Elasticsearch 到底是強在哪裡之前，我們可以先回頭來看一下，在 Elasticsearch 還沒有誕生之前，我們是如何使用 SQL 資料庫來實作「查詢」的功能的。\n在以前的 SQL 資料庫時代 # 想像一下，假設現在你要開發一個「評論」的網站，因此在你的資料庫中，就會有一張 review 的 table，裡面記錄了每一筆評論的詳細內容：\nreview_id review_text（評論內容） 1 I jumped out the brown fox 2 hello how are you 那麼問題來了，假設現在使用者輸入了 brown，要你查詢出帶有 brown 的評論，這時候你要怎麼實作？\n最直覺的做法，就是使用「SQL 的模糊搜尋 LIKE %」，從這些評論中找出到底哪些評論中包含 brown 這個單字，因此通常就會寫出類似於下方的 SQL 語法，去資料庫中查詢所有評論。\nSELECT * FROM review WHERE review_text LIKE \u0026#34;%brown%\u0026#34; 雖然上述的作法能夠查詢出帶有 brown 單字的評論，但是這個查詢的效率實在是太差了！因為模糊搜尋沒辦法使用 index 來加速查詢效率，只能呆呆的一筆一筆數據去比對，因此縱使這個方法勉勉強強能夠解決查詢的問題，但是卻解的不夠漂亮。\n所以為了更好的解決這個問題，Elasticsearch 就被發明出來了！\n使用 Elasticsearch 之後 # 在 Elasticsearch 中，一樣是會先對每一筆 review 評論建立一筆數據（在 Elasticsearch 中稱為「文件」），並且因為 Elasticsearch 是屬於 NoSQL 資料庫，所以會使用 JSON 格式來儲存該文件。\n[ { \u0026#34;review_id\u0026#34;: 1, \u0026#34;review_text\u0026#34;: \u0026#34;I jumped out the brown fox\u0026#34; }, { \u0026#34;review_id\u0026#34;: 2, \u0026#34;review_text\u0026#34;: \u0026#34;hello how are you\u0026#34; } ] 其實到這邊其實都跟上面的 SQL 資料庫一模一樣，只是 Elasticsearch 換個方式用 JSON 儲存而已，到目前為止沒有任何的差別。\n不過！！重點來了！！Elasticsearch 之所以能夠快速的查詢出「帶有 brown」的評論，就是因為 Elasticsearch 在插入一筆文件時，會預先對該文件建立「反向索引」（Inverted Index），因此之後就可以借助「反向索引」的力量，快速的找出相關評論。\n什麼是反向索引（Inverted Index）？ # 舉例來說，當 Elasticsearch 插入一筆數據 hello how are you 時，他除了正常的插入一筆文件之外，他同時也會將 hello how are you 拆分成許多不同的小單字，譬如說會拆分成 hello、how、are、you 這四個字，並且 Elasticsearch 會將這些單字「對應」到該筆評論數據上。\n因此後續當使用者查詢 hello 或是查詢 are 時，Elasticsearch 就可以去檢查反向索引，如果有任何一個命中的話（譬如說命中 hello 這個反向索引），那麼就可以根據反向索引，去找出該索引指向的真實的數據（也就是 hello how are you 這個評論），這樣子就可以快速的在大量的資料中，去找到我們想要的數據了！\n也因為在整個 Elasticsearch 的世界中，他不僅要儲存每一筆真實的評論數據之外，Elasticsearch 同時也需要為每一筆評論數據生成對應的反向索引，因此佔用的數據空間會比一般的 SQL 資料庫還要多，不過這其實也是演算法中典型的「用空間換取時間」的概念，即是先對數據進行預處理，將數據處理成好查詢的方式，等到未來需要時就可以馬上查詢，就可以達到「降低查詢時間」的終極目標了！\nElasticsearch 介紹 # 了解了 Elasticsearch 中最最最核心的「反向索引」的概念之後，我們再回頭來看 Elasticsearch 的介紹時，就可以比較看得懂了。\n所謂的 Elasticsearch，他是一個強大的「全文搜尋」（Full-Text Search）工具，目標是讓我們可以從大量的數據中，快速找到想要的資料在哪裡。而 Elasticsearch 之所以可以做到這件事，就是因為他使用了「反向索引」（Inverted Index）的設計，先對數據進行預處理，因此才能夠在查詢時達到比傳統 SQL 資料庫更高的效率。\n也因為 Elasticsearch 的專長是「全文搜尋」，所以一般通常會將他用來「架設搜尋引擎」，因此像是旅遊攻略、評論、社群貼文、電商產品\u0026hellip;等等諸如此類的查詢，背後都很適合使用 Elasticsearch 來實作！\n圖片來源： Elasticsearch 官網 而除了一般的功能開發之外，Elasticsearch 也很常拿來用在內部系統中，最常見的就是用來「收集所有系統的 log」，這樣子 DevOps 或是後端工程師就可以搭配 Kibana、Grafana 之類的 UI 介面，從 Elasticsearch 中直接查詢 log 的資料，就不用再登入每一台 server 中去找 log 了！\nElasticsearch 總結 # 所以總結來說，只要牽扯到「全文搜尋」，基本上就是 Elasticsearch 的天下（當然 FAANG 大廠自行研發的搜尋引擎除外，他們的數據量真的太大，應該不是用 ES），並且內部的 log 系統通常也都是用 ELK 三兄弟來建置，所以了解一下 Elasticsearch 的用法絕對是很吃香的！\n大家如果對 Elasticsearch 有興趣的話，後續也可以參考 Elastic 官網的文件教學，或是我很久很久以前也有寫過一些 Elasticsearch 的文章，大家有興趣的話也可以參考看看～（不過這些文章年代有點久遠，可能很多東西已經不適用了，後續有時間我再來把這部分重寫一下）。\n補充：反向索引的實戰經驗分享 # 最後再跟大家補充一個我經歷過的 Elasticsearch 反向索引的真實故事…\n前面有提到，「","date":"2025-04-08","objectID":"9ceef27dc5c29f4f1673c70906e2e1a0","title":"Elasticsearch 是什麼？認識地表最強的全文搜尋工具！","url":"https://kucw.io/blog/elasticsearch-intro/"},{"categories":["其他技術分享"],"content":"在網路全球化的趨勢下，CDN 現今已經是一個很重要的技術了，但是 CDN 的用途到底是什麼？以及使用 CDN 的好處有哪些？這篇文章我們就來詳細了解一下吧！\n目錄 什麼是 CDN（內容傳遞網路）？ 海底電纜的故事 海底電纜的限制 CDN 如何解決網路傳輸的限制？ 補充：使用 CDN 的注意事項 使用 CDN 的三大好處 1. 降低網路延遲、降低頻寬成本 2. 提升網站可靠性、平衡流量 3. 防禦資安 DDoS 攻擊 CDN 總結 結語 什麼是 CDN（內容傳遞網路）？ # 所謂的 CDN，他的全稱是 Content Delivery Network，中文翻譯為「內容傳遞網路」，而 CDN 的用途，就是 「將靜態資源（ex: 圖片、影片、JavaScript 套件）部署到全球化的節點上，讓當地的居民可以更快速的取得到這些內容」。\n不過，在了解 CDN 所帶來的好處之前，大家要先對「海底電纜」有一點基本的認識，因此下面我們就先來介紹一下什麼是海底電纜，再回頭來說明 CDN 所帶來的好處有哪些。\n海底電纜的故事 # 大家有沒有曾經好奇過，為什麼我們人明明站在台灣，但是卻能夠看到美國網友所上傳的影片或梗圖呢？這確實是透過「網路」來傳遞沒錯，但是這個「網路」，他到底是如何從美國發送到台灣的？是透過衛星嗎？還是透過高壓電？還是說是透過微波通訊來傳遞？？\n特別像是台灣這種海島來說，我們四周都是海，感覺要拉高壓電也不太可能，全部透過衛星來傳遞成本也太高了，因此所謂的「海底電纜」，就是台灣主要的對外網路連線方式。\n大家可以把「海底電纜」想像成是中華電信在台灣四周海域中所鋪設的光纖，每一條光纖會長得像是下面這樣，也就是因為有鋪設這些海底電纜，因此我們的網路訊號才可以傳遞出去，外部的網路訊號也才可以傳遞進來。\n圖片來源： Inside the Extreme Life of Divers Repairing Billion $ Underwater Cables 而其實不光是中華電信，很多大型企業如 Google、Amazon\u0026hellip;等，也都會鋪設自己的海底電纜！像是 Google 一直以來都有海底電纜的鋪設計畫，下圖就是 Google 目前為止所鋪設的海底電纜。\n圖片來源： Google Cloud 據點 而 Google 自己鋪電纜的好處是可以拉一條專線給 Google 自己的服務使用，確保未來不會因為各種因素（ex: 地緣政治）導致全球化的網路失效。\n所以在現今的網路時代下，我們人處在台灣卻可以刷著 YouTube、Instagram，觀看其他國家的人所上傳的影片，背後就是因為有海底電纜在進行網路數據的傳輸。\n海底電纜的限制 # 而說到這裡，就不得不提一下海底電纜的一些物理限制了。\n因為海底電纜畢竟還是屬於光纖網路，因此 「兩者距離越長，傳輸的時間就要越久」，這個完全是物理上的限制，現今沒有更好的方式可以解決。\n所以換句話說，即使台灣和美國之間有一條海底電纜，能夠把彼此的網路串接在一起，但是彼此傳輸的時間仍舊需要很久，可能你人在台北，傳一張圖片給高雄的朋友只要 1 秒，但是傳給海的另一邊的美國朋友卻需要 20 秒，這個就是海底電纜的網路傳輸限制，專業術語稱為 「latency（延遲）」。\n因此簡單的說的話，物理距離越遠，你們之間的網路 latency 就會越高，因此就算全球化的海底電纜能夠將你我連接在一起，但是這個使用者體驗其實是不太好的。\n所以為了解決這個問題，CDN 就出現了！！\nCDN 如何解決網路傳輸的限制？ # 基於上述的限制，現在我們知道 「物理距離越遠，傳輸的時間就要越久」，那麼，我們是不是只要把圖片放在離使用者近一點的地方，這樣子使用者就可以更快的拿到那張圖片了？沒錯！！這個就是 CDN 的核心邏輯，也就是 「將靜態資源（ex: 圖片、影片、JavaScript 套件）部署到 全球化 的節點上，讓 當地的居民 可以更快速的取得到這些內容」。\n舉例來說，當 Netflix 製作完一部新的影集之後，他除了把這個影集上傳到美國的主 server 之外（稱為 Origin Server），他還可以把這部影集也上傳到其他地區的 server（稱為 Edge Server），譬如說日本、台灣、新加坡、澳洲…等。\n因此到時候，使用者就可以「就近」挑選離他最近的 Edge Server，並且可以直接從該 Edge Server 中串流一模一樣的影集觀看，因此就可以達到「降低網路傳輸時間」的好處了！\n所以透過 CDN，Netflix 就可以在世界各地提供高品質、無延遲的影片觀看服務了！\n補充：使用 CDN 的注意事項 # 不過在這邊要再特別提醒一下，只有「靜態資源」（ex: 圖片、影片、JavaScript 套件\u0026hellip;等檔案）才能夠放在 CDN 上，借助 CDN 的力量擴散到全世界，因此像是後端程式的動態處理，這些是沒辦法放到 CDN 上面的，只能夠自己在雲端中部署。\n所以基本上在後端開發中，最常見的部署到 CDN 上的就是影片和圖片，其他像是前端的 JS 等相關檔案，因為後端不太會接觸到，所以比較少需要處理這部分。\n使用 CDN 的三大好處 # 了解了 CDN 的用途和限制之後，接下來我們也可以來看一下使用 CDN 的三大好處。\n使用 CDN 有三大好處，分別是：\n降低網路延遲、降低頻寬成本 提升網站可靠性、平衡流量 防禦資安 DDoS 攻擊 1. 降低網路延遲、降低頻寬成本 # 使用 CDN 的第一個好處，就是可以降低網路延遲和頻寬成本。\n降低網路延遲其實就是 CDN 當初被發明出來的目的，也就是上面所介紹的內容，透過將靜態資源部署在「全球化」的 Edge Server 上，就可以讓「當地的居民」就近取得到相關內容，降低網路傳輸的延遲，進而提升使用者體驗。\n而且也因為有了 CDN 之後，台灣人就可以就近取得影片，因此 Netflix 就不需要每次都把影片從美國傳到台灣，也就可以降低海底電纜的傳輸頻寬成本了。\n2. 提升網站可靠性、平衡流量 # 使用 CDN 的另一個好處，就是可以提升網站的可靠性、以及平衡相關的流量。\n舉例來說，假設 Netflix 在台灣的 CDN 節點掛掉了，那就可以趕快把台灣的流量轉移到附近地區（ex: 日本、香港、新加坡），確保台灣的 Netflix 用戶能夠繼續觀看劇集（雖然這不可避免地會多增加一些網路延遲，但是總比完全不能看好）。\n除此之外，即使台灣的 CDN 節點沒掛掉，Netflix 也可以根據台灣的尖峰流量，動態的調整香港的 CDN 來協助分攤，因為有的時候可能台灣某一陣子很瘋某個劇，流量就會大量上升，因此這時候就可以讓附近的 CDN 一起幫忙分擔流量，降低台灣 CDN 節點的負擔。\n3. 防禦資安 DDoS 攻擊 # 使用 CDN 的第三個好處，是可以防禦資安相關的 DDoS 攻擊。\n不過這部分我其實還不太熟，因此只能先貼相關的連結給大家參考（Cloudflare 官方的 CDN SSL/TLS | CDN security）。\n其實資安的部分算是使用 CDN 的附加好處啦，因為這跟 CDN 最一開始要解決的問題不太一樣，算是大家先因為「降低網路延遲」的原因用了 CDN 之後，才發現 CDN 在資安的表現上也很讚這樣，因此目前也常常會透過 CDN，來避免相關的資安攻擊。\n更新：以下是來自 Kuma 大大使用 CDN 防禦 DDoS 的實務經驗分享！大家有興趣也可以追蹤他的 YouTube 頻道：Kuma 老師的軟體工程教室。\nDDoS 我可以粗淺的補充一下，以前我前公司被打爆勒索過。\n如果你把 domain name 直接指到你的伺服器，當","date":"2025-03-18","objectID":"ff6caecc434989de506c53229349c979","title":"CDN 是什麼？一次搞懂 CDN 的用途和三大好處","url":"https://kucw.io/blog/cdn/"},{"categories":["其他技術分享"],"content":"只要是後端開發，多多少少都會有一些撰寫定時任務的需求，像是每天晚上 10 點固定處理報表、或是每 30 分鐘處理一批次的任務\u0026hellip;等等，因此這篇文章我們就來介紹一下，這個「定時任務」的語法到底要怎麼寫吧！\n目錄 什麼是 Cron？ Cron 表達式的寫法（5 個 *） 補充：Cron 表達式的進階用法 Cron 總結 補充：Spring Boot 的 Cron 表達式寫法（6 個 *） 結語 什麼是 Cron？ # 所謂的 Cron，其實是 Linux 系統下的一個定時任務管理服務，但是因為 Cron 的表達式實在是太萬用了，所以目前也很廣泛用在 GitHub Actions、Spring Boot 的 @Scheduled\u0026hellip;等框架上。\n舉例來說，只要大家有看過類似的表達式 - cron: \u0026quot;0 21 * * *\u0026quot; 或是 @Scheduled(cron = \u0026quot;0 0 21 * * *\u0026quot;) 的寫法，裡面的一堆莫名其妙的 *，其實就是 Cron 的「定時任務」的寫法！\n因此像是在上面這兩個例子中，\u0026quot;0 21 * * *\u0026quot; 指的其實就是「每天晚上 9 點整執行一次這個任務」的意思。\n不過也因為 Cron 的表達式實在是有點抽象，所以下面我們就來介紹一下 Cron 中的每一個 * 代表的意義，來徹底了解一下如何使用 Cron 寫出我們想要的時間吧！\nCron 表達式的寫法（5 個 *） # 在 Cron 中，最傳統且常見的表達式為「5 個 *」，不過因為 Spring Boot 本身的設計不太一樣，所以 Spring Boot 會採用「6 個 *」 的方式來撰寫（因此大家如果不是寫 Java/Spring Boot 的話，就只要看上半部分的 Cron 傳統寫法就好，但如果是寫 Java/Spring Boot 的話，建議也要把下面的補充部分看完）。\n而如果要寫出 Cron 的傳統表達式（5 個 * 的版本），首先必須要先寫出 5 個 *，並且每個 * 之間要用一個空白鍵隔開，所以最基本的 Cron 表達式就會像是下面這個樣子：\n* * * * * 而這 5 個 *，他們所站的每一個「位置」，就會代表不同的意思，譬如說左邊數過來第一個 * 是表示「分鐘」的意思，左邊數過來第二個 * 是表示「小時」的意思…以此類推，因此這 5 個 * 的實際意義，可以用下面這張圖來表示：\n所以舉例來說，假設 Cron 表達式為 0 16 * * *，那就是表示「我們指定在每天的 16:00」要執行一次這個任務（因為我們設定了第一個 * 為 0，第二個 * 為 16，所以就是表示我們指定要在分鐘數為 0、並且小時數為 16 的時候執行任務，因此 0 16 * * * 的結果才會是在每天的 16:00 執行任務）。\n再舉一個例子，假設 Cron 的表達式為 23 1 * * *，就是表示我們指定在「每天的凌晨 1 點 23 分」要執行這個任務。\n再再舉一個例子，假設 Cron 的表達式為 59 23 * * *，就是表示我們指定要在「每天的晚上 23:59 分」執行任務。\n所以對於 Cron 表達式而言，只要你將某一個 * 改為一個確切的數字，其實就是指定要在「那個時間點」執行任務，就只是這樣子而已！！Magic！！\n不過另外也補充一下，除了有被修改過的數字，其他維持原樣的 * 則是表示「所有」的意思，所以像是 23 1 * * *，就只有前面的 23 1 的「凌晨 1 點 23 分」是固定的，其他的 * * * 則是表示「所有」，也就是在「所有日期、所有月份、所有星期」底下的「凌晨 1 點 23 分」，都會執行當前這個任務，因此 23 1 * * * 才會被解讀成「每天的凌晨 1 點 23 分」。\n下面再舉更多的例子給大家參考：\nCron 表達式 實際意義 備註 0 21 * * * 在每天的晚上 21:00 執行一次任務 * * * * * 每分鐘都要執行一次任務 0 * * * * 在每個小時的 0 分執行一次任務（ex: 1 點 0 分、2 點 0 分…. 23 點 0 分） 0 0 * * 5 在每週五的 0 點 0 分執行任務 但一般會避免寫 0 點 0 分這種數字，因為 0 點 0 分是一天的開始，所以如果是要進行那種「當天晚上結算的任務」，改成 23:59 會比較好 59 23 * * 0 在每週日的 23 點 59 分執行任務 注意 0 是週日（Sunday），1-6 是週一到週六 30 10 1 * * 在每個月的 1 號的早上 10:30 分執行一次任務 所以透過上面的例子，我們就可以指定「現在是想要在哪一天、哪一個小時、哪一分」，去執行我們想要執行的任務了！\n補充：如果想要自己玩玩看 Cron 表達式，也可以到 Crontab.guru 這個網站玩一下，他不僅有基礎教學，還可以檢測你寫的 Cron 表達式是否正確，因此也很推薦大家在撰寫 Cron 表達式時先丟上來檢查一下，確認沒問題之後再寫進程式裡面。\n補充：Cron 表達式的進階用法 # 不過，Cron 除了有上述的常見用法之外，Cron 其實有更進階的用法，但以下這些用法其實我自己也用的不多，所以建議大家參考就好。\n舉例來說，在上面的 Cron 的基本用法中，當我們寫 30 10 * * * 時，就是要在「每天的 10:30」執行任務，很直覺對吧，但是如果我們寫成 */3 10 * * * 時，就是表示要在「每天的 10 點 0 分、10 點 3 分、10 點 6 分、10 點 9 分\u0026hellip;（不斷加三分鐘）\u0026hellip;、10 點 54 分、10 點 57 分」執行任務。\n除此之外，我們也可以改寫成是 30 10-15 * * *，則是表示要在「每天的 10:30、11:30、12:30、13:30、14:30、15:30」執行任務。\n再或者，我們也可以改寫成是 30 10,23 * * *，表示要在「每天的 10:30 和 23:30」執行任務（這個好用）。\n所以在 Cron 的進階用法中，已經進展到「不只是指定某個特定的時間點了」，而是變成「指定一段時間區間」或是「指定時間的變動方式」了，因此大家如果有比較特殊的需求，就可以研究一下 Cron 的進階用法，寫出最符合你想要的時間表達式。\n這邊也是舉更多的例子給大家參考，或是大家也可以直接到 Crontab.guru 上測試，他也有支援進階的用法。\nCron 表達式 實際意義 0 22 * * 1-5 在週一到週五的每天晚上 22:00 執行任務 23 12-20/2 * * * 在每天的 12:23、14:23、16:23、18:23、20:23 執行任務 30 10,11 * * 0 在每週日的 10:30 和 11:30 執行任務 59 23 1 */2 * 在每個奇數月（1、3、5、7、9、11）的 1 號的 23:59 執行任務 Cron 總結 # 所以總結上面的介紹的話，Cron 原本是 Linux 中的一個定時任務管理工具，但是因為他的表達式實在是太好用，所以現在很廣泛使用的定時任務的表達上面。\n而常見的 Cron 為 5 位數（預設是 5 個 *），每一個位數分別代表了「分鐘」、「小時」、「日期」、「月份」、「星期幾」的含義，因此大家就可以透過修改不同位數的值，用來表達你想要在哪個時間點執行任務了！\n補充：Spring Boot 的 Cron 表達式寫法（6 個 *） # 在 Spring Boot 中，如果我們想要使用 @Scheduled 創建一個定時任務的話，則是要","date":"2025-03-11","objectID":"72bcf4c692c3bab41f551aa8a50b87c3","title":"Cron 是什麼？定時任務的語法怎麼寫？","url":"https://kucw.io/blog/cron/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，這篇文章是每月一次的自媒體月報，主要分享我經營《古古的後端筆記》個人品牌的幕後秘辛，如果你也對於「自媒體創業」有興趣的話，歡迎繼續閱讀本文～\n補充：如果想了解《古古的後端筆記》電子報的起源，也可以先查看創刊號的文章。\n2025.2 月粉絲追蹤數、電子報訂閱人數 # 本月份（2025.2）的粉絲成長人數如下：\n2025/1/28 人數 2025/2/25 人數 2 月份（1 月份）總成長人數 Facebook 粉專追蹤數 4547 4607 +60（+72） 電子報訂閱人數 2872 3079 +207（+392） Threads 粉絲追蹤數 7354 8135 +781（+1358） IG 粉絲追蹤數 600 688 +88（+111） 這個月的數據也是順順成長，雖然數據可能相比上個月有稍微降低，但也是因為我這個月比較忙碌，比較沒辦法很好的兼顧社群的營運，所以才會這樣。\n等到之後比較穩定之後，應該就比較有時間可以跟大家在社群上互動惹！在此之前可能只有在電子報才看得到我😂，大家如果有什麼想跟我說的悄悄話可以直接私訊我 or 回信給我，我看到都會盡量回的🙏。\n本月的自媒體主題：電子報為什麼又開始紅了？ # 這個月剛好有一個自媒體主題很想跟大家分享，所以就借月報的篇幅來寫了，那就是：「電子報為什麼又開始紅了？」。\n最近看到台灣越來越多人開始寫電子報，覺得很讚！其實電子報作為上古時代的行銷產物，曾經有一度是被大家唾棄的，曾經我們對電子報的印象是「促銷信件」，就是那種直接會被你丟到垃圾信件的廣告信，但是現在我們對電子報的印象卻是「吸取知識的管道」，到底為什麼電子報有這麼大的轉變？今天就來討論一下這個話題吧！\n電子報和其他社群媒體有一個最最最不一樣的差別，在於 「曝光度」，也就是俗稱的 「流量」。\n其實在現今的社群媒體中（不管是 Facebook、Threads、YouTube、抖音），當創作者寫完一篇文章之後，其實這篇文章不一定會被我的讀者所看到。\n舉例來說，當我在 Facebook 上發一篇文時，可能只有三分之一的追蹤者能看到這篇文章，其他的追蹤者是「根本連看都沒看到，連想按讚的機會都沒有」，而為什麼會有這個差別？就是 Facebook 的演算法所導致的。\n目前的演算法是當你發一篇文之後，他會先小範圍的散播這篇文章給你的部分追蹤者，假設這些追蹤者的反應很好（譬如說按讚、留言、分享），那麼演算法就會覺得這篇文章是個好文，所以就會再把這篇文章擴散出去，讓更多人能看到這篇文章。\n但如果反過來呢？假設你寫的文章沒什麼人按讚分享，那麼演算法就會覺得這篇文章不好，因此後續就不會推送這篇文章給其他人，因此這篇文章的觀看人數就會非常少，原因就是因為被演算法給掐滅了。\n所以在大社群時代下，只有 Facebook 這種平台是贏家，我們這些小創作者都只是他的囊中之物而已，今天 Facebook 心情好，他就會將你的文章推送給更多人，假設今天他心情不好，你寫的任何寶藏文章都沒人看得到，這就是演算法的機制，也就是 「曝光度」 的概念。\n那麼，要打破這個機制，有沒有其他解決方案呢？有的，那就是電子報！！\n電子報有一個特性，就是 「創作者可以直接把他寫的文章交到你手上」，舉例來說，只要我有你的信箱，我寫的每一封電子報，都可以確保這封信一定會進到你的信箱裡面。\n所以對於電子報而言，再也沒有「曝光度」的問題了，只要我肯寫，只要你肯讀，創作者和讀者之間一定可以維持某種聯繫，再也不會發生「我寫了，但是你沒收到」的幽靈情況出現。\n並且電子報還有一個附加價值，就是 「隱私性」，舉例來說，假設我在 Facebook 上公開發一篇文，可能你對於電子報中的某個議題很感興趣，但是又不好意思在下面留言，私訊好像又有點麻煩，所以就作罷。\n但是電子報可以直接透過「回信」的方式，就可以直接讓讀者有一個窗口和創作者對話，並且這個過程是完全隱私的！！！只有讀者和創作者彼此知道信件內容而已，因此寫起來可以更隨心所欲一點，不用擔心會被第三者看到。\n所以基於上述的種種好處，電子報就從以往的促銷信件時代，轉換成了大知識時代，將來可能會有越來越多創作者同時經營「電子報 + 社群平台」（我現在就是這樣），除了持續在 Facebook 等社群平台打造影響力之外，同時也確保喜歡我的內容的讀者真的可以收到我創作的內容，不然寫了那麼久的文章，結果最終被 Facebook 吃掉，沒辦法傳遞給想閱讀的讀者手上，還是會有點難過的🥹。\n希望後續台灣也會有越來越多的創作者使用電子報進行創作，讓這些精彩的內容可以被看見，而不是埋沒在社群平台的底下。\n本月撰寫的電子報主題、文章 # 這邊記錄了我這個月撰寫了哪些電子報和文章，大家如果對於其中的內容有興趣的話，也可以前往查看：\nDNS 是什麼？在瀏覽器中輸入 URL 會發生什麼事？ Polling 和 Webhook 是什麼？如何更有效的串接第三方系統？ Http 中的 GET 和 POST 的差別在哪裡？ 2025.2 月報總結 # 呼～這個月大概是這樣吧！電子報的部分因為覺得很有共鳴所以打的有點長😂，希望你們會喜歡這類內容。\n那我們就下個月再見啦！\n","date":"2025-03-04","objectID":"8d992ff7f509cc6e08e392c0297aaa62","title":"軟體工程師的自媒體之路 - 2025.2 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202502/"},{"categories":["其他技術分享"],"content":"在軟體開發中，Http 可以說是大家最常打交道的一個協議了，因此這篇文章就會來介紹一下 Http 中的 GET 和 POST 的差別。\n目錄 什麼是 Http Method？ GET 介紹 POST 介紹 GET 和 POST 總結 補充：GET 和 POST 的更多細節比較 結語 什麼是 Http Method？ # 所謂的 Http Method，就是在發送一個 API 時，所使用的請求方式。\n較常見的 Http Method 有以下四種：\nGET POST PUT DELETE 其中 GET 和 POST 是使用上最常見的兩種請求方法，所以接下來我們就來比較一下，GET 和 POST 之間的差別在哪裡吧！\nGET 介紹 # GET 是最常使用的 Http Method，大家可以把 GET 想像成是 「明信片」 的概念，也就是 「當你使用 GET 來請求時，你所傳遞的參數就會被別人看見」。\n舉例來說，在使用 GET 來請求時，我們就要將請求參數添加在 url 的最後面。因此在下面的例子中，我們就必須要將參數 id=123\u0026amp;name=Judy 添加在 url 的最後面（黃色區塊所示），這樣子才能夠成功傳遞參數給後端。\n所以在使用 GET 方法來請求時，所有的請求參數都是「公開的」，因此任何人都能查看參數的值，所以 GET 就像是「明信片」一樣，信中的內容可以被所有人查看。\n也因為使用 GET 來請求的所有參數都會被看見，因此如果你的請求參數是比較敏感的資訊（ex: 身分證字號、手機號碼），那就不建議使用 GET 方式來請求，因為這些數據會暴露在 url 中給其他人看到，可能就會導致數據的洩漏。\n甚至瀏覽器也會 cache GET 請求的 url，以記錄你曾經訪問過哪些網頁，因此使用 GET 來請求真的是走過路過都會留下痕跡，因此敏感數據絕對不可以使用 GET 來傳遞！！\n因此如果想要更隱私的傳遞參數，就可以使用下面的 POST 來實作。\nPOST 介紹 # 不同於 GET 是公開所有的請求參數，POST 就是相反過來，POST 會隱藏所有的請求參數，因此任何人都沒辦法取得其中的請求參數，所以 POST 也可以說是「信封」的概念，所有的內容都會好好的被封裝在信封內，不會隨意向外部展示。\n舉例來說，在使用 POST 來請求時，我們就要改成將請求參數添加在 request body 中，並且通常會使用 JSON 格式來添加數據。因此在下面的例子中，我們就會將 JSON 參數 {\u0026quot;id\u0026quot;: 123, \u0026quot;name\u0026quot;: \u0026quot;Judy\u0026quot;} 添加在 request body 中（藍色區塊所示），這樣子就能夠隱私的將參數傳遞給後端了。\n所以在使用 POST 方法來請求時，所有的請求參數都是「隱私的」，只有收件者（後端）都能查看，其他人是完全看不到的，因此就像是「信封」一樣，信中的內容會對其他人隱藏，只有收件者後端可以查看。\n也因為使用 POST 來請求時，所有的參數都會被隱藏起來，不被其他人所看見，因此像是比較敏感的資訊（ex: 身分證字號、手機號碼），就很適合使用 POST 來請求，確保敏感數據不會洩漏。\n補充：雖然 POST 是信封的概念，會將所有請求參數隱藏起來，但是駭客仍舊是可以透過其他招數來偷看裡面的內容，因此就算是 POST 來請求還是會有風險，所以仍舊需要做其他的資安加強（ex: Https），才能確保數據不會被駭客給偷走。\nGET 和 POST 總結 # 所以總結上面的介紹的話，就可以將 GET 和 POST 做一個比較：\nGET： 明信片的概念，將參數放在 url 中傳遞，並且所有參數都是「公開的」，任何人都能看見。 POST： 信封的概念，將參數放在 request body 中傳遞，並且會將所有參數「隱藏起來」，其他人都看不見。 所以大家以後就可以根據自己目前的 API 的需求，選擇最適合的請求方式了！\n補充：GET 和 POST 的更多細節比較 # 如果大家想了解更多關於 GET 和 POST 的技術細節，也可以參考下方的表格整理（資料來源：前后端数据交互(八)——请求方法 GET 和 POST 区别）。\nGET POST 上一頁、重新整理網頁 無影響 數據會重新被提交 收藏 可收藏至瀏覽器中的「我的最愛」 不可收藏 cache 能被瀏覽器、後端 cache 不能被 cache 歷史 參數會保存在瀏覽器的歷史中 參數不會保存在瀏覽器歷史中 安全性 較差，因為請求參數添加在 URL 中 較安全，因為參數不會被保存在瀏覽器的歷史中、也不會隨著 URL 被記錄在 log 裡 數據長度 有限制，因為 URL 的最大長度是有規範的（最大 2048 個字符） 無限制 使用情境 一般網頁讀取 表單提交、新增數據 結語 # 這篇文章我們先介紹了 GET 和 POST 的用法，並且也比較了他們之間的差別，其實就是一個是「明信片」、另一個是「信封」而已～\n如果還想了解更多關於 Http、API 設計的相關知識，也可以參考過往的文章：\n一文搞懂 Http Status Code，詳細解析 200、301、401、403、500、503 RESTful API 設計指南，3 個必備條件缺一不可！ 如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2025-02-25","objectID":"9bb2e3f8f5517f0194864faf55a8e66a","title":"Http 中的 GET 和 POST 的差別在哪裡？","url":"https://kucw.io/blog/http-get-post/"},{"categories":["其他技術分享"],"content":"一般在實作後端系統時，如果想要去串接外部的第三方系統，這時候就會有兩種串接方式：Polling 和 Webhook，所以這篇文章我們就來了解一下 Polling 和 Webhook 之間的差別吧！\n目錄 什麼是 Polling？什麼是 Webhook？ 串接股票中心的故事 使用 Polling 來串接 使用 Webhook 來串接 Polling 和 Webhook 總結 補充：在 Webhook 的實作中，股票中心是如何通知我們的後端程式？ 結語 什麼是 Polling？什麼是 Webhook？ # 在串接第三方的系統時，我們可以使用兩種方式來串接，分別是 Polling 和 Webhook，而他們兩個在實作上其實是完全相反的兩種方式：\nPolling（輪詢）： 由「我們的後端程式」主動且瘋狂的去問「第三方系統」，詢問當前數據的值為何。 Webhook： 當數據有變動時，由「第三方系統」主動通知「我們的後端程式」，告訴我們數據已經變化了。 這個乍聽之下會覺得有點抽象，所以下面就透過一個例子，來介紹一下 Polling 和 Webhook 的差別。\n串接股票中心的故事 # 假設你現在想要實作一個後端的系統，當你偵測到某隻股票的價格低於某個點時（譬如說台積電股價跌破 1000 元），這時候你就要在後端系統中儲存當下的時間，如果是由你來實作這個後端程式，你會怎麼實作？\n使用 Polling 來串接 # 最直覺的做法，就是在你的後端程式裡面，每秒鐘都去 call 股票中心的 API，瘋狂的去問台積電現在的價格是多少，因此假設在 12:01 分時台積電的價格跌到 960 元，你就可以在你的系統中記錄當前的這筆時間。\n上面這種「不斷的去詢問股票中心 Server 當前的股價為何」的行為，就稱為是 Polling（輪詢）。\n但是上面這個做法有一個缺點，就是你 「每一秒」 都得要 call 一次股票中心的 API，才能知道當下的台積電股價，又因為一天有 86400 秒，所以你的後端程式一天就得要 call 股票中心的 API 86400 次，就會造成很大量的 API call，加重股票中心 server 的負擔。\n並且上面的做法除了會使得 API call 的數量飆升之外，其實在這些 API call 中，也不是每一次的 API 都很有用處。因為我們真正想要的功能，是「當台積電跌破 1000 元以下時，記錄當下的時間」，所以當台積電現在是 1100、1200、1300 元時，這些 API call 的返回值對我們都是沒有意義的，就只有跌破 1000 元的那個時間點，才是我們真正關注的部分。\n所以為了解決 Polling 的問題，Webhook 就出現了！\n使用 Webhook 來串接 # 在剛剛的 Polling 中，主動方是「我們實作的後端程式」，也就是由我們主動發起 API call，一直不斷地瘋狂去煩股票中心，問他現在台積電的股價是多少。\n而在 Webhook 中，則是會反過來，主動方變成是「股票中心」，也就是當台積電跌破 1000 元時，股票中心就會主動 call 我們的 API，通知我們台積電跌破 1000 元了！！\n所以在 Webhook 的設計中，主動方會變成是「股票中心」，只有當股票中心發現台積電跌破我們預期的值時，才發送通知給我們，在其他時間時，股票中心則是會什麼事都不做，完全不會 call 我們的 API，大大的減少了 API call 的次數。\n因此透過 Webhook 的設計，就可以將 API call 的次數降到最低，只有在台積電的股價跌破 1000 元時才會 call 一次我們的 API，比起前面的 Polling 的設計要 call 86400 次少多了！！因此在一般在實作上，通常會建議採用 Webhook 的設計，這樣子可以讓兩邊的 Server 負擔都不會那麼大，減少浪費的 API call 次數。\nPolling 和 Webhook 總結 # 所以總結上面的介紹的話，當我們在串接第三方的系統時，就可以使用兩種方式來串接，分別是：\nPolling（輪詢）： 由「我們的後端程式」主動且瘋狂的去問「第三方系統」，詢問當前數據的值為何。 Webhook： 當數據有變動時，由「第三方系統」主動通知「我們的後端程式」，告訴我們數據已經變化了。 因此下次大家在串接第三方系統時，就可以看一下他們的系統是否有支援 Webhook 的設計，如果有的話，就可以使用 Webhook 的方式來串接，這樣子就不用再瘋狂的一直去重複大量的 API call 了！\n補充：在 Webhook 的實作中，股票中心是如何通知我們的後端程式？ # 了解了 Polling 和 Webhook 的差異之後，這裡也順便補充一下 Webhook 的實作。\n因為 Webhook 的核心設計理念是「由股票中心變成主動方」，因此當台積電股價跌破 1000 元時，股票中心需要主動通知我們的後端程式，告訴我們台積電目前的股價。\n所以為了讓「股票中心有能力通知我們的後端程式」，需要實作兩個步驟：\n1. 我們需要先在我們自己的後端程式中，先新增一個新的 API\n首先我們需要在我們的後端程式裡面，先新增一個 API 出來（假設叫做 /myapi），至於這個 API 的請求參數有哪些、以及這個 API 的請求方法（ex: GET、POST）為何，通常第三方系統都會有詳細的定義，到時可以查詢一下第三方系統的相關文件。\n@RestController public class MyController { @RequestMapping(\u0026#34;/myapi\u0026#34;) public void myApi(@RequestParam String stockName, @RequestParam Integer price) { // 記錄當前股價和時間 } } 2. 到第三方系統中，在 Webhook 處填上你的 /myapi 的 API\n創建好 API 之後，接著可以登入到第三方系統，然後通常就會有一個填寫 Endpoint URL 的地方（如下圖所示），讓你將你剛剛所創建的 /myapi 的 API 給填上來（這邊因系統不同而有不同的介面，甚至有的第三方系統沒有 UI 介面，要自己去跟他們的工程師對接）。\n只要完成了上述的步驟之後，未來當你預期的事件發生時（ex: 台積電跌破 1000 元），這時候第三方系統就會 call 你所填上的 API（也就是 https://example.com/myapi）。\n所以換句話說的話，當你的 /myapi 被 call 時，就表示台積電跌破 1000 元了，因此你就可以在這個 /myapi 的程式中，去實作你想要實作的功能（如記錄當前的股價和時間…等等），這樣子就可以和第三方系統成功使用 Webhook 串接在一起了！\n結語 # 這篇文章我們先分別介紹了 Polling 和 Webhook 是什麼，並且也比較了他們之間的差別，Polling 和 Webhook 可以說是在串接第三方系統時很常見的兩種方式，雖然目前比較流行的是 Webhook 的作法，但是多了解一下 Polling 的設計理念也是很可以的～\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2025-02-18","objectID":"1112e9d93e5f1efe24770390415a11a5","title":"Polling 和 Webhook 是什麼？如何更有效的串接第三方系統？","url":"https://kucw.io/blog/polling-webhook/"},{"categories":["其他技術分享"],"content":"相信大家在準備面試時，常常會遇到一題是「在瀏覽器中輸入 URL 會發生什麼事？」，而這背後其實就和 DNS 息息相關。\n因此這篇文章我們就來介紹一下，到底在瀏覽器中按下 URL 會發生什麼事，以及其背後的 DNS 原理吧！\n目錄 什麼是 DNS？ 在沒有 DNS 的年代 DNS 的雛形 現今 DNS 的全貌 DNS 總結 結語 什麼是 DNS？ # 所謂的 DNS，全稱是 Domain Name System（好像沒有中文翻譯），DNS 的用途在於「查詢某個域名的 IP 位置為何」，這個聽起來可能有點抽象，所以接下來我們就透過例子來了解一下 DNS 的用途到底是什麼。\n在沒有 DNS 的年代 # 首先回到最最最古老的年代，其實當我們運行一台 Server 起來時，這台 Server 擁有的只有 IP 位置而已，譬如說某一台 Server 就會運行在 142.250.196.206 這個 IP 上面，因此當前端想要 call 後端的 api 時，前端就是要 call http://142.250.196.206:80/api ，就可以訪問這台 Server 上所運行的後端程式了。\n所以到這裡要先掌握一個重點，就是 「前後端之間溝通，只需要知道對方的 IP 位置即可」，只要知道對方的 IP 位置，前端就可以直接去 call 該 IP 上所運行的 Server，去請求後端的數據了。\nDNS 的雛形 # 不過，雖然對於前後端來說，他們彼此之只要知道對方的 IP 就可以進行溝通，但是對於使用者而言，要記住這麼長一串的 IP 地址可是很累人的啊！！譬如上方所提到的 IP 例子 142.250.196.206，要記住這麼長一串 IP 位置是很痛苦的。\n但是如果我們能把這個 IP 位置，轉換成「一個簡單好記的英文單字」，這樣子是不是會更方便記憶？譬如說我們將 142.250.196.206 轉換為 google.com，google.com 總是比一串數字來的好記多了對吧！！也因為如此，DNS 就被發明出來了！！！\n大家可以把 DNS 想像成是一張很大很大的 table，他裡面會儲存 域名:IP位置 的一一對應，所以以上面的例子來說，就可以在 DNS 這張 table 中插入一筆 google.com 和 142.250.196.206 的對應關係。\n域名 IP google.com 142.250.196.206 \u0026hellip; \u0026hellip; 因此這時當使用者在瀏覽器中輸入 http://google.com/api 時，就會依序執行以下的步驟：\n使用者在瀏覽器中輸入 http://google.com/api 網址 瀏覽器詢問 DNS：「google.com 的 IP 位置是多少？」 DNS 查詢他的 table 中有沒有記錄 google.com 所對應的 IP 的值，因為 table 中有記錄（即是 142.250.196.206），所以 DNS 就會回覆瀏覽器：「google.com 的 IP 為 142.250.196.206」 瀏覽器知道 IP 位置之後，改成去請求 http://142.250.196.206/api，成功連線到後端 Server，因此就可以成功的去 call 後端的 api 了 後端返回 api 的數據 所以在其實在這整個前後端的溝通中，DNS 所負責的任務就只是「查詢一下 google.com 這個域名的 IP 位置為何」而已，就這樣！！比想像中單純對吧🤣。DNS 至始至終都只是在記錄「域名和 IP 之間的對應關係」而已，只要掌握好這個核心邏輯就可以了！！\n補充：實際上 142.250.196.206 其實就是 Google 在台灣的一組 IP 地址，如果在瀏覽器中輸入 142.250.196.206 的話，就會跳轉到 https://www.google.com/ 上，大家有興趣也可以玩一下😆。\n現今 DNS 的全貌 # 不過，雖然 DNS 所負責的任務就只是「查詢一下 google.com 這個域名的 IP 位置為何」而已，但是當這個域名數量變的很多很多很～～～多的時候，DNS 也逼不得已得進化一下，設計成更能符合當前量級需求的架構。\n因此在現今的 DNS 架構中，不會再只使用一張 table 來記錄所有的 域名:IP 的對應關係，而是會分成：\nRoot Name Server（根域名） TLD Name Server（頂級域名） Authoritative Name Server（權威域名） 這三層來依序將「域名」解析成「IP」。\n舉例來說，假設今天的域名是 www.googe.com，那 DNS 內部就會先拿著 www.google.com 去問 Root Server 這個該去找誰解析成 IP，Root Server 會先檢查這個域名的結尾（也就是 .com），因此就會叫 DNS 拿去找第二層的 .com TLD Name Server。\n而當 DNS 去問第二層的 .com TLD Name Server www.googe.com 的 IP 為何時，第二層的 .com TLD Name Server 就會去解析主域名（也就是 google.com），然後叫 DNS 拿著去找第三層的 google.com Name server。\n而當 DNS 終於抵達第三層的 google.com Name Server 時，DNS 就會一樣會問他 www.google.com 的 IP 為何，這時候第三層的 google.com Name Server 就會找出他的真實 IP 為何（也就是 142.250.196.206），然後將他回傳給 DNS。\n到這裡為止，DNS 內部才是真正的找出 www.google.com 所對應的 IP 地址為 142.250.196.206，因此就可以將這個 IP 地址回傳給瀏覽器，讓瀏覽器去請求這個 IP 地址了！\n不過即使 DNS 的內部架構變的稍微複雜一點，但是 DNS 實際上要做的事情仍舊是不變的！也就是「查詢一下 google.com 這個域名的 IP 位置為何」而已，因此大家如果覺得上面的 DNS 架構有點複雜的話，暫時忘記他其實也是可以的😆，大家只要知道 DNS 的目的是「查詢域名的 IP 位置為何」就好，細節可以等到真的要用到的時候再回來參考即可。\nDNS 總結 # 所以總結一下上面的介紹的話，所謂的 DNS，全稱是 Domain Name System，並且 DNS 的用途在於「查詢某個域名的 IP 位置為何」。\n而在 DNS 的內部架構中，為了提升查詢的效率以及提高能同時乘載的查詢量，所以 DNS 內部會分成三層（Root、TLD、Authoritative），根據域名的結尾不同，去分配到不同的 DNS Server 上，盡可能的分散每一台 DNS Server 所需要負責的查詢數量，讓這些 DNS Server 能夠承載目前世界上所有人的 IP 查詢。\n也因為 DNS 的用途在於「查詢某個域名的 IP 位置為何」，因此當使用者在瀏覽器中輸入某個 URL 時，實際上瀏覽器在背後就會偷偷先去詢問 DNS：「這個域名的 IP 為何？」，等到問到 IP 位置之後，瀏覽器才會實際去 call 該 IP 上所運行的後端服務，因此下次再有人問你「在瀏覽器中輸入 URL 會發生什麼事」時，你就可以大膽的回覆他「DNS 會先去….」的答案了😆。\n補充：其實在瀏覽器拿到 Server 的 IP 位置之後，還會需要和 Server 建立 TCP/IP 的連線，等到建立完連線之後才能真正的 call API 取得數據，不過老","date":"2025-02-04","objectID":"99557aa4e80c4dba2c632fb92fa863cb","title":"DNS 是什麼？在瀏覽器中輸入 URL 會發生什麼事？","url":"https://kucw.io/blog/dns/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，這篇文章是每月一次的自媒體月報，主要分享我經營《古古的後端筆記》個人品牌的幕後秘辛，如果你也對於「自媒體創業」有興趣的話，歡迎繼續閱讀本文～\n補充：如果想了解《古古的後端筆記》電子報的起源，也可以先查看創刊號的文章。\n2025.1 月粉絲追蹤數、電子報訂閱人數 # 本月份（2025.1）的粉絲成長人數如下：\n2024/12/31 人數 2025/1/28 人數 1 月份（去年 12 月份）總成長人數 Facebook 粉專追蹤數 4475 4547 +72（+86） 電子報訂閱人數 2480 2872 +392（+404） Threads 粉絲追蹤數 5996 7354 +1358（+1430） IG 粉絲追蹤數 489 600 +111（+136） 這個月一樣是各項指標持續成長中（感謝大家的支持🙏），不過這個月有一篇文章的流量比較突出，是「分享我從零開始刷 LeetCode 的數據」這篇貼文的分享（有興趣的可以前往 Facebook 或是 Threads 查看，或是直接查看下方原文）。\n過年後準備開始投履歷，和大家分享我從零開始刷 LeetCode 的數據～\n先說一下我刷 LeetCode 的策略，我用的是 NeetCode 和 Grind 整理的題目，並且會分成一周目、二周目、三周目來刷（跟玩遊戲一樣）\n一周目：先刷 Easy + 簡單 Medium，重點在於讓自己熟悉常見的資料結構和解法（Linked List、Tree、Graph、Sliding Window 等） 二周目：回頭刷當初沒刷完的 Medium，並且學習一周目跳過的較難的演算法（DP、Greedy、Monotonic Stack 等） 三周目：把 Hard 刷完，並且複習前面所有內容 我目前已經把一周目 + 二周目刷完了，正要進展到三周目，而我在一二周目的學習時間統計下來，大概各自花費了 100 小時左右（所以總共是 200 小時）\n所以如果是以上班族一週學習 4～5 小時的話，200 個小時就是要準備 10 個月左右，真的是長期抗戰啊\u0026hellip;，刷題的大家都辛苦了🥹，我們一定可以攻克難關的！！\n老實說最近我真的是刷題刷吐了，這篇文章也是想說可以整理一下刷題的過程給大家參考，也給自己做一個記錄這樣。\n不過有細心的古粉發現這篇文章的最開頭有寫到「過年後準備開始投履歷」這個小細節，確實是這樣沒錯！我確實最近要回頭找工程師的工作了！！所以想說剛好藉著這個自媒體月報的篇幅，也想跟大家分享一下「為什麼我做了自媒體之後，仍舊想要回頭找工程師的工作」。\n為什麼我做了自媒體之後，仍舊想要回頭找工程師的工作？ # 會想要回頭找工程師的工作，主要原因應該還是因為我在 2024 年的自媒體一年中過的不是很好，覺得「全職自媒體」這個行業可能不太適合我，自媒體可以是我的一個事業，但是沒辦法完全是我的主業這樣。\n可能是因為剛好我走的這條路是「技術自媒體」，我自己也很喜歡分享知識、教課，如果我自己沒有真的在第一線的程式上打拼的話，感覺自己好像會離技術越來越遠，並且也不確定自己目前分享的東西業界到底還有沒有在用、是不是其實分享了沒用的知識…等等，諸如此類的焦慮會很常浮現在我心中🥹。\n我想要變強，不想要原地踏步，但是有很多經驗是必須在第一線實戰中累積的，不管是解決問題的能力、團隊合作的能力、甚至是通靈客戶的能力（懂的都懂），這些都是得真實的和別人相處之後，才有辦法累積的實力，相較來說，「自媒體」終究只有「自己」，一個人的能力還是有極限的。\n不過我不是要放棄自媒體了唷！！我覺得自媒體作為一個副業來說還是很讚的👍，至少有一個管道可以分享最近學到的知識、可以抒發心情、可以認識到很多不同的大大們、可以幫助到需要幫助的人，這份工作對我而言真的很有意義。\n我希望將來可以在工作中越變越強，並且在變強之後，也想帶著大家一起變強，2025 年給自己的期許是「Sharing is Learning!」，我會努力做到的！\n所以如果總結一下的話，我會想要從「自媒體」回去做「工程師」，有以下幾點：\n「自媒體」會離業界越來越遠，不確定自己所知是否仍是業界主流，容易感到焦慮 「自媒體」是很自由沒錯，但是長期的精神壓力會很重（壓力來自於不確定感和不安感），不確定未來方向要往哪裡走、下一個客戶在哪裡 當「工程師」的好處有：可以磨練自己的技術、團隊合作能力、客戶通靈能力，並且可以參與大型的專案開發，累積系統架構的設計能力 同時兼顧「自媒體 + 工程師」感覺是目前最棒的選擇，但是要兼顧兩者真的很累（曾經兼過，那陣子都沒睡飽過），不知道是否能找出更平衡的兼顧方式 （2025 來找找看吧！） 目前的想法大概是這樣，想把目前的心情記錄下來，不僅給自己做一個記錄，也一起分享給大家～\n但我覺得人的每一個階段都會喜歡不一樣的東西啦，所以我也是有可能工作個 3、5 年之後，就又辭職跑去全職做自媒體了🤣，未來的事情誰知道呢？人生在世一場，如果能夠在每一個時間點，都做出不愧於自己的決定，應該就能盡量不留下遺憾了吧。\n希望 2025 年，我們都可以做出自己最想要的那個決定！\n本月撰寫的電子報主題、文章 # 這邊記錄了我這個月撰寫了哪些電子報和文章，大家如果對於其中的內容有興趣的話，也可以前往查看：\n電腦中的 RAM（記憶體）的用途是什麼？他和硬碟的差別在哪裡？ 如何成為資深工程師？心態最重要 Event Sourcing 是什麼？他和 CRUD 的差別在哪裡？ GPU 是什麼？為什麼 GPU 對 AI 的發展很重要？ 2025.1 月報總結 # 最近這幾個月都在忙刷題，比較沒有多餘的心力看自媒體的書了🥹，近期的目標應該會優先擺在找工作上，後續如果有比較多求職心得時，也再來跟大家分享求職的過程～\n那我們就下個月的月報再見啦！\n","date":"2025-02-04","objectID":"31251f5946e36deaf38b9c13c348e00e","title":"軟體工程師的自媒體之路 - 2025.1 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202501/"},{"categories":["其他技術分享"],"content":"在 AI 的浪潮下，GPU 可以說是一間公司的算力的體現，但是 GPU 到底是什麼？為什麼他對 AI 的發展這麼重要呢？所以這篇文章我們就來介紹一下，到底什麼是 GPU 吧！\n目錄 什麼是 GPU？ 影像處理簡介 GPU 和影像處理的關聯 為什麼 GPU 在 AI 時代這麼重要？ 補充：為什麼提升「訓練模型」的速度這麼重要？ GPU 總結 結語 什麼是 GPU？ # 所謂的 GPU，全稱是 Graphics Processing Unit，中文翻譯為「圖形處理器」，所以顧名思義，其實 GPU 最一開始的用途，就是拿來「處理影像圖形」用的。 只不過因為 GPU 的「平行處理」的特性實在太好用，所以後來才被拿來挖礦、甚至發展 AI。\n所以在了解 GPU 為什麼對 AI 很重要之前，首先一定要先了解 GPU 的用途為何，了解 GPU 是怎麼做到「平行處理」的概念的，才能夠了解為什麼 GPU 對於 AI 的發展這麼重要。\n影像處理簡介 # 其實 GPU 最一開始的用途就是拿來「處理圖形」的，而在了解 GPU 的用途之前，我們必須要先了解一下目前電腦螢幕裡面是如何構成的，才能知道 GPU 的用途為何。\n舉例來說，在每一台電腦螢幕中，其實都是由好幾個微小的像素（pixel）所組成，而每一個像素你可以想像成就是一個小格子。\n所以像是現在主打的 4K 螢幕（3840×2160），就是指這台螢幕的長度有 3840 個像素，而高度則是有 2160 個像素，因此在整台螢幕上面，就是有 3840 x 2160 = 829 萬個像素（pixel）存在。\n所以我們平常在使用螢幕時，不管是觀看 YouTube 影片、寫程式、打遊戲…等等，電腦就要分別計算「每一格 pixel 當前應該要呈現什麼顏色」，而當所有的 pixel 都計算好自己應該長什麼顏色之後，最終就會呈現一張完整的圖片了。\n如果大家有生成過點陣圖、或是有玩過 Minecraft 的話，其實就是背後用的就是「像素」的概念，像是下圖就是一個 pixel 實際呈現的結果，當每一格 pixel 都計算出他的顏色之後，最終呈現給我們人眼看的結果，就是最右邊的「寶劍」了！\n所以在影像處理的世界中，「每一格 pixel 都是獨立的，自己控制自己要呈現什麼顏色」，而當這些 pixel 們計算好自己要呈現什麼顏色之後，最終合成再整個合在一起看，就是對我們人類有意義的圖片了～\n所以其實對於 pixel 而言，他其實根本不知道現在是要呈現什麼圖片😂，他的任務就是乖乖計算自己要什麼顏色，剩下的怎麼解讀就交給人類自己去判定。\nGPU 和影像處理的關聯 # 所以透過上面的介紹，現在我們知道「一張圖片之所以能夠呈現出來，靠的就是 pixel 們自己在背後默默的運算」，而這其實就是 GPU 被發明出來的契機！\n在 GPU 還沒被發明出來之前，如果我們用傳統 CPU 的去計算 pixel 的值的話，那就是得寫個 for loop，然後每一次 loop 就去計算每一格 pixel 的值。\n圖片來源： 【OpenGL 篇】为什么游戏总要编译着色器？ 但是使用 CPU 這樣子 for loop 一格一格計算實在是太慢了，所以為了加快計算的過程，GPU 就被發明出來了！！\n在 GPU 裡面，有超超超超超級多個核心存在，每一個核心都可以執行一個簡單的計算，所以對於 GPU 而言，他就可以為每一個 pixel 都分配一個核心，然後 「在同一時間，所有核心都去計算他所分配到的 pixel 的值」，所以 GPU 就可以一口氣計算出所有 pixel 的顏色，因此就可以得到最終的結果了。\n圖片來源： 【OpenGL 篇】为什么游戏总要编译着色器？ 所以 GPU 之所以強大的地方，就在於「平行處理」，每一個 GPU 核心都有強大計算的能力，只要你告訴他要計算的是哪一格 pixel、以及要處理的數據有哪些，GPU 就可以為那一格 pixel 分配一個核心，專門去計算這個 pixel 的最終顏色，又因為 GPU 中的核心數量非常多，因此就可以達到「同時計算所有 pixel」的效果了～\n所以對於 GPU 來說，「平行處理」就是他的最強大的武器，而這也是 GPU 為什麼會在 AI 世界大放異彩的特質！\n為什麼 GPU 在 AI 時代這麼重要？ # 首先在 AI 時代中，所有的模型都是「訓練」出來的，而在訓練模型的過程中，使用 GPU 的「平行處理」就可以加快訓練的過程。\n「訓練」的概念有點像是在教導小孩子一樣，每一個模型一開始都是一個天真無邪的孩子，並且每個孩子的特長和天賦都不同，而我們工程師所做的，就是拿著「同樣的對話範例」去給這個孩子看，試試看這個孩子最終會長成什麼樣子。\n像是你可以拿下面這兩段對話，去訓練 X 模型，所以這時候 X 模型就會去「旁觀」這兩段對話，並且自己去理解這段對話（就像是小時候我們看著父母的言行一樣，就是從旁觀中去學習，所以身教大於言教啊！）。\nA：你好 B：哈囉你好，你吃飽了沒？ A：你好 B：心情不好，滾 而當你使用上面這兩段對話「訓練」完 X 模型之後，這時候當你跟 X 模型說「你好」時，他可能會回你：\n你：你好 X 模型：滾 或是 你：你好 X 模型：哈囉滾 因此在「訓練」模型的過程中，其實我們作為工程師，也是不知道 X 模型最終到底會回覆什麼的，一定得等到訓練完成之後，我們才能夠透過一問一答的測試的方式，去檢查 X 模型的回覆是否如我們預期。\n所以在 AI 的時代中，所有的模型其實都是通過「訓練」出來的，並且上面這種給 X 模型參考旁觀對話的過程，也稱為「訓練模型」。\n而 GPU 之所以在 AI 的發展中很重要，就是因為 「使用 GPU 的平行處理，就可以加快訓練的過程」。\n舉例來說，如果我們使用傳統 CPU 來訓練 X 模型的話，那就像是教育小孩一樣，一次只能夠教導一個科目，等到這個科目完成之後，才教導下一個科目。\n而如果我們使用 GPU 來訓練 X 模型的話，那就像是教育一個超級天才一樣，你可以一口氣教超級多科目，這個天才會用他的超級 GPU 腦袋，為每一個科目都分配一個核心，並且同時處理這些科目（就像前面的處理 pixel 一樣）。\n所以只要我們使用了 GPU，就可以「大幅提升訓練模型的速度」，因此就可以快速將 X 模型訓練完畢，進而檢查他的訓練成果了。\n補充：為什麼提升「訓練模型」的速度這麼重要？ # 這裡可能會有人有疑問，就是「為什麼提升訓練模型的速度這麼重要？」，就讓我緩緩道來我個人的訓練模型的經驗，跟大家分享一下我 train model 的血淚史\u0026hellip;.🥹（以下「訓練模型」會簡稱為 train model）。\n以前我在唸研究所的時候有稍微旁聽過 Machine Learning 的課（機器學習，算類 AI 吧），那時候有個作業要自己去 train 一個 model 出來，我當時就是調調調參數，然後按下 Enter 鍵放下去讓他跑，接著我就沒事幹了只能苦苦等著他跑完🥹。\n想當然第一次跑出來的結果一定是很爛，需要重 train，所以這時候我又只能重新調一下參數，然後再按下 Enter 鍵，重新用新參數去 train model，然後我就又只能再苦苦等一天\u0026hellip;.。\n所以 train model 真的是會花很多時間都在空等….，就像是你寫了一段程式，然後要等一天後你才能 debug 一樣，只有煎熬可言（而且進度也會進展的很慢），所以這也是為什麼 GPU 真的很重要。\n只要有一個好的 GPU，就可以讓 train model 的時間從 3 天降成 30 分鐘（是有點誇張的比喻，但大概是這個概念），同樣是 30 天的開發週期，有一個好 GPU 的工","date":"2025-01-21","objectID":"c93798d2b665f2b9cbbe43d0561e88bf","title":"GPU 是什麼？為什麼 GPU 對 AI 的發展很重要？","url":"https://kucw.io/blog/gpu-ai/"},{"categories":["其他技術分享"],"content":"Event Sourcing 的設計理念可以說是和一般常見的 CRUD 完全不同，我一開始聽到的時候也驚嘆「啊？？竟然還能這樣？？」，所以這篇文章我們就來了解一下，到底什麼是 Event Sourcing 吧！\n目錄 什麼是 Event Sourcing？ Event Sourcing 和傳統 CRUD 的差別在哪裡？ Event Sourcing 的優化 Event Sourcing 的優勢 Event Sourcing 總結 補充：Event Sourcing 和 DDD 補充 2：Event-Driven Design 是什麼？ 結語 什麼是 Event Sourcing？ # 所謂的 Event Sourcing，中文硬翻的話是「事件溯源」，重點在於「事件」這兩個字。\n而 Event Sourcing 的概念，其實就只是 「將每一筆發生的 Event（事件）都記錄下來」 而已，比想像中單純！\n舉例來說，如果大家去銀行存款/提款的話，銀行一定會記錄「你在某某時間存了多少錢」，因此當你後續去畫簿子（或是查看交易記錄時），就可以看到自己「每一筆存款/提款的時間、以及金額的大小」，而這種「把每一個 Event 都記錄下來」的設計方式，就稱為是 Event Sourcing。\n也因為在 Event Sourcing 中，我們所儲存的是「每一個 Event 的記錄」，而不是儲存「最終的餘額結果」，所以假設使用者想要知道他現在到底有多少存款的話，那我們就只能手動的一筆一筆 event 加總，也就是「把事件全部重頭執行一遍」，最終就可以得到他的最終餘額。\n所以像是在上面的例子中，我們就需要加總這 3 筆 event，也就是「存款 500」、「存款 1000」、「提款 200」，因此使用者最終的餘額就是 500 + 1000 - 200 = 1300。\n因此 Event Sourcing 說穿了，就只是將 「每一筆發生的 Event 忠實的記錄下來」 的架構模式而已！\nEvent Sourcing 和傳統 CRUD 的差別在哪裡？ # 大概了解了 Event Sourcing 的概念之後，我們也可以來比較一下 Event Sourcing 和傳統 CRUD 的差別在哪裡。\n由於 Event Sourcing 的特徵是「將每一個發生的 Event 都記錄下來」，因此在資料庫中我們所儲存的，是每一個 Event 發生的時間。優點是可以知道每一個 Event 的具體發生的時間，缺點則是沒辦法馬上知道最終的總額是多少，需要一筆一筆將 Event 全部加總之後，才能夠得到結果。\n而傳統 CRUD 則不一樣，傳統的 CRUD 是「直接將最終的結果儲存下來」，因此當使用者存錢時，CRUD 就是直接去修改使用者的當前餘額，將他修改成最終的數字。因此 CRUD 的優點是可以快速地知道當前餘額是多少，但是缺點是不知道使用者中間到底交易過幾次。\n也由於 Event Sourcing 和 CRUD 在本質上的不同，因此一般來說，通常是「對每一個 Event 都要詳細記錄的情境」才需要使用 Event Sourcing 來設計，像是金融業銀行、股票買賣、Audit Log\u0026hellip;等等，這種要細到每筆帳都要清算的情況，就很適合用 Event Sourcing 來實作。\n而如果沒有這種需求的話，其實用 CRUD 就真的很夠用了，因為一般來說大家在意的不是那個過程，而是最終的結果，所以這也是為什麼 CRUD 目前還是後端設計主流，就是因為 CRUD 可以應付大部分的真實場景。\n所以作為一個後端工程師，能夠熟練掌握最基本的 CRUD 能力還是非常重要的，基本功不能丟啊！\nEvent Sourcing 的優化 # 呈上面所說，因為 Event Sourcing 就是瘋狂的把每一筆發生過的 Event 給記錄下來，所以每次要得到最終結果的時候，都得要重新一筆一筆的把 Event 給加總，才能夠得到最終結果，但其實在實作上，是有一些手段可以加速的。\n像是我們可以在每天加總該天的 Event，最後生成一個當天的最終結果給使用者看，所以像是股票買賣記錄，就可以在當天晚上結算，先得到一個結果（術語稱為 snapshot），這樣後續就不用再重頭一筆一筆加總 Event 了，只要從今天的結果繼續往下加總明天的 Event 就好。\nEvent Sourcing 的優勢 # 補充：本段內容擷取自 高速大量業務的應用架構關鍵\n另外 Event Sourcing 也有一個超級強大的優勢，就是能夠 「提高後端系統的吞吐量」。\n因為 Event Sourcing 是瘋狂把每一筆 Event 記錄下來就好，所以他就不需要執行 SELECT SQL 語法，先查到要更新的是哪一筆數據，再進行更新，Event Sourcing 就只要直接無腦把 Event 數據 INSERT 到資料庫就好，因此資料庫就可以特意挑選那種擅長寫入的資料庫了！\nEvent Sourcing 總結 # 所以總結上面的介紹，所謂的 Event Sourcing，就是 「將每一筆發生的 Event 都記錄下來」，因此將來不管是要查詢使用者的哪一筆操作有問題、還是要回溯到當下的狀況，都可以用我們所儲存的 Event 數據來回溯。\n而 Event Sourcing 和 CRUD 最大的差別，就在於：\nEvent Sourcing 是「儲存每一筆 Event」。 CRUD 是「儲存數據的最終結果」。 因此大家就可以根據自己的情境，選擇最適合你的架構了～\n補充：Event Sourcing 和 DDD # 其實只要講到 Event Sourcing，就一定會提到跟他很有關係的 DDD。\n所謂的 DDD，全稱是 Domain-Driven Design（領域驅動設計），概念大概是將程式劃分成好幾個領域，每一個領域需要搭配一個領域專家，並且會有一個聚合根（Aggregate），負責該領域的所有事件和數據處理。\n上面這段文字看起來很難對不對？嗯其實我也覺得很難😂，感覺文字分開看我都懂，合在一起看就不太懂🥹。\n由於 DDD 我也只有略懂而已，所以大家如果有興趣的話，也可以再上網查詢「CQRS、Aggregate」\u0026hellip;等關鍵字，這邊就不班門弄斧了🙏。\n補充 2：Event-Driven Design 是什麼？ # 其實了解了 Event Sourcing 的目的是「儲存每一筆 Event 的記錄」之後，要了解另一個名詞 Event-Driven Design 就很容易了～\n所謂的 Event-Driven Design，中文翻譯為「事件驅動設計」，概念大概就是 「當某事件發生時，我們要執行什麼功能」。\n舉例來說，假設我們架設了一組 RabbitMQ，你就可以說「當某個 Queue 出現 message 時，我們就要更新資料庫」，而這其實就是一個 Event-Driven Design，也就是「當某個事件出現時，我們就執行什麼程式」這樣。\n另外如果大家有寫過前端的程式的話，其實整個前端的架構都是 Event-Driven Design，像是「當使用者點擊某個 Button，我就要執行某段程式」，或是「當使用者滑動視窗時，我要改變 Dom 的狀態」\u0026hellip;等等，整個前端其實就是超級大的 Event-Driven Design 架構。\n也因為 Event-Driven Design 的使用情境很廣泛（不僅前端廣泛使用，後端其實也會用到），所以建議大家要了解一下他的概念會比較好～。\n結語 # 這篇文章我們介紹了 Event Sourcing 是什麼，並且也比較了 Event Sour","date":"2025-01-14","objectID":"707e5baa57964af9f7bf09b59f5b0cdd","title":"Event Sourcing 是什麼？他和 CRUD 的差別在哪裡？","url":"https://kucw.io/blog/event-sourcing/"},{"categories":["職涯相關"],"content":"其實所謂的「資深工程師」，他並不是一個突然從天而降的改變，而是一個緩慢的、融入到你工作中的微小變化。\n可能是你今天多幫同事解決了一個 bug 可能是你今天又多學到了一些技術 可能是你以前每一個微小的功能所累積下來的穩定表現，讓主管願意相信你，將更重要的任務交給你 所以要成為資深工程師，並不是突然神蹟降臨，將你頭上的稱號從「初階工程師」升級成「資深工程師」這樣。\n而是會融入在每一天的工作生活中，你慢慢的變強，慢慢的累積周遭同事、主管對你的信任，慢慢的成為人們口中的英雄，這時候你突然回首一看，才發現自己其實已經變成資深工程師了。\n通常到這個時候主管就會約你 1-1，將你的職稱升到資深工程師，確保「你的職稱」和「你所具備的能力」是相符的，這時候就恭喜你～正式成為資深工程師啦🎉🎉\n所以不用急、不用太焦慮，只要慢慢的累積技術實力，不斷變強，每一次提交 commit 時都當成是出 bug 會嚴重到被電到飛高高的那種，戰戰兢兢的前進，慢慢的就會到達你想要的高度了～祝大家職場順遂！\nPS: 其實我覺得「成為資深工程師」和「談戀愛」很像😂，都是那種累積許多微小的改變，最終慢慢的喜歡上一個人這樣（可能我不是那種突然來個浪漫大餐、盛大告白就會心動的類型XD）。\n每一次的 commit 提交，就像是在觀察曖昧對象的每一個舉動一樣：\n可能是 coding style 不好（沒有整理儀容） 可能是架構設計的有問題（金錢觀、感情觀的落差） 可能是單元測試沒寫好出 bug（訂位餐廳但是沒有 double check 所以訂錯時間） 每一個小舉動，都影響著我是否對這個人有加分或扣分（當然對方也是用同樣的標準在審視我），最終當這個人的分數突破 100 分之後，就可以在一起了（就可以升遷了），大概是這種感覺吧XDD\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2025-01-08","objectID":"454c65d47d19ac3956676267cff8db83","title":"如何成為資深工程師？心態最重要","url":"https://kucw.io/blog/how-to-become-senior-developer/"},{"categories":["其他技術分享"],"content":"RAM 可以說是組裝電腦的必備元件，也是電腦運行程式的基礎，所以這篇文章我們就來介紹一下，電腦中的 RAM 到底是什麼吧！\n目錄 RAM（記憶體）簡介 RAM 的用途 RAM 和硬碟的差別 RAM 總結 補充：「重開機治百病」是什麼原理？ 結語 RAM（記憶體）簡介 # 所謂的 RAM，他的全稱是「Random Access Memory」，台灣通常翻譯為「記憶體」、大陸則是翻譯為「內存」，不過不管是記憶體還是內存，他們指的都是電腦中的 RAM。\n大家平常最接近 RAM 的情境，通常會是在買手機 or 買電腦的時候，像是在買電腦時，常常會聽到賣家宣傳：「這台電腦的記憶體有 16GB」或是「你要不要擴充記憶體到 16GB？」，這個記憶體所指的，實際上就是 RAM。\n像是下圖中就是一條常見的 RAM：（平常有在組桌機的人可能有摸過，但如果你是買筆電的話，就很少有機會能看到 RAM，因為廠商在生產筆電時，就已經把他嵌進筆電裡面了）\n那麼 RAM 到底可以幹嘛呢？RAM 真的是容量越大越好嗎？（當然越大也越貴），下面就介紹 RAM 實際的用途為何。\nRAM 的用途 # 不知道大家在寫程式時，有沒有曾經思考過：「這個程式到底是怎麼運行在電腦上的？」，譬如說，當我們在程式中使用一個變數時，這個變數的空間到底哪裡來的？實際上這些變數就是存放在 RAM 裡面。\n舉例來說，有一台電腦，他的 RAM 有 25 個空格：\n假設此時有一個 A 程式，他裡面宣告了 5 個變數（假設一個變數佔用一格），所以當這台電腦執行 A 程式時，A 程式就會去 RAM 中佔用 5 格（沒錯就是佔地為王），因此 RAM 就只會剩下 20 格可以給其他人用。\n假設這個時候，這台電腦又想去執行 B 程式，而剛好 B 程式裡面宣告了 15 個變數，因此 B 程式就會佔走 RAM 中的 15 格，因此 RAM 只剩下 5 格可以使用。\n而如果這個時候，這台電腦又想去運行 C 程式，但因為 C 程式裡面宣告了 20 個變數，因此 C 程式需要 RAM 中的 20 格才能成功運行起來。但是因為目前 RAM 中僅存的空格只剩 5 格（其他的被 A 程式和 B 程式佔走了），因此 RAM 就沒有足夠的空間能夠運行 C 程式，所以此時就沒辦法運行 C 程式。\n但如果這時候你還是很想運行 C 程式的話，那該怎麼辦呢？答案很簡單，你只要關掉 A 程式或是 B 程式，將 RAM 的空間釋放出來，就可以運行 C 程式了。\n舉例來說，因為 C 程式需要 20 格，所以在這個情境下，就需要關掉 B 程式，將 B 程式的 15 格釋放出來，這樣子才有足夠的空間運行 C 程式。\n所以這也是為什麼常常當我們 Google 分頁開太多時、或是 VS Code + IntelliJ + 其他 IDE 雙開時，電腦會開始有點卡卡的，這就是因為每一個分頁、每一個程式，他們實際上都在搶 RAM 的地盤啊！！！\n當 RAM 剩餘的空間不夠時，電腦就會開始卡卡的，所以這也是為什麼在買電腦的時候，一定會建議大家要把 RAM 從 8GB 升到 16GB，因為當你的 RAM 有 16GB 時，就表示你的 RAM 的空地比較大，因此就可以 「同時容納更多的程式」，重點在「同時」。\n所以換句話說，只要 RAM 越大，你就可以越瀟灑的運行更多的程式，再也不用擔心 RAM 的空間不夠用了，讚讚讚！！\nRAM 和硬碟的差別 # 了解了 RAM 的用途之後，接著我們可以看一下 RAM 的特性，以及 RAM 和硬碟的差別。\nRAM 有一個很重要的特性，就是「當電腦關機之後，RAM 中的所有數據就會全部被刪掉，無法救回」，所以像是我們在程式中所宣告的變數，因為變數是儲存在 RAM 裡面，所以如果沒有及時的把變數中的值儲存到資料庫的話，那麼當這台電腦關機時，該變數的值也就會跟著被刪掉了，因此即使你重新開機，也無法找回當初的變數的值。\n所以為了能夠有一個地方能「永久保存數據」，因此這時候就是「硬碟」派上用場的時候了！\n其實硬碟大家日常生活中應該用的滿多的了，像是我們可以在 Windows 的 C 槽中創建一個檔案、或是在 Macbook 中創建一個檔案，這些檔案所儲存的位置，其實都是儲存在硬碟上。\n而硬碟的特性，就是「即使電腦關機，裡面的數據仍舊會保存下來」，因此硬碟在電腦中所扮演的角色，就是一個永久的儲存空間，只要是關機後仍然想保留下來的數據，一定要儲存在硬碟中，這樣才不會隨著電腦關機而消失。\n因此電腦就是透過 RAM 和硬碟的搭配，來完成程式的執行和數據的儲存了！\nRAM 總結 # 所以總結上面的介紹，RAM 的全稱是「Random Access Memory」，中文翻譯為「記憶體」或是「內存」，並且 RAM 的用途是「用來儲存程式執行過程中的變數」，也就是提供足夠的空間讓程式能夠順利運行。\n而 RAM 和硬碟的差別就在於：\nRAM： 當電腦關機之後，RAM 中的所有數據就會全部被刪掉，無法救回。 硬碟： 即使電腦關機，裡面的數據仍舊會保存下來。 因此大家如果想要將程式中的計算結果永久儲存下來的話，記得一定要寫進硬碟裡面（不管是寫進檔案裡也好、還是寫進資料庫也好，就是要寫進硬碟就是了），不然的話那些數據就會隨著電腦關機，一起跟著消失了！\n補充：「重開機治百病」是什麼原理？ # 當大家了解 RAM 的概念之後，其實就可以理解「重開機治百病」的背後原理到底是什麼了🤣。\n不知道大家有沒有遇到過，不管遇到什麼問題，好像只要重新開機一下，就可以解決他，很有可能就是因為 RAM 出問題了。\n在前面有提到，RAM 的用途是「儲存程式執行過程中的變數」，而因為每個程式各自都會需要宣告一些變數，所以 RAM 需要提供足夠的空地給每一個程式。\n但是重點來了！！假設有某個程式在結束執行時，不好好的歸還他所租借的空地的話，那這樣 RAM 就會越租越少，到最後就會變成無地可租的情況。\n舉例來說，如果有一個 D 程式，他明明租借了 5 塊空格，但是在他程式執行完畢之後，他卻只歸還了 4 格，這樣子 RAM 的總額就會只剩下 24 格（原本是 25 格），進而降低了後續能租出去的空地，因此 RAM 就會越用越少，嚴重一點就會導致無空地可用，因此電腦就會整個卡住，沒辦法運行任何程式。\n如果沒有寫過較底層的程式語言（如 C、C++），可能會覺得「當程式結束了，變數仍舊會佔用空間」這件事情怎麼可能發生，但是事實上，這件事情是真的會發生的🥹。\n一般來說，較高階的程式語言（如 Java、Kotlin、Python、JavaScript、PHP…等），都會自動處理那些沒有用到變數，所以對於寫這些高階程式語言的工程師來說，當我們創建一個變數時，其實我們不需要思考「什麼時候要銷毀它」，我們只要創建它、然後用它來解決程式邏輯就好，根本不需要思考什麼時候要銷毀它，因為高階語言會自動幫我們去銷毀這些變數。\n但是對於 C 和 C++ 這類較底層的語言來說，當你創建一個變數時，你是得要記得去銷毀他的，如果你忘記銷毀的話….那麼你的 RAM 就會一直沒辦法被釋放，因此那格 RAM 就會永遠被佔用，即使你的程式已經停止運作了，那格 RAM 仍舊會一直被佔用。\n那假設那一格 RAM 一直被佔用該怎麼辦？目前只有一種手段可以解決，就是「重新開機」，因為重開機會強制刪除 RAM 中的所有數據，因此該格 RAM 就可以重新被釋放出來，讓其他人租借。\n所以這也是為什麼重開機可以解決許多問題的原因，根本原因就是因為有某個工程師程式沒寫好，老是寫出一些忘記銷毀的變數，使得 RAM 的可用空間越變越小，所以才會導致電腦越跑越慢🥹。\n所以下次當你的電腦又越跑越慢時，先","date":"2025-01-07","objectID":"499821c0d6bb84a8b28133623fbf96b2","title":"電腦中的 RAM（記憶體）的用途是什麼？他和硬碟的差別在哪裡？","url":"https://kucw.io/blog/ram/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，這篇文章是每月一次的自媒體月報，主要分享我經營《古古的後端筆記》個人品牌的幕後秘辛，如果你也對於「自媒體創業」有興趣的話，歡迎繼續閱讀本文～\n補充：如果想了解《古古的後端筆記》電子報的起源，也可以先查看創刊號的文章。\n2024.12 月粉絲追蹤數、電子報訂閱人數 # 本月份（2024.12）的粉絲成長人數如下：\n2024/11/26 人數 2024/12/31 人數 12 月份（11 月份）總成長人數 Facebook 粉專追蹤數 4389 4475 +86（+239） 電子報訂閱人數 2076 2480 +404（+444） Threads 粉絲追蹤數 4566 5996 +1430（+2009） IG 粉絲追蹤數 353 489 +136（+106） 各項指標仍舊成長中！可能目前還在流量成長期，所以我目前也會先朝著「盡量提高曝光度」努力，先讓大家「能看到我」、讓大家知道「我有在分享後端內容」，再由大家自由決定是否想要訂閱電子報。\n本月撰寫的電子報主題、文章 # 這邊記錄了我這個月撰寫了哪些電子報和文章（花了整個月在介紹 JWT😂），大家如果對於其中的內容有興趣的話，也可以前往查看：\nSession 和 JWT 的差別在哪裡？ 密碼學中的 Encode、Encrypt、Hash 的差別在哪裡？ JWT 是什麼？一次搞懂 JWT 的組成和運作原理 JWT 是如何實作「數位簽名」的？揭秘 signature 的面紗 2024 回顧 # 回顧 2024 年，這是我身為自由工作者的整整一年，在這個期間，我做了：\n錄製完成一門線上課程：Spring Security 零基礎入門 出版一本書：Spring Boot 零基礎入門 參加了幾場社群分享 還有就是，創立了這份電子報 老實說身為一個自媒體工作者，我其實是對我的 2024 年有點沮喪的😞，我深刻感受到一人工作在自律上的種種困難，以及很討厭自己的創作產能不是很穩定，心情很常會變得很低落\u0026hellip;（我想再次跟購買過課程的同學說聲抱歉😭，抱歉中間延期了那麼多次才完成所有課程影片\u0026hellip;）。\n有時候我會坐在電腦前面，明明已經想好要寫哪個主題，我也大概知道大綱要怎麼寫、素材要怎麼找，但是就提不起勁，最終就變成坐在電腦前發呆，然後那一天就這樣消失了\u0026hellip;。\n不過，即使 2024 年的我過得不是很好，但我仍舊想要試著改變，希望可以透過不同的形式，找回當初創作的快樂，而這個嘗試之一，就是創立了電子報！\n我也不知道為什麼，可能是寫電子報的負擔沒有錄製課程影片大，也可能是電子報的頻率比較低（一週寫一篇就好），目前電子報也已經寫了 20 期了，真的很感謝大家一路以來的訂閱和支持🙏。\n不過未來我還是會想錄製課程的！還有好多主題想要錄，而且我相信 「軟體的價值在於重複使用」，只要我好好的錄一次影片，這支影片就可以被許多人重複觀看，不斷的重複使用，超級讚！\n因此在 2025 年，我希望可以再錄製一堂線上課程出來，不過目前主題暫時還沒想好，等到確定了會再跟大家說～（只是有了前幾次錄製課程的教訓，這次我會盡力將進度安排得更合理一點的🥹）。\n2025 展望：Sharing is Learning! # 拍謝上面的 2024 年回顧好像有點沈重😂，只是想說既然是自媒體月報，就想要把自己真的經歷過的心情跟大家分享這樣，自由工作者其實真的沒有想像中輕鬆（壓力來自於不確定感和不安感），雖然是能每天睡到自然醒啦，不過長期下來的心情其實是沒辦法太放鬆的，大概就是「長期精神慢性病 + 睡到飽的健康身體」這樣。\n不過 2025 年展望，我們說點輕鬆的！！沒錯我要開始畫餅了🤣（展望就是拿來畫餅的對吧XD）\n在 2025 年，除了上面提到的「想要再錄製一堂線上課程」之外，2025 年我想用一句話來期許，就是 「Sharing is Learning!」（分享就是最好的學習！）。\n在新的 2025 年，我想學更多，也想分享更多！想要把學習到的知識分享給大家，也想要讓自己變得更強！！！\n老實說我不知道我還能寫多久（希望至少寫 3 年啦），但我相信我沒辦法寫一輩子（就算我能寫一輩子，我也沒辦法重現當下學習知識的快樂和成就感）。\n人的成長只有一次，我希望我能慢慢變強，我也希望能夠帶著大家一起變強，與其看著照片牆上一張張專業的攝影牆讚嘆不已，倒不如真的親身一步一步爬上山比較有趣對吧？\n希望 2025 年會是起飛的一年，大家坐穩繫好安全帶，我們一起變強💪！！！\n如果你喜歡這份電子報，歡迎分享給你的朋友、同事，揪他一起訂閱電子報，大家一起變強！\n電子報訂閱連結：https://kucw.io/bio/\n","date":"2025-01-07","objectID":"67b062c4eb6b086bb13b2a9d6e610e69","title":"軟體工程師的自媒體之路 - 2024.12 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202412/"},{"categories":["其他技術分享"],"content":"因應 Medium 的付費牆、以及 SEO 的政策調整，我發現我在 Medium 上的文章越來越難被搜尋到了🥹。\n有的人可能不知道什麼是 SEO（Search Engine Optimization，搜尋引擎最佳化），簡單的說，你 SEO 做得越好，這篇文章就可以在 Google 搜尋中排名越前面，而在 Google 搜尋中排名越前面，自然點閱率就越高，曝光度就越大（因為大家在找資料的時候，一定都是先點前幾個連結嘛）。\n所以在寫文章時，除了提升自己文章的內容之外，要怎麼樣讓自己的文章可以 「被看見」，也是非常重要的一件事！\n是 Medium 還是 Google 把我文章吃了？ # 鑑於 SEO 實在是太重要了，所以這也是為什麼我會離開 Medium 的主因。\n回想 Medium 剛起家的時候，他憑藉著自身簡約的網站風格、再加上 SEO 的優化政策做得非常好，所以有非常多的作者轉移到 Medium 上寫文章。\n但是近幾年 Medium 對 SEO 優化做了非常多調整，我自己曾在 Medium 上和 GitHub 個人網站上，分別寫了兩篇一模一樣的文章，但是當我用同樣的關鍵字去查詢的時候，GitHub 個人網站的文章可以排在前三名，而 Medium 的文章我翻到第 10 頁了還找不到，差距非常大。\n上圖為我使用同樣的關鍵字「lombok」去查詢的結果，我在自架的 GitHub 個人網站上寫的文章可以進到第 1 頁，但是 Medium 的文章卻連前 10 頁都找不到\nMedium 的文章連結：https://medium.com/@kujudy/java-lombok-cc4a2947ab5a\nGitHub 個人網站的文章連結：https://kucw.io/blog/2020/3/java-lombok\n老實說，當時看到這種情況我真的是哭笑不得，我沒想到 Medium 對 SEO 的影響會大到這麼誇張，如果是差個幾頁就算了，但是在 10 頁之內都找不到自己寫的文章，知道的當下還是有點沮喪的😞。\n有寫過文章的人就知道，產出一篇文章的背後，要花很多時間去研究、安排架構、調整用字遣詞，自己都花很多時間和熱情去寫文章了，但是卻因為這個 Medium 的 SEO 政策的關係，導致自己寫的內容沒辦法被別人看見，只能默默地在網路的世界不斷下沉\u0026hellip;\n想到這裡，就更加讓我下定決心要離開 Medium 了。\n下一站在哪裡？ # 有鑒於曾經在 Medium 上投注了心血，卻沒辦法得到相對應成果的痛苦經驗，所以這次我決定，我不要再受限在任何一個平台上了，因為難保下一個平台不會發生和 Medium 同樣的事情，如果這一次轉換平台，又碰到和 Medium 一樣的問題，那到時候就又只能崩潰的尋找下一個替代方案。\n在花了很多時間在網路上查了各種方法之後，最後我發現了在國外很熱門的 Hugo 架站工具，Hugo 在 GitHub 上有 76.9K 的 Stars，算是非常活躍的社群！\n只要使用這套 Hugo 架站工具，再搭配上 GitHub 提供的免費個人網站域名 GitHub Pages，這樣就可以架設出自己的個人網站了！聽起來好像滿簡單的，所以當時我就花了一些時間去看官方文件，因此就開始了我的自架個人網站之路。\n如何使用 Hugo 在 GitHub 上架設個人網站？ # 如果你本身有工程師背景的話，其實照著 Hugo 官方的文件做，再花一些時間 debug，很快就能在 GitHub 上架出個人網站了。\n但如果你完全沒有寫過任何一行程式的話，那要直接上手 Hugo 的難度還是滿高的，因為在使用 Hugo 的過程中，會有一些必要的背景知識，而這些是官方文件不會教你的。\n在使用 Hugo 架站之前，你必須要知道的幾件事：\n如何使用 brew or choco 去安裝 Hugo？ 如何修改程式碼？（工程師通常都有自己用習慣的編輯器，像我是 VS Code + IntelliJ 雙棲） 如何使用 Git，將程式碼上傳到 GitHub 上？ 如何撰寫 Markdown 文章？ 如果以上這四點你都了解的話，那麼直接看 Hugo 官方文件來架個人網站完全沒問題，但如果你對上面提到的 Git、Markdown 這些專有名詞有障礙的話，建議會需要先上網查一下相關資料，或是參考下面的線上課程。\n工商時間 | 一起在 GitHub 上架設個人網站吧！ # Hahow 線上課程：Github 免費架站術！輕鬆打造個人品牌， 輸入折扣碼「HH202506KU」即可享 85 折優惠\n如果你沒有工程師背景，但也想擺脫各大文章平台的束縛，擁有一個自己的個人網站的話（並且還是免費的！），也許這門線上課程就是你在找的引路人。\n本課程會一步步的從零開始教學，介紹要如何使用 Hugo 以及 Github Pages，架設出屬於你的個人網站，因此當你上完這門課時，你的個人網站也已經完成了！\n架設好自己的個人網站，然後呢？ # 架設出自己的個人網站之後，我才發現可以玩的東西多太多了！\n1. 文章終於可以分類了 # 光這點我覺得就完勝 Medium，我真的很不解為什麼 Medium 不願意提供一個分類的機制給大家使用，難道 Medium 覺得我們寫的文章都很整齊不用分類整理嗎！！！\n2. 可以添加個人經歷頁面，讓網站更豐富 # 以前在 Medium 寫文章時，只能設定一些簡單的自我介紹，但是如果是自架個人網站的話，就可以自由發揮，用圖形化的方式去呈現一些個人經歷，同時也可以去整合自己的其他作品，讓別人進到這個網站，就可以了解你過去所有的經歷、作品集、以及所寫的所有文章。\n不過要達到這個效果，會需要具備一些前端的 Html、Css\u0026hellip;等知識，但是如果很熟悉前端的知識的話，做出來的網站豐富度真的不是 Medium 可以比的，完勝 Medium！\n3. 使用 Google Search Console 查看搜尋成效 # 這個是 Google 提供的很酷的功能，我也是架了個人網站才接觸到這個功能。\n當架設好個人網站之後，可以到 Google Search Console 裡，申請在 Google 搜尋引擎上註冊你的個人網站，只要申請成功之後，這樣其他人就可以透過 Google 搜尋找到你的個人網站了！\n並且申請了入住 Google 搜尋之後，後續你也可以透過 Google Search Console，去查看個人網站的搜尋成效，譬如說我可以查看某段時間內，網頁的 曝光量 / 點擊量 有多少，或是網頁的點擊率是多少。\n如果網頁點擊率低的話，就表示別人在 Google 搜尋結果中看到我的文章，但是卻沒有點進來看內容，那原因可能就是因為我的文章標題下得不夠吸引人，所以我就可以朝這個方向去修改我的文章標題。\n4. 使用 Google Analytics（GA）查看網站成效 # 這個也是 Google 提供的功能，不過 GA 的名氣就比 Google Search Console 大多了，一般前端工程師和行銷的職位都會碰到 GA。\n架設好個人網站之後，可以到 Google Analytics 申請使用 GA 的服務，接著就可以使用 GA 去查看個人網站的流量了。\n5. 使用 Google AdSense 收取廣告費 # Google 真的是佛心來著，不只提供了許多好用的功能給我們使用，現在竟然還可以讓我們靠寫文章來賺錢！！\n大家在瀏覽網站的時候，常常會看到有的網站下面有一個「Google 提供的廣告」對吧？這個就是 Google Adsense 的功能。\nGoogle AdSense 這個功能簡單的說，就是你將個人網站中的一塊地租給 Google，而 G","date":"2025-01-01","objectID":"aec97ec3769fafde70d3e2508c4c802b","title":"為了 SEO！我離開了 Medium，改在 GitHub 上自架個人網站","url":"https://kucw.io/blog/2021/1/from-medium-to-github/"},{"categories":["其他技術分享"],"content":"在上一篇的 JWT 是什麼？一次搞懂 JWT 的組成和運作原理 文章中，我們有介紹了 JWT 是由三個部分所組成，分別是：header、payload 以及 signature。\n而在 header、payload、signature 這三個部分中，他們各自負責的功能如下：\nheader： 用來設定這個 JWT 的簽名算法、以及此 Token 的類型 payload： 用來儲存使用者的數據，可以根據自己的需求進行客製化 signature： 用來儲存「簽名」的計算結果 因此這篇文章我們就會接著來探討，為什麼 JWT 透過 signature 的部分就可以實作出「數位簽名」的概念，並且也會介紹「數位簽名」到底是有多神奇，可以直接阻擋駭客的竄改。\n本文為 JWT 系列文的最終章，會使用到前面所提到的所有概念（包括 JWT 的組成、非對稱加密、Hash），因此如果你對這些概念還不太熟悉，建議可以先回頭參考前面文章的介紹（建議按照順序閱讀，才會有最佳的閱讀體驗）：\nSession 和 JWT 的差別在哪裡？ 密碼學中的 Encode、Encrypt、Hash 的差別在哪裡？ JWT 是什麼？一次搞懂 JWT 的組成和運作原理 JWT 是如何實作「數位簽名」的？揭秘 signature 的面紗（本文） 目錄 signature（數位簽名）的運作邏輯 前置作業 JWT 的生成 JWT 的驗證 JWT 的數位簽名總結 結語 signature（數位簽名）的運作邏輯 # 前置作業 # 如果要實作 JWT 的數位簽名的機制，最常見的做法是使用 「非對稱加密」 來實作，所以在實作前，後端需要先生成一組「公鑰」和「私鑰」出來，等等就會使用這兩組密鑰來搭配實作「數位簽名」的功能。\n而當後端生成好一對公鑰和私鑰之後，就可以開始來進行 JWT 的生成了！\n補充：在非對稱加密中，公鑰是公開的、可以廣發給所有人，私鑰則是私有的，要自己保管好，絕對不可以洩漏。並且使用公鑰來加密的數據，只能使用私鑰來解密；使用私鑰來加密的數據，只能使用公鑰來解密。\nJWT 的生成 # 在生成 JWT 時，首先後端要先根據自己的商業邏輯，填上 header 和 payload 中的值。\n所以像是在下方的 payload 中，我就填上了 {\u0026quot;sub\u0026quot;:\u0026quot;123\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;古古\u0026quot;...} 的資訊，用來表示這個 JWT 中所裝的使用者資訊，是名叫「古古」的使用者資訊。\n這裡大家可以根據自己的需求，在 payload 中填上你想要的值，不管你在 payload 中填上什麼值，對後續的 signature 生成都不會有任何影響（只不過在生成 signature 之前，一定要先確定好 payload 中的值要填什麼就是了，一旦 signature 生成之後，就不能改 payload 中的值了）。\n而當寫好 header 和 payload 中的值之後，此時後端需要對 header + payload 的值 Hash 一下（此處使用的是 SHA-256 Hash 演算法），所以此時就會計算出 header + payload 被 Hash 過後的結果，即是 srtp3k。\n接著後端要使用「私鑰」，將這個 hash value srtp3k 加密，因此就會得到一個加密過後的結果 ap9fPl...。\n而當後端產生出 ap9fPl... 這個加密過後的結果之後，後端必須將這個值放在 JWT 中的 signature 的欄位中，因此最後後端傳送出去的 JWT 結果，就會在 signature 的部分填上 ap9fPl... 的加密過後的值。\n所以到這裡，就完成了第一階段的「JWT 的生成」了，因此此時後端就可以將這個 JWT Token 傳送給前端，交由前端保存這個 JWT Token。\nJWT 的驗證 # 而當前端後續拿著這個 JWT 來請求 api 時，我們身為後端，就必須要驗證這個 JWT 的正確性（即是是否有被駭客竄改過），而要驗證 JWT 的正確性，就是去檢查其中的 signature 的值就可以了。\n舉例來說，當前端拿著剛剛的那一個 JWT 來請求 api 時，這時候我們身為後端，必須要先對當下的 header + payload 的區塊進行 Hash，先計算出 header + payload 的 Hash 值，因此此時就會得到一個 Hash 結果（即是下方區塊中的 srtp3k）。\n這時候 JWT 中最精華的地方來了！！！當後端計算出 header + payload 的 Hash value 之後，後端只要使用「公鑰」將 signature 解密，然後比較一下「當初所計算出來 Hash 的值」和「現在所計算出來的 Hash 值」是否一樣，就可以知道此 JWT Token 是否有被竄改過了！\n像是假設駭客偷偷修改 name 的值，將他從 古古 改為 我是駭客 的話，那麼當後端在收到這個 JWT 時，當下所計算出的 header + payload 的值就會是 Fy93HE，而這個值就會和「使用公鑰解密 signature 所得到的原始 Hash 值 srty3k」不一樣，因此 JWT 就會驗證失敗，所以後端就會認為這個 JWT 中的內容有被偷偷修改過，因此就不相信裡面的數據。\n所以總結來說的話，當後端收到 JWT 時，後端只要驗證一下 「當下 header + payload 所計算出來的 Hash 值」 和 「使用公鑰解密 signature 所得到的值」 是否一樣，就可以知道這個 JWT 有沒有被偷偷修改了，這就是「數位簽名」的核心運行邏輯啦！！\n所以大家以後就可以透過 JWT 的數位簽名的特性，將 JWT 直接儲存在前端中，後端就只要在收到 JWT 時驗證一下簽名就好，因此就不需要再儲存大量的數據在後端裡面了！讚！！\nJWT 的數位簽名總結 # 所以總結上面的介紹，JWT 之所以可以透過 signature 的「數位簽名」來避免駭客的竄改，就是因為他使用了「非對稱加密」來實作數位簽名的機制。\n而我們身為後端，在生成 JWT 時，需要使用「私鑰」來加密 header + payload 的數據，並將結果放在 signature 中。\n並且在驗證前端傳過來的 JWT 時，需要使用「公鑰」來解密 signature 得到原始值，並且比較他和「當下 header + payload 所計算出來的 Hash 值」是否一樣，如果一樣的話，才表示 JWT 驗證成功，裡面的數據沒有被駭客所竄改，因此可以相信裡面的數據。\n因此大家就可以透過這個步驟，在你的後端程式中實作 JWT 的相關程式了！\n補充：本文是擷取自我開設的線上課程「資安一把罩！Spring Security 零基礎入門」的內容，如果你想了解更多的資安內容、以及如何應用 Spring Security，歡迎參考課程簡介 （輸入折扣碼「HH202506KU」即可享 85 折優惠）。\n結語 # 這篇文章我們介紹了 JWT 的「數位簽名」的核心運作邏輯是什麼，數位簽名可以說是 JWT 中最重要的特性，就是因為有了數位簽名，才使得我們可以將 JWT 儲存在前端 Client 中，打破了以往只能將數據儲存在後端的設計方式，可以說是超級厲害的發明！！\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n本文為 JWT 系列文的最終章，如果你想回顧前面的介紹，也可以參考前面的文章：\nSession 和 JWT 的差別在哪裡？ 密碼學中的 ","date":"2024-12-24","objectID":"7eb1dd8c7bb1414a121ff19a83fe8216","title":"JWT 是如何實作「數位簽名」的？揭秘 signature 的面紗","url":"https://kucw.io/blog/jwt-signature/"},{"categories":["其他技術分享"],"content":"在現今的架構中，JWT 可以說是使用非常廣泛的一項技術，所以這篇文章我們就來介紹一下 JWT 是由哪些部分所組成，以及每個部分所負責的功能為何吧！\n本文為 JWT 系列文之一，建議在閱讀本文前，要先了解 Session 和 JWT 之間的差別、以及 Encode/Decode 的概念，才會比較好上手。如果你對這些概念還不太熟悉，也可以參考前面的文章介紹（建議按照順序閱讀，才會有最佳的閱讀體驗）：\nSession 和 JWT 的差別在哪裡？ 密碼學中的 Encode、Encrypt、Hash 的差別在哪裡？ JWT 是什麼？一次搞懂 JWT 的組成和運作原理（本文） JWT 是如何實作「數位簽名」的？揭秘 signature 的面紗 目錄 回顧：Session 和 JWT 的差別 什麼是 JWT？ JWT 中的 header 部分：存放通用資訊 補充：JWT 中的 Encode 和 Decode JWT 中的 payload 部分：存放使用者的數據 補充 1：payload 中的常用變數 補充 2：不要在 payload 中存放敏感數據 JWT 中的 signature 部分：實作數位簽名 JWT 總結 結語 回顧：Session 和 JWT 的差別 # 在 Session 和 JWT 的差別在哪裡？ 的文章中，我們有介紹到 Session 和 JWT 的差別分別是：\nSession 認證： 將登入的資訊儲存在「後端 Server」上 JWT 認證： 將登入的資訊儲存在「前端 Client」中 所以大家就可以根據自己的需求，選擇到底要使用 Session 來進行認證、還是使用 JWT 進行認證。\n不過在當時的那篇文章中，我們先跳過了「為什麼後端可以無條件相信 JWT 中的內容？」的相關細節，因此這篇文章就會回過頭來介紹這部分，了解 JWT 中的組成有哪些，以及 JWT 的運作原理是什麼，使得後端可以無條件相信 JWT 中所記錄的資訊。\n什麼是 JWT？ # 所謂的 JWT，他的全稱是 JSON Web Token，而他的用途，通常就是用來「傳遞身份認證的數據」。\n在 JWT 中，會由三個部分所組成，分別是：header、payload 以及 signature，並且在每一個部分之間，就會使用一個 . 作為分隔。\n像是在下面的例子中，左邊的 eyJhb.... 這段亂碼，他其實就是一個 JWT Token，所以他就可以根據其中的 . 區分成三個部分，也就是 header、payload 以及 signature。\n所以其實 JWT 格式，他就只是一段亂碼，只是我們可以使用 .，把內部的資訊區隔成三個部分而已。\n而如果我們把下面這段 JWT 的亂碼，貼到 JWT 的官方網站 https://jwt.io/ 中的話：\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjMiLCJuYW1lIjoi5Y-k5Y-kIiwiaWF0IjoxNzIxMDAxNjAwfQ.G41XQGNNJ5Tp88U48aXh4n0XtGkpPkQ3xK6j43_61gE309hzyTVyciG5v05aVIvvY9NrApYiQdvwlMMrjRPFVV8xunghtKKFMj3kPx93Ll8Pf6n-tDiL_NZYqcusrgwtb-EDza80hMG5PTu75ogTIfRKr4jC0_FZzLaMix07LaZReoUSionTWTxJlm8qJc0BAFXgsaGNs9oVhCXOg_jJmOfFZBP0tD3q4xaKp9MTtLRTtslAhoAjPczdnPqaWGcaS8OY11RUTvvxijA7W-mPRlmqt0Hd_XForETUFZRdCKsPQIiGjkavycPtdiViVihQKstHlT4afEzYvzWSeK1cnw 就可以看到這個網站就會將此 JWT 分成三個部分，也就是剛剛提到的 header、payload、signature。\n所以在了解 JWT 的運作原理之前，一定要先知道 「JWT 是由 header、payload 和 signature 這三個部分所組成」，每一個部分所負責的功能都不一樣，所以先了解 JWT 的結構是非常重要的！\n而在 JWT 中，每一個部分所負責的功能都不一樣，以下分別介紹他們各自所負責的功能：\nJWT 中的 header 部分：存放通用資訊 # 首先 header 的用途，就是用來存放一些通用的資訊，譬如說可以用來設定這個 JWT 的簽名算法、以及設定這個 JWT 的 Token 類型。\n舉例來說，像是在下方的 header 例子中，裡面的 alg 就是用來設定這個 JWT 的簽名算法，而 typ 則是表示這個 Token 是 JWT 類型。\n{ \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } 所以在 header 中，主要就是用來設定這個 JWT 的通用資訊。\n補充：JWT 中的 Encode 和 Decode # 而大家如果觀察一下的話也可以發現，在 JWT 的官網中（https://jwt.io/），當我們在左側貼上 JWT 的亂碼之後，在右邊所呈現的會是 Decode（解碼）過後的結果。\n像是以 header 的亂碼 eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9 為例，將他 Decode 之後，就可以得到原始的 JSON 字串 {\u0026quot;alg\u0026quot;:\u0026quot;RS256\u0026quot;,\u0026quot;typ\u0026quot;:\u0026quot;JWT\u0026quot;}。\n所以在 JWT 中，header 的亂碼看似好像有加密，但他實際上是完全沒有加密的！！！JWT 只是使用 Base64 將 JSON 字串給編碼一下而已，完全沒有加密的成分在裡面，因此所有人都有能力 Decode 這段亂碼，取得原始的 JSON 字串。\n補充：不熟悉 Encode 和 Decode 的概念的話，可以回頭參考 密碼學中的 Encode、Encrypt、Hash 的差別在哪裡？ 中的介紹。\nJWT 中的 payload 部分：存放使用者的數據 # 了解了 header 的用途之後，接著我們可以來看一下 payload 的用途。\npayload 的用途，就是用來存放使用者的數據，所以在 payload 中，我們就可以根據自己的需求，自由決定我們想要放什麼資訊在裡面。\n所以像是在下面的 payload 例子中，裡面就存放了 \u0026quot;name\u0026quot;: \u0026quot;古古\u0026quot; 的數據，用來表示這個使用者的名字為「古古」，因此我們就可以根據自己的需求，在 payload 中存放客製化的使用者數據了！\n{ \u0026#34;sub\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;古古\u0026#34;, \u0026#34;iat\u0026#34;: 1721001600 } 補充 1：payload 中的常用變數 # 在客製化 payload 的數據時，我們也是可以使用 JWT 預先定義好的變數名稱來實作的。\n舉例來說，sub 所代表的就是「使用者的 id 的值」，至於 iat 所代表的就是「這個 JWT Token 的創建時間」，這些都是 JWT 預設的變數名稱，算是使用 JWT 的一個默契。\n所以大家在使用上，除了自己定義變數名稱之外，建議也可以多多利用 JWT 所定義的變數名稱，這樣就可以讓不同工程師所設計出來的 payload 長得差不多，就不用再每次都一個一個詢問「這個變數名稱的含義","date":"2024-12-17","objectID":"c1bdb26661440edca46b780306f991c3","title":"JWT 是什麼？一次搞懂 JWT 的組成和運作原理","url":"https://kucw.io/blog/jwt/"},{"categories":["其他技術分享"],"content":"在密碼學中，Encode、Encrypt、Hash 可以說是三大基礎概念，並且也是後端工程師必備的密碼學知識，因此這篇文章就會詳細來介紹什麼是 Encode、Encrypt、Hash，以及比較他們之間的區別。\n本文為 JWT 系列文之一，如果你對 JWT 的其他文章有興趣，也可以直接點擊下列連結跳轉到該文章（不過建議還是按照順序閱讀，才會有最佳的閱讀體驗）：\nSession 和 JWT 的差別在哪裡？ 密碼學中的 Encode、Encrypt、Hash 的差別在哪裡？（本文） JWT 是什麼？一次搞懂 JWT 的組成和運作原理 JWT 是如何實作「數位簽名」的？揭秘 signature 的面紗 目錄 Encode、Encrypt、Hash 簡介 什麼是 Encode（編碼）？ 什麼是 Encrypt（加密）？ 什麼是對稱加密？ 什麼是非對稱加密？ 什麼是 Hash（雜湊）？ 補充：Hash 真的沒辦法被破解嗎？ 補充 2：我還是很想用 MD5，真的沒辦法了嗎？ Encode、Encrypt、Hash 總結 結語 Encode、Encrypt、Hash 簡介 # 在密碼學中有三大概念，分別是：\nEncode（編碼） Encrypt（加密） Hash（雜湊） 以下分別介紹如何透過這三種不同的方法，去對數據做不同的處理。\n什麼是 Encode（編碼）？ # 所謂的 Encode（編碼），就是「數據可以直接被編碼」，並且中間「不需要」任何的密鑰參與。\n像是在下面的例子中，左邊的原始字串 123，就可以被 Encode（編碼）成右邊的 gg2oGVzdD，而右邊的 gg2oGVzdD，他也是可以直接被 Decode（解碼）回原始的字串 123 的。\n因此在 Encode 和 Decode 的過程中，是不需要任何的密鑰，「所有人」 都可以輕易的 Encode 和 Decode。\n而一般在實務上，最常見的 Encode 演算法為「Base64」，像是在這個 Base64 Encode 的網站中，只要在上面輸入 test123，下面就會出現 Encode 過後的結果 dGVzdDEyMw==。\n而這時如果大家複製一下下方的結果 dGVzdDEyMw==，然後點擊左邊的 Decode 連結（跳轉到 Decode 的頁面），然後將剛剛的結果 dGVzdDEyMw== 貼在上面的話，這時候就可以直接將這個結果 Decode 回原始的字串 test123。\n所以對於 Encode 和 Decode 來說，他們完全不需要密鑰的參與，也可以對數據進行「編碼和解碼」，因此所有人都可以針對某一筆數據進行 Encode 和 Decode。\n也因為如此，大家在使用 Base64 去 Encode 數據時，一定要注意這個數據是沒有被加密過的，所以所有人都有能力將他 Decode 回原始的字串！！一定要小心！！！\n所以下次當你看到有人在網路上隨意散播使用 Base64 Encode 的數據時，你完全可以大膽的將他 Decode，就可以得到原始的字串，因此千萬不要再誤解 Base64 很安全了🥹，他真的就只是一個編碼的工具而已，完全沒有任何加密的成分在裡面！\n補充：Encode 其實嚴格上來說不算加密，因為他只是改變數據的呈現方式，完全沒有加密可言，但口語上有時候還是會把 Encode 歸類成加密，因此才放在這裡一起介紹。\n什麼是 Encrypt（加密）？ # 了解了 Encode 之後，接下來我們可以來看一下 Encrypt 的相關介紹。\n所謂的 Encrypt（加密），就是「將數據加密成密文」，所以透過 Encrypt 加密過後的數據，他可以說是非常安全的。\n而在 Encrypt 的世界中，又可以再細分成兩種加密方式，分別是：\n對稱加密 非對稱加密 所以以下針對這兩種加密方式來介紹。\n什麼是對稱加密？ # 在對稱加密中，只會有「一把」密鑰存在，因此不論是加密還是解密，就統統是用這把密鑰來進行。\n舉例來說，左邊的原始的字串 123，他就可以透過這把密鑰（或是簡稱為這把 key），加密成右邊的 gg2oGVzdD，而右邊的 gg2oGVzdD，他也可以透過這把密鑰，解密回原始的字串 123。\n所以在對稱加密中，就只會存在「一把」密鑰，因此不管你是要加密還是解密，統統就是透過這把密鑰進行。\n也因為在對稱加密中只有一把密鑰存在，所以這把密鑰必須要好好的保護，絕對不可以洩漏，一但密鑰洩漏，駭客就可以透過這把密鑰解密出原始的字串，因此在對稱加密中，保護密鑰是非常重要的任務！\n什麼是非對稱加密？ # 而不同於對稱加密，在非對稱加密中，則是會有「兩把」密鑰存在。\n在這兩把密鑰中，其中一把會叫做「公鑰」（也就是 Public Key），另一把則叫做「私鑰」（也就是 Private Key），因此在非對稱加密中，就是由「公鑰」和「私鑰」共同配合，對數據進行加密和解密。\n所以這兩把密鑰，他們的特性如下：\n公鑰（Public Key）：公開的，可以複製好幾份，廣發給所有人 私鑰（Private Key）：私有的，必須要好好的保護，不可以洩漏 並且在非對稱加密中，有一個很神奇的運作邏輯，也就是：\n由 「公鑰」 所加密的數據，只能夠由 「私鑰」 解密 由 「私鑰」 所加密的數據，只能夠由 「公鑰」 解密 所以換句話說的話，就是 「由其中一把鑰匙所加密的數據，只能夠用另一把鑰匙來解密」，這就是「非對稱加密」的運作方式。\n也由於非對稱加密的神奇特性（其中一把加密的數據，只能用另一把解密），所以在 JWT 中的「數位簽名」，實際上就是用非對稱加密所實作的。甚至不止 JWT，像是區塊鏈中的數位簽名，他背後也是用非對稱加密實作的（不如說只要是扯到「數位簽名」或是「數位簽章」這個概念，底層都是同一套，都是用非對稱加密實作）。\n所以在了解 JWT 的「數位簽名」是如何實作的之前，建議大家一定要先了解「非對稱加密」的運作邏輯（也就是其中一把加密的數據，只能用另一把解密），這樣子後續在了解 JWT 的相關知識時，才能夠知道他底層的運作邏輯為何。\n什麼是 Hash（雜湊）？ # 在了解了 Encrypt 中的「對稱加密」和「非對稱加密」之後，最後我們也可以來看一下 Hash 的相關介紹。\n所謂的 Hash（雜湊），就是「單方面的將數據轉換成亂碼（或稱為 Hash Value，雜湊值）」，並且這個 Hash Value，他是「不能夠」轉換回原始的字串的。\n舉例來說，左邊的原始字串 123，他可以被 Hash 成右邊的亂碼 gg2o861kdn5（又稱為 Hash Value，雜湊值），但是我們卻沒辦法將右邊的亂碼 gg2o861kdn5，轉換回原始的字串 123。\n所以在 Hash 的世界中，所有的數據只要被 Hash 過，就再也沒辦法轉換回原始的數據，就算老天來也沒用，世界上沒有人（包含駭客和你自己）有能力將 Hash Value 轉換回原始的數據。\n補充：Hash 真的沒辦法被破解嗎？ # 從根本的邏輯上，只要一個數據被 Hash 過，那我們是真的沒有辦法將這個數據轉換回原始的字串，但是！！駭客就想到一招破解的方式：「既然我沒辦法將數據轉換回來，那我乾脆提前做一張很大很大的表格，提前記錄 什麼樣的數據 會轉換成 什麼樣的亂碼 不就好了？」\n舉例來說，駭客可以提前計算出 123 這個字串會被 Hash 成 gg2o861kdn5、456 這個字串會被 Hash 成 aaaa777k3nt\u0026hellip;等等的結果。因此當駭客發現某一筆被 Hash 過的值是 aaaa777k3nt 時，駭客就只要回來查詢這張表格，就可以馬上找出 aaaa777k3nt 的原始字串是","date":"2024-12-10","objectID":"a601351e37d769ac312a94d4d0166cef","title":"密碼學中的 Encode、Encrypt、Hash 的差別在哪裡？","url":"https://kucw.io/blog/encode-encrypt-hash-intro/"},{"categories":["其他技術分享"],"content":"在現今的架構中，JWT 可以說是使用非常廣泛的一項技術，但是 JWT 到底是什麼？以及他的用途為何？甚至 JWT 和 Session 的差別在哪裡？這些都是在了解 JWT 的過程中，需要具備的知識。\n因此在這個 JWT 系列文中，我會用 4 篇文章介紹 JWT 的前世今生，包含在 JWT 的用途是什麼、他要解決的是什麼問題、以及 JWT 的底層運作邏輯為何。\n所以首先這篇文章，我們就先來介紹 Session 和 JWT 的運作原理為何，以及並且比較他們之間的差別。\n目錄 什麼是 Session 認證？ 什麼是 JWT 認證？ 補充：後端為什麼能無條件相信 JWT？ 小結：Session 和 JWT 的運作邏輯 Session 和 JWT 的優缺點比較 使用 Session 的優點和缺點 使用 JWT 的優點和缺點 到底使用 Session 好，還是使用 JWT 好？ 結語 什麼是 Session 認證？ # Session 和 JWT 可以說是現今主流的兩種登入認證機制，不過在開始比較 Session 和 JWT 的差別之前，首先我們要先來介紹一下什麼是 Session 身份認證機制。\n所謂的 Session 認證，就是「將登入的資訊儲存在後端 Server 上」的一項技術，所以換句話說的話，就是後端 Server 會儲存這個使用者的登入資訊，並且回傳一個 Session id 給使用者。\n舉例來說，假設我是一個使用者，我的帳號密碼是 Judy / 123，這時候當我在網站上輸入帳號密碼，並且嘗試登入時，首先後端 Server 會先去驗證我的帳號密碼 Judy / 123 是否曾經註冊過（畢竟得先註冊過才能登入），假設這組帳號密碼存在的話，後端 Server 就會生成一個 Session id（也就是 66EE89C175E），並且將 Session id 存放在資料庫中，然後將 Session id 回傳給前端。\n所以當前端（此處以瀏覽器為例）收到 Session id 的值時，前端就可以將這個 Session id 的值存放在 Cookie 中，等待後續使用。因此到這裡就完成了登入的操作，所以使用者理論上就會被跳轉回首頁，並且已經是成功登入的狀態。\n而當使用者成功登入之後，假設使用者後續想要去 call 其他受保護的 api（ex：查看個人設定），這時候前端就可以在 call api 時，同時也帶上當初後端 Server 發放的 Session id，這樣後端在收到這一次的請求時，就可以去資料庫查詢一下這個 Session id 的值是否存在，如果存在的話，就表示這個使用者已經有成功登入過了，所以就可以允許這一次的 api 通過，進而返回實際的結果給前端。\n所以對於 Session 這種登入機制而言，所有的登入記錄都是儲存在「後端 Server」上，前端所拿到的 Session id 的值，其實就只是一組沒有意義的亂碼，只有後端 Server 才知道這組 Session id 實際上對應到的是哪個使用者的登入資訊。\n因此 Session 這種登入機制，他就是「將登入的資訊儲存在後端 Server 上」的一項技術。\n什麼是 JWT 認證？ # 了解了什麼是 Session 的登入機制之後，接著我們可以來介紹什麼是 JWT 的登入機制。\n所謂的 JWT 認證，就是「將登入的資訊儲存在前端 Client 中」的一項技術，所以換句話說的話，就是會將使用者的登入資訊，直接儲存在前端的 Client 中，因此在後端 Server 中是不會儲存任何一筆登入資訊的！\n這聽起來可能有點神奇，不過 JWT 的運作邏輯是這樣子的：\n和前面一樣的例子，假設我是一個使用者，我的帳號密碼是 Judy / 123，這時候當我在網站上輸入帳號密碼，並且嘗試登入時，首先後端 Server 一樣是會先去驗證我的帳號密碼 Judy / 123 是否曾經註冊過。\n但是這裡重點來了！！假設這組帳號密碼存在的話，後端 Server 就會生成一個 JWT 格式的 Token，並且會直接返回該 Token，而且「不需要」在後端 Server 中儲存這個 Token 的數據。\n所以在 JWT 認證的世界中，後端 Server 是不需要儲存任何一筆 JWT 的數據的！！！後端 Server 要做的事情，就是生成 JWT Token，然後直接回傳就好，就是這麼簡單暴力！！！\n這時候你可能會想：這樣到時候要怎麼驗證這個 JWT Token 是有效的？這時候就是展現 JWT 的神奇地方之處了！！\n當使用者成功登入之後，假設使用者後續要想去 call 其他受保護的 api，這時候前端就可以在 call api 時，同時也帶上當初後端 Server 發放的 JWT Token，而當後端收到這個 Token 時，只要這個 JWT 的簽名驗證成功，後端就會無條件相信 JWT Token 中的內容，也就是 JWT Token 聲稱這個使用者的名字叫做 Judy，那後端就無條件相信這一次來請求的人就叫 Judy；JWT Token 聲稱這個使用者的 id 是 111，後端就無條件相信這個使用者的 id 是 111。\n所以在 JWT 認證的世界中，後端就是這麼的天真可愛，只要 JWT 說什麼，後端就無條件相信他，完全不需要質疑 JWT 的安全性，因此在 JWT 的認證機制中，所有的登入記錄都是儲存在「前端 Client」上，前端所拿到的 JWT Token，就是一個包含所有使用者資訊的 Token，因此只要能夠正確解讀這個 JWT Token，就可以拿到使用者的所有資訊。\n所以 JWT 這種登入機制，他就是「將登入的資訊儲存在前端 Client 上」的一項技術。\n補充：後端為什麼能無條件相信 JWT？ # 第一次接觸 JWT 的人，一定會很疑惑覺得：「到底為什麼後端可以這麼無條件的相信 JWT？」，事實上後端之所以可以無條件相信 JWT 中的內容，是因為 JWT 本身是一個帶有數位簽名的 Token 格式。\n在 JWT 的組成結構中，可以分成 3 個部分，分別是：header、payload、signature。\n舉例來說，下面這一串亂碼：\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjMiLCJuYW1lIjoi5Y-k5Y-kIiwiaWF0IjoxNzIxMDAxNjAwfQ.G41XQGNNJ5Tp88U48aXh4n0XtGkpPkQ3xK6j43_61gE309hzyTVyciG5v05aVIvvY9NrApYiQdvwlMMrjRPFVV8xunghtKKFMj3kPx93Ll8Pf6n-tDiL_NZYqcusrgwtb-EDza80hMG5PTu75ogTIfRKr4jC0_FZzLaMix07LaZReoUSionTWTxJlm8qJc0BAFXgsaGNs9oVhCXOg_jJmOfFZBP0tD3q4xaKp9MTtLRTtslAhoAjPczdnPqaWGcaS8OY11RUTvvxijA7W-mPRlmqt0Hd_XForETUFZRdCKsPQIiGjkavycPtdiViVihQKstHlT4afEzYvzWSeK1cnw 就可以被解析成 header、payload、signature 這三個部分（每一個部分之間用一個點 . 隔開）。\nheader：\neyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9 payload：\neyJzdWIiOiIxMjMiLCJuYW1lIjoi5Y-k5Y-kIiwiaWF0IjoxNzIxMDAxNjA","date":"2024-12-03","objectID":"928d0098dad7feaefea15cfc9d537791","title":"Session 和 JWT 的差別在哪裡？","url":"https://kucw.io/blog/session-vs-jwt/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，這篇文章是每月一次的自媒體月報，主要分享我經營《古古的後端筆記》個人品牌的幕後秘辛，如果你也對於「自媒體創業」有興趣的話，歡迎繼續閱讀本文～\n補充：如果想了解《古古的後端筆記》電子報的起源，也可以先查看創刊號的文章。\n2024.11 月粉絲追蹤數、電子報訂閱人數 # 本月份（2024.11）的粉絲成長人數如下：\n2024/10/29 人數 2024/11/26 人數 11 月份總成長人數 Facebook 粉專追蹤數 4150 4389 +239 電子報訂閱人數 1632 2076 +444 Threads 粉絲追蹤數 2557 4566 +2009 IG 粉絲追蹤數 247 353 +106 這個月的各項指標就是順順的成長，沒有特別突出的爆文，比較意外的是 Threads 和 Facebook 竟然黃金交叉了😂。\n但不得不說我最近也都是滑 Threads 比較多，趁著現在 Threads 裡面沒有廣告，滑起來真的很愉快舒暢XDD，推推！（天曉得以前在 Facebook 要被多少篇商周雞湯文、或是各種廣告轟炸）\n本月自媒體活動：我出書了！！ # 這個月我出了人生的第一本書《Spring Boot 零基礎入門》！！！而且也感謝博碩出版社願意支援我「預購送簽名」的活動，完全是圓了我的簽書夢，謝謝博碩🥹。\n出書的寫作心法和幕後花絮，在之前的文章中已經有和大家分享：\niThome 鐵人賽 - 得《優選》獎項的寫作心法 iThome 鐵人賽 - 出書的幕後花絮 所以這邊就不多贅述，就以一張我簽名簽到很開心的照片作為收尾吧🤣！\n如果你也想要出自己的一本書，非常推薦大家可以參加每年 iThome 舉辦的 30 天鐵人賽活動，只要得獎就可以出書（冠軍、優選、佳作皆可出書）。\n老實說就連在我已經開過好幾堂線上課程的前提下，靠自己的力量還是很難出一本書（首先你要有人脈能聯繫上出版社編輯，然後你要說服他們願意讓你出書），所以藉由 iThome 所舉辦的盛事活動，先用得獎來證明自己的文章是值得出書的，後續再直接和出版社對接，真的是素人出書最簡單的方式了～\n本月撰寫的電子報主題、文章 # 這邊記錄了我這個月撰寫了哪些電子報和文章，大家如果對於其中的內容有興趣的話，也可以前往查看：\n資料庫的 ACID、Transaction（交易）介紹 免費仔萬歲！使用 GitHub Actions 實作 CI/CD、網路爬蟲 iThome 鐵人賽 - 出書的幕後花絮 給工程師的求職建議（年資 1-3 年） 雲端服務中的 IaaS、PaaS、FaaS、SaaS 的差別 2024.11 月報總結 # 呼～大概是這樣！這個月其實花了大半部分的精力在出書上，所以沒有太多時間學習自媒體的相關知識，最後也要真的感謝大家的支持🙏，希望你們會喜歡我的簽名🤣。\n下個月應該就比較有空學習自媒體的內容了！我已經囤積了好幾本書想看了🤣，等到有更多心得也會再和大家分享～那我們就下個月的月報再見啦！\n如果你對後端筆記有興趣，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，一起變強💪\n","date":"2024-12-03","objectID":"d5fb4323b2b9ca3138617f665f26889c","title":"軟體工程師的自媒體之路 - 2024.11 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202411/"},{"categories":["其他技術分享"],"content":"IaaS、PaaS、FaaS、SaaS 這些名詞，是大家一開始在接觸雲端服務時，很容易搞混的名詞，所以這篇文章我們就來介紹一下他們的差別吧！\n目錄 1. 什麼是 IaaS？ IaaS 的優點 IaaS 的缺點 2. 什麼是 PaaS？ PaaS 的優點 PaaS 的缺點 小結：IaaS 和 PaaS 的區別 3. 什麼是 FaaS？ FaaS 的優點 FaaS 的缺點 補充：FaaS 也稱為 Serverless（無伺服器運算） 4. 什麼是 SaaS？ IaaS、PaaS、FaaS、SaaS 總結 結語 1. 什麼是 IaaS？ # IaaS 是 Infrastructure as a Service 的簡寫，中文翻譯為「基礎結構即服務」，IaaS 是指「你能夠使用這個服務來創建 VM」。\n所以假設有一個雲端服務是 IaaS，那就表示他可以讓我們在上面自由的創建 VM，並且我們可以在該 VM 中安裝喜歡的 Java 版本、或是安裝喜歡的 Python 版本，然後我們也可以在這個 VM 裡面自由的運行想運行的程式，完全不受到任何限制！\n所以只要是直接提供一台 VM 給你自由運用的服務，即是屬於 IaaS 的一種，也就是 Infrastructure as a Service。\n舉例來說，像是 AWS 的 EC2、GCP 的 Compute Engine，他們就都是屬於 IaaS 的服務（因為他就是直接提供一台 VM 給我們，讓我們自由去運用）。\nIaaS 的優點 # 因為 IaaS 提供完整的 VM 存取權，所以靈活性非常高，想幹嘛就幹嘛！ IaaS 的缺點 # 因為雲端服務商只會提供一台 VM 給你，所以凡事都得自己來，上到安裝 Java 版本、下到網路防火牆設定，全部都得靠自己完成。 2. 什麼是 PaaS？ # PaaS 是 Platform as a Service 的簡寫，中文翻譯為「平台即服務」，而到了 PaaS 這裡之後，就沒有了 VM 的概念。\n因此假設有一個雲端服務是 PaaS，那麼 他只會要求你上傳你的程式碼，然後他就會像變魔法一樣，直接幫你把這個程式運行起來了，magic！\n所以當你使用了 PaaS 的服務之後，你就再也碰不到 VM 層了（或是非常難），雲端服務商會把你的程式運行在一個「容器 Container」裡面，你只要告訴他你要幾個容器就好，剩下的雲端服務商會全部包辦。\n因此 PaaS 也可以稱為是懶人部署法，你不需要像上面的 IaaS 一樣，自己去搞 VM 然後自己安裝 Java 版本，你要做的，就是寫好程式，上傳，然後剩下的雲端服務商會全部幫你搞定，世界和平！\n舉例來說，像是 AWS 的 Elastic Beanstalk、GCP 的 App Engine、或是 Zeabur、Heroku、Vercel…等等的網站，他們就都是屬於 PaaS 的服務（因此我們就只要直接上傳我們的程式就好，不需要處理任何環境安裝的問題）。\nPaaS 的優點 # 只需要上傳程式碼即可運作，降低維運的人力和時間成本。 PaaS 的缺點 # 靈活度比較低，碰不到實際的 VM 層級，沒辦法直接連線到容器裡面做特別的設定。 通常會限制程式語言，只支援熱門的，太冷門的不支援。 收費較貴（不過貴不是他的缺點，是我的🥹。 小結：IaaS 和 PaaS 的區別 # 所以從上面的 IaaS 和 PaaS 的介紹，大概可以感覺得出來 IaaS 和 PaaS 其實是一個對立的關係。\nIaaS 就是直接丟一個最原始的 VM 給你，你愛蓋什麼就蓋什麼，有點像是給你一塊地你自己自由發揮。\n而 PaaS 則像是一棟蓋好的大樓，裡面的設施非常先進漂亮，你只要提著你的行李箱（程式碼）就可以入住，但缺點就是你不能隨便更動大樓裡面的管線，只能照著他們既定的規則走這樣。\n所以如果你只是要做一個小型的 Project，不想要管環境的安裝問題，那就可以直接採用 PaaS 的服務來部署；而如果你是想要自己掌握所有控制權，想要自己處理防火牆、軟體版本\u0026hellip;等等的控制，或是你想省點錢的話😂，那就可以考慮採用 IaaS 來部署。\n3. 什麼是 FaaS？ # 了解了經典的 IaaS 和 PaaS 的概念之後，接著我們可以來看一下什麼是 FaaS。\nFaaS 是 Function as a Service 的簡寫，中文翻譯為「函式即服務」或是「功能即服務」。\n相較於 IaaS 和 PaaS，FaaS 其實是近十年才被提出來的新概念，所以雖然 FaaS 也是屬於 XaaS 家族的一員，但其實他和上面的 IaaS 和 PaaS 沒什麼關係。\nFaaS 的概念，是 「把程式當成方法來執行」，即是讓程式不用一直運行著，而是當有請求來時，就快速啟動這個程式，然後請求走的時候就 shutdown 這個程式，簡單的說就是不讓程式一直啟動著，而是有需要的時候才開啟他，這就是 FaaS 的概念！\n大家也可以想像一下，一般我們在寫後端程式的時候，通常就是把程式運行起來，然後這個程式就會一直運行著，等著去接收前端的請求，即使沒有前端的請求過來，這個程式仍舊會一直運行著。\n而 FaaS 即是想要提出一個新概念，就是只有當前端發請求過來的時候，才會去運行起這個後端程式去處理前端的請求，當請求執行完畢後，就關掉這個後端程式，不讓他在那邊空轉，把「程式」當成是一個「方法」來運行，即是 FaaS 的概念。\n舉例來說，像是 AWS 的 Lambda、GCP 的 Cloud Functions，他們就都是屬於 FaaS 的服務。\nFaaS 的優點 # 只需要在使用時付費，不需付錢讓程式空轉。 FaaS 的缺點 # 功能要拆分的比較細，每一份程式要保持在處理非常輕量化的小功能，像是處理一張圖片的 resize…等。 和雲端服務綁比較深，萬一將來要下雲會比較麻煩。 補充：FaaS 也稱為 Serverless（無伺服器運算） # 在這裡也補充一下，其實 FaaS 服務還有另一個稱呼，即是 Serverless（無伺服器運算）。\nFaaS 之所以能夠被稱為 Serverless，是因為從定義上來說，我們並沒有長期運行一個 server，而是當前端請求來時，我們才啟動這個 server，並且當前端請求走了之後，這個 server 也被關掉了。所以在定義上，我們「並沒有」長期運行一台 server，傻傻的去等待前端發送請求過來，因此這種部署方式，就稱為是 FaaS，也叫做 Serverless（無伺服器）。\n所以 FaaS 和 Serverless，他們指的其實都是同一件事情，就是把程式當成方法一樣來使用，用完即丟，不會長期運行某份程式這樣。\n補充：其實我一開始有點不能接受 Serverless 的定義😂，因為他就是有運行 server 啊！只是中間的過程很短我們看不見而已！！不過這邊的定義就是這樣，所以建議大家就先接受這個定義吧🥹，FaaS 就是 Serverless，Serverless 就是 FaaS，他們指的是同一件事情。\n4. 什麼是 SaaS？ # 介紹完前面的 IaaS、PaaS、以及 FaaS 之後，最後我們可以來看一下什麼是 SaaS。\nSaaS 是 Software as a Service 的簡寫，中文翻譯為「軟體即服務」。\nSaaS 其實就是泛指 Gmail、Google Drive 這種已經很成熟的軟體，SaaS 跟工程師其實沒有什麼特別的關係，通常只是在提到 IaaS、PaaS 時，會一起拿出來被介紹到。\n舉例來說：\n我們在工作上，可以使用 Jira、Trello 這類的 SaaS 的軟體，幫助我們管理敏捷看版的流程。 我們也可以","date":"2024-11-26","objectID":"ba956c5655eec6c31b9044fbebbe8de5","title":"雲端服務中的 IaaS、PaaS、FaaS、SaaS 的差別在哪裡？","url":"https://kucw.io/blog/iaas-paas-faas-saas-intro/"},{"categories":["其他技術分享"],"content":"哈囉，我是古古！之前有在 iThome 鐵人賽 - 得《優選》獎項的寫作心法 的文章中，和大家分享我得獎的寫作心法，這篇文章則是會分享在 iThome 鐵人賽得獎之後，出書過程中的一些幕後花絮，如果你也對出書感興趣的話，歡迎繼續觀看本文章～\n目錄 這本書《Spring Boot 零基礎入門》在介紹什麼？ 出書的幕後花絮 書稿要用 Word 檔繳交 書稿的中英之間不能有空白鍵 封面文案、封底文案設計 結語 這本書《Spring Boot 零基礎入門》在介紹什麼？ # 首先先介紹一下我所出版的書籍，這本書是改編自 iThome 鐵人賽的得獎文章《Spring Boot 零基礎入門》，當初會想用 Spring Boot 零基礎入門這個主題參賽，就是因為自己剛入門 Spring Boot 時，發現單純透過網路上零散的介紹，很難完整了解 Spring Boot 的運作邏輯是什麼（一開始覺得IoC 好難…🥹），一度覺得很挫折。\n所以希望可以透過這本書，系統性的去整理 Spring Boot 的知識，讓想要入門 Spring Boot 的人有個方向，至少在看完這本書之後，能夠學到：\nSpring IoC、Spring AOP 的概念和用法 Spring MVC 的用法 Spring JDBC 的用法 能夠使用 Spring Boot，開發出一個簡易的後端系統（書中使用圖書館管理系統作為範例） 如果你也對 Spring Boot 有興趣、或是打從心底想要從頭開始學習 Spring Boot，那麼這本書真心推薦給你！\n出書的幕後花絮 # 以下僅列出我在寫書中覺得很意外 or 很新鮮的事，記錄的不是很完全，大家就看個開心就好～\n書稿要用 Word 檔繳交 # 這個應該是在寫書時痛苦指數最高的事情🥹，在出版社真的拿著書稿內容去排版之前，會需要作者先用 Word 檔把全部的內容整理起來（包含各章標題、圖號、程式碼樣式），先給編輯看過一遍之後，編輯才會把 Word 檔交給美編，交由美編後續去排版。\n但是在習慣使用 Markdown、Notion 這類的筆記軟體之後，突然要回頭使用 Word 檔.…真的是只有想哭可言🥲（我從學校畢業之後就沒打開過了）。\n首先 Word 的排版就真的很難搞，再來就是 Mac 上的 Word app 常常會閃退（我不知道是我電腦有問題還是 Word 有問題），反正就是要花很多時間和 Word 戰鬥.…如果大家將來也想要出書的話，建議先跟你家 Word 打好關係，因為你真的會很依賴他🥹。\n給大家看看我的 Word 檔，以及給美編排版過的成果：\n這樣看下來，美編是不是真的很神！！！只要給她醜醜的 Word 檔，她就可以幫你排出好看的排版，出這本書真的不能沒有美編XDD，感謝美編！！！\n書稿的中英之間不能有空白鍵 # 又是一個撰寫書稿的痛苦（出書大概有 80% 的痛苦都是在寫書稿，其他部分相較還好），一般我在寫文章的時候，中英文之間會有空白鍵，但是出版社要我們繳交的書稿，竟然要把中英文間的空白鍵刪掉！！！（好像是因為排版軟體的設定，所以必須要刪掉空白鍵，這樣子弄出來的書稿才會正常）\n反正這部分也是折磨我很久…就算用「取代」功能一鍵把所有空白鍵刪掉，也需要整份稿子重新檢查有沒有缺漏的部分。最可怕的是在改稿的時候，手會不小心習慣在中英之間按下空白鍵，然後事後發現之後又要整個回頭重改🥹，我只能….哭惹😭。\n反正中英不能有空白鍵這件事真的是造成我心理陰影，甚至還想跟編輯說我之後不想再寫書了XDD，但老實說現在回頭看起來也是一次特別的經驗就是了，畢竟是第一次出書，再怎麼樣都是一次新鮮的體驗！\n封面文案、封底文案設計 # 這部分算是我很意外的部分！我沒想到出版社可以讓我們自己想封面和封底設計，其實對於作者來說彈性還滿大的，先給大家看看我的封面和封底設計（沒錯編輯竟然是給我一張這樣的展開圖，我第一次收到有嚇到XDD）：\n其中有關書名、封面的大圖、封面文案、封底文案、作者簡介，這些都是可以讓作者設計的，所以作者可以根據你自己的書，填上你想放的宣傳語。\n舉例來說，下面這些都是我提供給編輯的文案：\n不得不說，真的是當了作者之後，才發現原來一本書的封面、封底設計，都是有意義的！！所以大家下次再走進書店買書時，建議可以仔細研究一下封面上的各種小字，每一行小字其實都是作者精心放上去的內容，絕對沒有那種來湊數的文字存在！\n然後在設計封面和封底文案時，中間也有發生一件很好笑的事，因為我實在是不知道怎麼寫文案比較好，我就跑去附近的諾貝爾書局，然後一本一本翻以前鐵人賽的書是怎麼寫的，邊翻邊拍照，我大概拍了 20 幾本有（還好店員沒有覺得我很奇怪🤣），在書局待了一整個下午就只為了參考其他書是怎麼寫的，現在回想起來，真的是很特別的一次經驗XD。\n結語 # 呼～大概是這樣吧！雖然上面吐嘈了一些寫書稿的痛苦，但是老實說看到自己寫的書上架還是滿有成就感，如果你也對出書有興趣的話，非常推薦參加一年一度的 iThome 鐵人賽，希望大家都可以成功出版你人生的第一本書👍。\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報 ，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2024-11-19","objectID":"72d052c07d994fb85a23ef2db475c5fc","title":"iThome 鐵人賽 - 出書的幕後花絮","url":"https://kucw.io/blog/ithome-write-a-book/"},{"categories":["職涯相關"],"content":"在面試找工作時，選擇一家心儀的公司絕對是最重要的事，以下分享 2 點找工作的建議給 1-3 年年資的工程師：\n1. 找工作時，發展性 \u0026gt; 錢 # 這個階段其實大家都還是新手，成長性仍舊很高，所以在考慮要不要去某間公司時，要思考的是：「3 年後你離開這間公司時，你有哪些武器可以拿來面試下一間公司」。\n舉例來說，如果 A 公司是 65k（新創），B 公司是 50k（台灣大型企業），那麼你願不願意為了「大企業鍍金」的價值，屈就自己去領 50k 的薪水。這沒有正確答案，就只是個人選擇而已，沒有誰能為你的人生負責，只有自己能為自己負責。\nPS: 但如果真要說，我會建議選擇大公司，畢竟大公司將來要跳新創比較容易，新創要跳大公司會稍微難一點，考量到人生很長，有大公司的經歷還是不錯的。\n2. 學習是下班的重要活動，健身之餘也要健腦 # 有很多人入職之後就疲於工作奔命，沒有留時間給自己繼續精進，其實這是風險很高的事情，因為你沒辦法跟新鮮人拉開差距。\n畢竟你當初可能是努力了一年才成功轉職，如果你不繼續往前走，那麼被其他新鮮人努力一年之後取代，只是遲早的事。\n長江後浪推前浪，身為前浪，真的要避免死在沙灘上😂。\n結語 # 如果你想保持學習的步調，歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，大家一起變強💪！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2024-11-14","objectID":"876708626555369ba865ccefb2d0c9cf","title":"給工程師的求職建議（年資 1-3 年）","url":"https://kucw.io/blog/developer-interview-suggestion/"},{"categories":["其他技術分享"],"content":" 目錄 什麼是 GitHub Actions？ GitHub Actions 在 CI/CD 中的用途 實戰：使用 GitHub Actions 架設第一個 CI/CD GitHub Actions 在網路爬蟲中的用途 實戰：使用 GitHub Actions 實作網路爬蟲 GitHub Actions 總結 結語 什麼是 GitHub Actions？ # 所謂的 GitHub Actions，是 GitHub 所提供的一個自動化集成服務，簡單的說的話，就是我們可以自由指定：「當有 commit 被 push 到 GitHub 中的某個 repository 時，我們要 GitHub 做什麼事」。\n舉例來說，我們可以設定成：「當有 commit 被 push 到 mytest 這個 repository 時，我們要求 GitHub 要傳送一則訊息到 Slack 裡面通知大家」，所以當我們添加了這一個 GitHub Action 之後，以後只要有人 push 了任一個 commit 到 mytest 中，GitHub Action 就會被觸發，因此就會傳送一筆訊則到 Slack 群裡面通知大家了。\n所以 GitHub Actions 他最一開始被發明出來的目的，就是為了「程式的自動化集成」，也稱為是「CI/CD」（Continuous Integration/Continuous Deployment）。\nGitHub Actions 在 CI/CD 中的用途 # 如上面所介紹到的，GitHub Actions 本來就是為了「程式的自動化集成」而發明，簡單的說的話，就是我們可以預先設定好「當工程師上傳 code 之後，要 GitHub Actions 做什麼事」，所以這裡就延伸出各式各樣的用法。\n舉例來說，有的團隊會設定成：\n當工程師上傳 code 之後，自動執行單元測試，檢查所有測試是否能夠成功通過 當工程師上傳 code 之後，自動執行 ESLint、SonarCube 這類的檢查，確保程式的品質正常 當工程師上傳 code 之後，自動將該程式部署到 dev 環境 …等等 而上述這些用法，如果要給他們一個統稱的話，就稱為是「CI/CD」，也就是「持續整合/持續部署」 （Continuous Integration/Continuous Deployment），所以當大家在工作中聽到 CI/CD 時，基本上就是在對我們所上傳的程式做一些「後處理」，確保一些無聊的重複工作可以被自動化執行，這就是 CI/CD 的目的！\n實戰：使用 GitHub Actions 架設第一個 CI/CD # 了解了 GitHub Actions 的概念之後，接著我們也可以試著到 GitHub 上設定看看 GitHub Actions，實作第一個 CI/CD 程式。\n老實說要在 GitHub 中設定一個 GitHub Actions 真的比想像中容易🤣，只要在 GitHub repo 中添加一個 .github 資料夾，並且在裡面再創建一個子資料夾 workflows（也就是 .github/workflows），接著就可以在裡面撰寫 GitHub Actions 的設定了！\n所以創建好 .github/workflows 這兩層資料夾之後，接著可以在裡面添加一個 demo-github-action.yml 的檔案，表示這是一個 action（檔案的檔名可以隨意取，要叫做 demo-github-action.yml 或是 xxx.yml 都可以，每一個檔案就是一個 action）。\n此時在 demo-github-action.yml 裡添加下列程式之後：\nname: GitHub Actions Demo run-name: GitHub Actions Demo # 觸發此 action 的時機 on: push: branchs: # 只要有任何一個 commit 被 push，就會觸發此 action \u0026#39;*\u0026#39; workflow_dispatch: # 可以手動執行此 action # 預先定義此 action 要幹嘛 jobs: demo: runs-on: ubuntu-latest steps: - run: echo \u0026#39;執行成功\u0026#39; 只要這樣寫之後，就可以在「任何一個 commit 被 push」以及「手動執行此 action」這兩個時機點，去觸發這一個 GitHub Action。\n因此當有 commit 被 push 上來時，此 action 就會輸出執行成功的結果，如下圖所示：\n所以透過在 .github/workflows 中添加 demo-github-action.yml 的檔案（檔名可以隨意取），我們就可以為這個 GitHub repo 去添加他專屬的 GitHub Actions，因此就可以用 GitHub Actions 不斷的去集成這份程式，實作 CI/CD 的效果了！\n另外大家以後在查看程式時，如果有發現某份程式有 .github/workflows，就表示他有使用 GitHub Actions，所以就不會看不懂這資料夾到底是在幹嘛的了～（在知道有 GitHub Actions 這功能之前，我一直都以為 .github/workflows 只是 GitHub 產生的檔案，沒啥用處，真的是誤會大了🤣）。\n補充：因為篇幅有限，所以沒辦法詳細介紹 GitHub Actions 中每一行程式的用途，如果大家有興趣的話，可以再查詢 GitHub Actions 的用法介紹。\nGitHub Actions 在網路爬蟲中的用途 # 而 GitHub Actions 除了可以用在最經典的 CI/CD 用途上之外，同時 GitHub Actions 其實也是可以拿來實作爬蟲的！\n之所以可以將 GitHub Actions 用來實作爬蟲，是因為 GitHub Actions 被觸發的時機點，除了可以設定成「當任何一個 commit 被 push 時觸發此 action」之外，也可以設定成 「定時觸發此 action」，而就是這個「定時觸發此 action」的功能，讓我們可以拿來活用他，進而實作網路爬蟲。\n實戰：使用 GitHub Actions 實作網路爬蟲 # 如果要使用 GitHub Actions 實作網路爬蟲的話，首先需要先在 mytest 中新增一份爬蟲程式（此處以 crawler.py 為例）。\n接著在 .github/workflows 中創建一個新的 action 檔案 crawler-demo.yml（檔名可以隨意取），並且在 crawler-demo.yml 中添加下列程式：\nname: Crawler Demo Action run-name: Crawler Demo Action # 觸發此 action 的時機 on: schedule: - cron: \u0026#34;55 12 * * *\u0026#34; # UTC 每天下午 12:55 執行此 action（等同於台灣晚上 8:55 執行） workflow_dispatch: # 可以手動執行此 action # 預先定義此 action 要幹嘛 jobs: crawler-demo: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Setup Python uses: actions/setup-python@v4.5.0 with: python-version:","date":"2024-11-12","objectID":"a8e7e9a9542e440e76870f69e9653a55","title":"免費仔萬歲！使用 GitHub Actions 實作 CI/CD、網路爬蟲","url":"https://kucw.io/blog/github-actions-intro/"},{"categories":["其他技術分享"],"content":" 目錄 什麼是資料庫的 ACID？ Transaction（交易）是什麼？ 例子：銀行轉帳 使用 Transaction 解決轉帳的問題 Transaction 和資料庫 ACID 的關聯 資料庫的 ACID 總結 結語 什麼是資料庫的 ACID？ # 在常見的關聯式資料庫中（例如：MySQL、PostgreSQL、MS SQL），都會有 ACID 的特性，而 ACID 即是：\nAtomicity（原子性） Consistency（一致性） Isolation（隔離性） Durability（永久性） 也因為有 ACID 這四個特性，使得 MySQL 這類的資料庫支援 「Transaction（交易）」 的操作，所以接下來我們就直接用一個例子，來了解一下什麼是「Transaction（交易）」，以及了解 ACID 在其中的用途是什麼。\nTransaction（交易）是什麼？ # 在資料庫中，有一種用法叫做 Transaction，而他的中文可以被翻譯成「交易管理」或是「事務管理」。\n而 Transaction 的用途，就可以在「一個 Transaction 裡面，包含多個資料庫的操作，並且這些資料庫的操作，要嘛全部一起成功，要嘛全部一起失敗，也就是 All or Nothing 原則」。\n例子：銀行轉帳 # 舉例來說，假設我們現在要實作一個轉帳的功能，讓 A 可以轉 1000 元給 B，那麼在實作轉帳的功能時，就需要對資料庫進行兩次操作：也就是先將 A 的存款減去 1000 元，再將 B 的存款增加 1000 元，這樣子就可以完成轉帳的實作。\n所以像是在上面的 transfer() 方法中，就是會先去對 A 的帳戶減去 1000 元（A 的存款從 3000 變成 2000），再為 B 的帳戶增加 1000 元（B 的存款從 500 變成 1500），進而完成轉帳的操作。\n不過，在這段 transfer() 的程式中，其實是有一個潛在的問題的。\n舉例來說，我們在執行這段程式時，一開始一樣是先從 A 的帳戶中減去 1000 元（A 的存款從 3000 變 2000），但是！假設這時程式突然出現了錯誤（有可能是程式出現 bug、或是機器故障、機房停電…等因素），導致在扣完 A 的錢之後，程式沒辦法繼續往下運行，再去 B 的帳戶中增加 1000 元。\n因此最終結果就會變成 A 損失了 1000 元、但是 B 卻沒收到 1000 元，導致 1000 元就這樣消失了！這種狀況一定是大家不想看見的。\n所以為了解決這個問題，Transaction 就被發明出來了！\n使用 Transaction 解決轉帳的問題 # 在資料庫中，他支援一種用法叫做 「Transaction（交易）」，當我們使用 Transaction 來管理這整個轉帳的流程之後，那麼「扣掉 A 的錢」以及「增加 B 的錢」這兩件事情，他們就只能夠一起成功，或是一起失敗。\n舉例來說，在 Spring Boot 程式中，我們可以在某個方法上面加上 @Transacional，這樣子就可以宣告「使用一個 Transaction 來管理這個方法」，因此在這個例子中，當我們在 transfer() 方法上面添加 @Transactional 時，就表示我們創建了一個 Transaction，來管理這個 transfer() 方法。\n而當某個方法被 Transaction 來管理時，那麼 「在那個方法中的所有資料庫操作，要嘛全部一起成功、要嘛全部一起失敗，也就是 All or Nothing 原則」。\n因此在這個例子中，首先 A 一樣是會先被減去 1000 元沒錯（所以 A 的存款從 3000 變成 2000），然後這時候一樣，程式遇到了錯誤，因此程式就只能夠被迫中斷，沒辦法繼續往下執行，為 B 的帳戶增加 1000 元。\n但是這個時候，因為我們有在 transfer() 方法上面加上 @Transactional，用一個 Transaction 來管理此方法，因此此時 Transaction 就會復原（也稱為 rollback）前面做過的所有資料庫操作，也就是會將 1000 元還給 A，所以 A 的存款又從 2000 元變回 3000 元，就像是剛剛那個扣除的 SQL 語法從來沒有執行過一樣，A 和 B 的存款都回到最初的樣子，這就是 Transaction 的厲害之處！！\n所以對於轉帳這種「涉及多個資料庫操作」的功能而言，如果我們想要確保轉帳中的所有資料庫操作要嘛一起成功、要嘛一起失敗，這時候就可以使用資料庫所支援的 Transaction 用法，使用 Transaction 來保護這整筆轉帳的交易過程了。\n也因為 Transaction 會確保方法中的所有資料庫操作，要嘛全部一起成功、要嘛全部一起失敗，因此這也稱為是 All or Nothing 原則。\n補充：Transaction 只能確保「同一個資料庫」中的交易會一起成功、一起失敗而已，如果涉及到多個資料庫（像是分散式資料庫），那麼使用 Transaction 的情況會變得複雜得多，大家有興趣的話，可以再上網查詢「分散式交易（Distributed Transactions）」的相關資訊。\nTransaction 和資料庫 ACID 的關聯 # 了解了 Transaction（交易）要解決的問題之後，我們終於可以說回到最一開始的「資料庫的 ACID」的特性了！\n我們在前面有提到，常見的關聯式資料庫中（例如：MySQL、PostgreSQL、MS SQL），都會有 ACID 的特性，而 ACID 即是：\nAtomicity（原子性） Consistency（一致性） Isolation（隔離性） Durability（永久性） 也因為有 ACID 這四個特性，所以 MySQL 這類的資料庫才能支援 Transaction 的操作。所以換句話說的話，假設某個資料庫他不具備 ACID 的特性的話，那他是不能使用 Transaction 的操作的！！！\n舉例來說，假設你使用的資料庫是 NoSQL（例如 MongoDB、Elastic Search），那麼即使你有在 Spring Boot 程式中添加 @Transactional，指定這段方法要使用 Transaction 來管理，但是因為 NoSQL 他不支援 ACID 啊！所以他就不支援 Transaction 的操作🥹。\n因此不管你在程式中加了多少個 @Transactional，那段程式仍舊是沒有受到 Transaction 的交易保護的，所以就算程式運行中出現錯誤，仍舊是沒辦法 rollback 成原始數據的，這是大家在使用 Transaction 時，一定要注意的細節！\n補充：軟體變化瞬息萬變，現在也已經有些 NoSQL 資料庫支援 ACID 的特性、進而支援 Transaction 了，大家在使用之前，建議也可以參考一下你使用的 NoSQL 資料庫介紹。\n而如果我們把 ACID 展開來，一一對應到 Transaction 中的概念的話，就可以發現他真的每一條都是為了 Transaction 來設計的：\n特性 描述 Atomicity（原子性） Transaction 是一個不可被分割的單元 Consistency（一致性） Transaction 執行的前後，必須確保數據的完整性是一致的 Isolation（隔離性） 資料庫同時處理多個 Transaction 時，各個 Transaction 之間不能互相影響 Durability（永久性） Transaction 一旦提交之後，他對資料庫所做的改變永久有效，不會因為系統重啟","date":"2024-11-05","objectID":"7024481b7855732aacf6dc82a3051845","title":"資料庫的 ACID、Transaction（交易）介紹","url":"https://kucw.io/blog/db-acid-transatcion/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，這篇文章是每月一次的自媒體月報，主要分享我經營《古古的後端筆記》個人品牌的幕後秘辛，如果你也對於「自媒體創業」有興趣的話，歡迎繼續閱讀本文～\n補充：如果想了解《古古的後端筆記》電子報的起源，也可以先查看創刊號的文章。\n2024.10 月粉絲追蹤數、電子報訂閱人數 # 本月份（2024.10）的粉絲成長人數如下：（因為剛好前面的數據也都還在，就放上來做個對比）\n2024/9/24 人數 2024/10/29 人數 10 月份總成長人數 Facebook 粉專追蹤數 2687 4150 +1463 電子報訂閱人數 1195 1632 +437 Threads 粉絲追蹤數 535 2557 +2022 IG 粉絲追蹤數 130 247 +117 從這個誇張的線圖可以看到，這個月份真的是被流量眷顧一波🤣，會造成這麼誇張的粉絲數成長，是因為我在 10/24 時有 Po 了一篇「周杰倫演場會搶票系統 - 拓元售票背後的架構」，裡面有提到拓元是架設在 AWS 的環境上，然後網路上各種大神、對架構有興趣的人，就紛紛轉貼這則貼文，然後就變成爆文了😂（當時的貼文在這 Threads 貼文、FB 貼文）。\n不得不說，經過這一次完全體會到什麼叫流量爆炸，人生第一次粉絲數漲那麼快，手機真的一整個晚上都在響通知，謝謝周杰倫🙏（雖然我還是沒搶到你的演唱會門票🥲）。\n不過事後覆盤一下的話，我發現中間還是有很多有趣的演算法運作的！\n首先對於 Facebook 來說，我發現真正要讓文章能擴散出去，是要靠大粉專幫忙分享，像是在這篇貼文中，就是在 科技工作講 Tech Job N Talk 大大先分享了我的貼文之後，後面才引發一連串的流量爆炸（不知道有多少朋友是透過科技工作講知道我的呢？）。\n所以對於 Facebook 而言，如果今天要寫一篇爆文，可能不是得靠你自己的粉絲分享，而是得讓某個大粉專也幫忙分享，這樣子的瞬間流量才會起來，Facebook 才會覺得這篇文章可能是爆文，因此演算法才會把文章推播出去，進而觸及到更多受眾。\n當然目前這都還只是我自己的猜想啦XD，沒有人知道 Facebook 演算法是如何運作的，我能做的事情仍舊沒有變，就是盡力分享我覺得有價值的內容，至於有沒有受演算法青睞….就隨緣吧🤣。\n不過經過這次的周杰倫事件之後，我之後確實會更加注意有沒有時事可以蹭一下（盡量是和後端有關的時事），畢竟偶爾蹭一下時事，還是對破圈漲粉絲滿有幫助的，這樣子對長期經營自媒體來說，可能會是更健康的循環。\n至於 Threads 的話，要讓文章能擴散出去，是靠大家的按讚、回覆、分享。我自己感覺 Threads 目前比較去中心化，文章是依靠「話題性」決定他是不是一篇爆文，像是在 Threads 上常常會滑到追蹤數 100 人以內、但是按讚數一兩千以上的貼文。\n也因為 Threads 是看文章的話題性決定觸及人數，所以老實說要在 Threads 上要寫爆文就更飄渺了😂，因為你沒辦法抱大腿取暖，完全就是看 Threads 心情要不要把你推播出去這樣。\n不過畢竟 Threads 也還在發展階段，演算法應該也是一直持續在改，所以 Threads 我可能要再摸一段時間之後，才能有更多心得可以跟大家分享～\n本月學習內容：詹雨安的創業筆記（Heptabase 創辦人） # 這個月的自媒體學習內容，是我意外在 Medium 上逛到的「詹雨安的創業筆記」，這系列總共有 5 集，我把連結都貼在下面這裡，大家有興趣也可以看一下這些文章：\n創業筆記（一）：休學創業學到的七件事 創業筆記（二）：在美國開公司的大小事 創業筆記（三）：在錄取 Y Combinator 之前 創業筆記（四）：核心指標的成長 創業筆記（五）：種子輪籌資 我自己最喜歡的是 「創業筆記（三）和（四）」，在新創軟體界中，Y Combinator（簡稱 YC）是最知名的孵化器，眾多新創軟體（ex: Zapier、PagerDuty、GitLab、Dropbox、Twitch…等），都是從 YC 出來的，而且每年 YC 也會舉辦 Demo Day，可以說是新創界的盛事，所以能夠入選 YC 的新創團隊，真的都是非常厲害的團隊！\n而 Heptabase，這個筆記軟體的新創團隊，就真的入選了 YC，然後大神還把他準備的過程全部寫出來，真的是滿滿乾貨！！超級推薦！！！\n雖然我自己沒有用 Heptabase，但是這個系列不需要你使用過 Heptabase 也能看（當然如果你看完之後願意付費使用 Heptabase，大神應該會很開心XD），推薦對創業有興趣、或是單純想了解看看 YC 到底在幹嘛的人閱讀這系列的文章。\n以下也節錄我自己最有感觸的幾段話，分享給大家：\n再多的方法論和聰明才智，重要性都比不上創業者自己的信念和決心。（出自創業筆記一） MVP 推出去功能太少、太爛、沒有人用，這些一點都不是問題。每一次更新 MVP 都是一次學習、一次揮棒。揮棒多了才有可能打安打、打全壘打。MVP 從來都不該一次到位，而是要能高速迭代、變得愈來愈好。（出自創業筆記三） 一家新創公司的基本要素 — 產品、客戶、團隊、市場，空有宏大願景，但是缺乏足夠執行力和商業思維的新創公司，九成以上會在第一年就陣亡。（出自創業筆記三） 流量可以用廣告買，但是留存率就完全取決於你的產品實力。（出自創業筆記四） 作為自媒體創業者，真的是對 「流量可以用廣告買，但是留存率就完全取決於你的產品實力」 這句話感到當頭棒喝🥹。\n其實我在創作的過程中，還是很容易會被流量綁架（特別是剛經歷周杰倫的爆文漲粉），但看到這句話之後，就是要常常提醒自己：流量是假象，留存率才是真相，我也想和 Heptabase 一樣，盡力做一個能幫助到大家的產品，以後也會繼續朝著這個目標努力的💪。\n總之這份創業系列文真的很推薦大家閱讀！對創業有興趣的話，真的推薦一看～～\n本月撰寫的電子報主題、文章 # 這邊記錄了我這個月撰寫了哪些電子報和文章，大家如果對於其中的內容有興趣的話，也可以前往查看：\nTDD 是什麼？認識 Test-Driven Development（測試驅動開發） Git 的好用技巧介紹 - Cherry-Pick 和 git reset HEAD^ 工程師如何和 PM 共事？PM 親自來解答！ RabbitMQ 介紹（一）- 什麼是 Message Queue？ RabbitMQ 介紹（二）- RabbitMQ 用法介紹 RabbitMQ 介紹（三）- RabbitMQ 安裝教學 + Web 管理介面導覽 淺談搶票系統 2024.10 月報總結 # 呼～總之這個月的月報大概就是這樣吧！感覺周杰倫真的是印象最深刻的了XDD，啊還有能發現詹雨安的寶藏創業筆記也很讚！！真的是處處有驚喜，到處都可以挖寶🤣。\n最後也是感謝從 Hahow 時代一路追蹤以來的老粉、以及剛加入訂閱的新粉們支持！後續也會盡力分享後端知識、以及經營自媒體的幕後祕辛給大家的，那我們就下個月的月報再見啦～\n如果你對後端筆記有興趣，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，一起變強💪\n","date":"2024-11-05","objectID":"04f9f3423bf35271e52b552100ed51d9","title":"軟體工程師的自媒體之路 - 2024.10 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202410/"},{"categories":["其他技術分享"],"content":"搶票系統，一直是後端工程師在面試時的一大難題，而搶票系統又與我們的生活息息相關（例如：周杰倫演唱會搶票、雙 11 搶購），因此這篇文章就會淺談一下搶票系統的設計和遇到的挑戰，帶大家簡易入門搶票系統的實作。\n補充：搶票系統真的是一個很難的題目，我其實也不到真的很懂，所以這篇文章真的只是淺淺談一下，和大家分享目前我所學習過的內容，大家如果有什麼想法也歡迎留言，一起學習成長💪\n目錄 創建一筆訂單的實作 解決商品超賣的問題 補充：系統設計的 Functional Requirement 和 Non-functional Requirement 如何應對高流量？ 補充：拓元的搶票系統架構 結語 創建一筆訂單的實作 # 在我們實作高流量的搶票系統之前，萬事都得從平地開始做起，所以我們就先從最基本的功能開始設計，並且也在設計的過程中，討論搶票系統可能會出現的潛在問題。\n一般來說，在搶票時，不管今天是搶周杰倫的票、搶五月天、或是搶 iPhone…等等，都是要執行 「創建一筆訂單，並且將商品庫存 -1」 的操作，所以如果用程式來實作的話，可以寫成下面這個樣子（此處使用 Spring Boot 程式當作範例）：\n所以到這裡，我們就完成最基本的下訂單的雛形了。不過這個實作其實是有問題的，在一次只有一個人來下訂單時，這段程式可以正常運作，但是只要「多人同時一起來下訂單」，這段程式就會出現「商品超賣」的情形。\n解決商品超賣的問題 # 在上面的實作中，雖然程式看起來很正確沒錯，但是在「多人同時下單」時，也就是「多個 Thread 同時去執行這段程式」時，就會出現商品超賣的問題。\n大家可以想像一下，假設現在資料庫中還有 1 個商品庫存，然後此時有三個人同時執行到了第 11 行的 getProductAmount() 程式的話，那麼這三個人都會覺得：「現在資料庫中還有 1 個商品庫存，太棒了！這最後一個就是我的了！！」，所以這三個人都會繼續往下執行後面的程式，去創建一筆訂單，並且將商品的庫存 -1。\n但是這樣子的行為，就會導致資料庫中的商品庫存被減了 3 次（也就是被賣了 3 次），但是我們實際上只剩下 1 個商品庫存啊！！所以這時候就會導致商品超賣的問題出現，也就是我們賣超過自己擁有的庫存，導致有些消費者付了錢、但是拿不到真正的商品。\n要知道，商品超賣這件事在公關危機上算是非常嚴重的事件，所以為了避免這個問題，我們就必須修改這段程式，也就是 「為他加上一個同步鎖，確保一次只會有一個人執行這段程式」，因此我們就可以將這段程式改寫成下面這樣：\n（Java 中的同步鎖是用 synchronized 來實作，不熟悉 Java 語法的人可以不用管細節沒關係，只要知道這裡是加一個同步鎖就好）\n也因為我們改寫了上面這段程式，在第 11 行的地方加上了一個 synchronized 的同步鎖，因此這個同步鎖一次只會放一個人進去執行第 14～19 行的程式，就可以避免商品超賣的問題了！\n所以到這裡，我們才真的算是完成了「下訂單」的基本功能實作，也就是先確保我們的程式運行是正確的，至少不會產生商品超賣的問題。\n而一般在系統設計的流程中，當我們實作完「基本功能」之後，就可以來探討「怎麼承受住高流量」這類的問題了。\n補充：系統設計的 Functional Requirement 和 Non-functional Requirement # 在實作「能承受高流量的搶票系統」之前，我們先跳出來補充一下系統設計的相關概念。\n系統設計通常分兩塊，一塊是 Functional Requirement（功能性需求）、另一塊是 Non-functional Requirement（非功能性需求）。\n所謂的 Functional Requirement（功能性需求），就是「你是否能夠正確的解決問題」，譬如說你是否能夠實作出一個不會商品超賣的訂單系統、你是否能夠實作出一個 url 的短連結跳轉…等等，所以所謂的 Functional Requirement，就是你先把這個功能真的實作出來這樣。\n而至於 Non-functional Requirement（非功能性需求），則是「你能夠多厲害的解決問題」，譬如說你這個訂單系統，是否能夠承受 1000 人的流量？是否能夠承受 1 萬人的流量？是否能夠承受 100 萬人的流量？或是你這個訂單系統，他的可用性是多少？有沒有辦法確保 99.9999% 的時間不會當機？…等等。\n因此所謂的 Non-functional Requirement，就是在考驗你的系統到底有多強、執行效率有多高、可用性有多好…等。\n所以在進行系統設計時，可以總結成一句話：「先求有，再求好」。\n首先是 「求有」，也就是一定要先確保 Functional Requirement 能正常運作，這種最最最基本的功能一定要先完成，至少要先確保商品不要超賣，討論後面的高流量才有意義。\n等到最基本的 Functional Requirement 實作完畢之後，再來就是 「求好」，也就是處理 Non-functional Requirement 的問題，例如高流量、高性能、高可用，這類的三高問題（沒錯系統設計跟血糖一樣，也有三高問題🥹）。\n當然一般在系統設計中，面試官更關注的是 Non-functional Requirement 的部分，也就是你的系統能夠架設到多厲害這樣，所以 Non-functional Requirement 也可以說是在準備系統設計時，最需要花大量時間準備的地方（因為牽涉範圍又深又廣），不過在實作上，還是會建議大家「先求有，再求好」，沒有一步到位的系統，Don’t Over Design，Facebook 也不是第一天就長成那麼龐大的架構，都是慢慢跟著需求一起變化而來的。\n如何應對高流量？ # 在了解系統設計的概念之後，如果我們回到最一開始的問題的話，現在我們已經把「創建訂單」的功能實作出來了，所以現在我們可以開始探討，要如何提升這個系統，讓他可以應付更高的流量。\n現在在這個架構中，因為我們是使用 synchronized 這個同步鎖，強制讓同時湧進來的許多人被擋在 synchronized 上，一次只允許一個人執行第 14 ～ 19 行的程式，因此雖然可以確保商品不會超賣，但是卻讓使用者會被同步鎖卡太久，導致使用者體驗不好。\n一個較常見的解法，就是改用 Redis 來取代同步鎖，即是將商品庫存的數據改成暫存到 Redis 中，然後 Redis 中可以使用 lua 腳本確保原子性（參考 Bilibili 影片介紹、阿里雲介紹）。\n圖片來源： 阿里雲官方幫助文檔 因此這個優化的概念，就是用 Redis 來取代 Java 內建的同步鎖，進而達到更高的效能。但是具體的實作細節，抱歉我也不是很熟悉、也沒有真的實作過，所以這樣子的作法到底能夠支撐多少流量呢？我可能也沒辦法給大家一個很好的量化數字🥹。\n我目前所學習到的知識，大概就是停在 Redis 這裡而已，後面的部分我也還在探索中，所以目前還沒辦法給大家答案🥹，如果大家有興趣的話，可以在 Bilibili 上面搜尋關鍵字「秒殺系統」，大陸那邊有很多類似的影片介紹可以參考，如果想找英文資源的話，也可以搜尋「How to design a Ticketmaster」，也會有滿多相關的討論可以看一下。\n也希望將來的某一天，我可以重寫這篇主題，重新深入探討搶票系統，到時候如果有學到更多新的知識，也想再重新整理出來分享給大家！\n補充：拓元的搶票系統架構 # 以台灣來說，比較知名的搶票系統為「拓元售票」，如果大家對拓元售票的系統架構感興趣的話，也可以參考 AWS 於 2020 年提供的公開資料（t","date":"2024-10-29","objectID":"50c01260e3da79653f23bda28b19ff06","title":"淺談搶票系統","url":"https://kucw.io/blog/ticket-system/"},{"categories":["其他技術分享"],"content":"寫稿中….✍️\n有需要的人可以先參考這個 Github repo 中的 code，感謝！\nhttps://github.com/kucw/spring-boot-demo/tree/master/spring-boot-demo-rabbitmq\n","date":"2024-10-23","objectID":"c3373c4dfd0ca897c7aeb377c69c3556","title":"RabbitMQ 介紹（四）- 在 Spring Boot 中串接 RabbitMQ","url":"https://kucw.io/blog/rabbitmq4/"},{"categories":["其他技術分享"],"content":" 目錄 RabbitMQ 安裝教學 Web 管理介面導覽 查看 Queue 的整體運行情況 查看/新增 Exchange 查看/新增 Queue Admin 管理 結語 RabbitMQ 安裝教學 # 需要先安裝 docker-desktop，可以到 Docker 官網下載\n可以使用 docker 在自己的電腦上快速架設起 RabbitMQ 以及 RabbitMQ 專屬的 Web 管理介面。\n推薦使用 rabbitmq:management 的 docker image，不僅可以幫我們架設起一個 RabbitMQ，還可以順便為我們架設起一個用來管理該 RabbitMQ 的 web 管理介面，非常方便。\n執行以下方法，就可以順利啟動這個 docker image：\ndocker run --name rabbitmq -d -p 15672:15672 -p 5672:5672 \\ -e RABBITMQ_DEFAULT_USER=root -e RABBITMQ_DEFAULT_PASS=admin1234 \\ rabbitmq:management 在上面的方法中，可以自己修改 RABBITMQ_DEFAULT_USER 和 RABBITMQ_DEFAULT_PASS 的參數去自定義帳號密碼，像是此處就是設定帳號為 root、密碼為 admin1234。\n啟動好 docker image 之後，RabbitMQ 使用的 port 預設是 5672，而 RabbitMQ 的 Web 管理介面使用的 port 預設是 15672，因此只要訪問 http://localhost:15672，就會出現 RabbitMQ 的 Web 管理介面，此時就可以在輸入剛剛運行 docker image 所設定的帳號密碼登入進去。\n到這裡 RabbitMQ 就安裝好了，所以就可以使用 SpringBoot 連接上此 RabbitMQ 了。\nWeb 管理介面導覽 # 查看 Queue 的整體運行情況 # 登入之後就會進到 Overview 頁面，在 Overview 頁面可以查看 Queue 的整體狀況以及 cluster node 的 cpu/memory 使用狀態。\n查看/新增 Exchange # 點擊上方的 Exchanges tab 可以進到 Exchanges 頁面，可以查看目前已存在的 Exchange，以及新增一個新的 Exchange。\n點擊其中一個已存在的 Exchange，可以查看該 Exchange 的詳細資訊：\n查看/新增 Queue # 點擊上方的 Queue tab 可以進到 Queue 頁面，可以查看目前已存在的 Queue，也可以新增一個新的 Queue。\n同理，點擊其中一個已存在的 Queue，可以查看該 Queue 的詳細資訊，也可以對該 Queue 進行一系列的操作（這裡和 Exchange 的操作差不多，因此就不重複列出）。\nAdmin 管理 # 點擊上方的 Admin tab 可以進到 Admin 頁面，然後在 Admin 裡點擊右側的 Users tab，可以查看/新增 RabbitMQ 的使用者，也可以查看該使用者允許 access 的 virtual host。\n點擊右側的 Virtual Hosts tab，可以查看/新增 RabbitMQ 的 virtual host。\n補充： 在 RabbitMQ 裡面有一個 virtual host 的概念，可以想像成是分組，也就是一個 virtual host 就是一組。\n當 RabbitMQ 運行時，預設會產生一個 virtual host 叫做 /，然後如果不特別調整的話，所有的 Queue 都是創建在這個 / 的 virtual host 裡面，而 user 預設也是被設定成能存取 /。\n所以如果想要在 RabbitMQ 裡面做分組的權限控管的話，只要多創建幾個 virtual host，user 就可以在不同的 virtual host 下創建 Queue 和 Exchange，不同 virtual host 裡的 Queue 和 Exchange 無法互通，然後也可以去限制說某些 user 只能存取某些 virtual host。\n結語 # 這篇文章我們介紹了如何透過 Docker 安裝 RabbitMQ，並且也介紹了 RabbitMQ 管理介面中的使用方法，了解要如何透過 RabbitMQ 管理介面新增、查看 Queue 和 Exchange 的資訊。\n如果你對 RabbitMQ 的其他文章有興趣，也可以直接點擊下列連結跳轉到該文章：\nRabbitMQ 介紹（一）- 什麼是 Message Queue？ RabbitMQ 介紹（二）- RabbitMQ 用法介紹 RabbitMQ 介紹（三）- RabbitMQ 安裝教學（本文） RabbitMQ 介紹（四）- 在 Spring Boot 中實作 RabbitMQ 如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2024-10-23","objectID":"91e16b378a5ceeffbc9b2723d9507f18","title":"RabbitMQ 介紹（三）- RabbitMQ 安裝教學 + Web 管理介面導覽","url":"https://kucw.io/blog/rabbitmq3/"},{"categories":["其他技術分享"],"content":" 前期提要：如果不了解什麼是 Message Queue，可以先參考 RabbitMQ 介紹（一）- 什麼是 Message Queue？ 的介紹\n目錄 什麼是 RabbitMQ？ RabbitMQ 中常見的 5 種模式 1. Direct 模式 2. Worker 模式 3. Publish/Subscribe 模式 4. Routing 模式 5. Topics 模式 RabbitMQ 總結 結語 什麼是 RabbitMQ？ # RabbitMQ 是在各企業中最為廣泛使用的 Message Queue，而在 RabbitMQ 的世界中，有三個重要的角色需要了解，分別是：\nProducer（生產者）：負責發送 message 到 Queue Consumer（消費者）：負責從 Queue 中接收 message Queue：暫存 message 的地方 以下圖為例，綠色的「訂單系統」是 Producer，藍色的「數據分析系統」是 Consumer，而橘色的「Message Queue」則是 Queue。\n在 RabbitMQ 的世界，Producer、Consumer、以及 Queue，是最重要的三個名詞，因此先了解這些名詞的意義，可以說是認識 RabbitMQ 的第一步。\nRabbitMQ 中常見的 5 種模式 # 在 RabbitMQ 中，常見的 5 種模式如下（所有模式可參考 RabbitMQ 官網）：\nDirect 模式 Worker 模式 Publish/Subscribe 模式 Routing 模式 Topics 模式 1. Direct 模式 # Direct 是最簡單的模式，也就是只有一個 Producer 負責發送 message 到 Queue 裡，並且也只有一個 Consumer 會從 Queue 中接收 message，接著處理該 message。\n像是以上面的「訂單系統」和「數據分析系統」的例子來說，他們其實就是使用最簡單的 Direct 模式，由「訂單系統」這個 Producer 負責發送 message，並且由「數據分析系統」這個 Consumer 接收 message。\n2. Worker 模式 # Worker 模式和 Direct 模式很像，差別只在 Worker 模式「同時有多個 Consumer」一起去接收 Queue 裡面的 message，進而提升 message 消化的速率。\nWorker 模式是 RabbitMQ 中滿常用的一個模式，假設今天 Queue 突然有很多 message 要處理，就可以多叫幾台 Consumer 來一起消化，因此可以輕易的達成「橫向擴展」，在實作上非常的好用！\n3. Publish/Subscribe 模式 # Publish/Subscribe 模式也是 RabbitMQ 中很常用的一種模式。從這個模式之後，在 Producer 和 Queue 之間，就會多出一個叫做 「Exchange」 的角色。\n所以在 Publish/Subscribe 模式中，Producer 就不會直接把 message 丟到 Queue 裡面了，Producer 反而是會先把 message 丟給 Exchange，然後再交由 Exchange 去決定要把這個 message 丟到哪個 Queue 裡面。\n所以換句話說的話，就是 Producer 再也不會直接和 Queue 溝通了，而是會藉由一個中間人 Exchange 去處理這樣。\n而在 Exchange 中，他有 3 種類型（type）可以選（direct、fanout、topic），所以在下面不同的模式中，就會搭配不同的 Exchange 設定來實作。\n像是在 Publish/Subscribe 的模式中，Exchange 使用的是 fanout 設定，也就是當 Producer 把 message 丟給 Exchange 時，Exchange 會把這個 message 丟到「他綁定的所有 Queue」中。\n舉例來說的話，假設今天 Producer 發送了一個 message 給 Exchange，那麼 Exchange 除了會將這個 message 丟到上面那條 Queue 裡面之外，同時 Exchange 也會 「複製一份 message」，然後將他丟到下面那條 Queue 裡面。\n所以在上下這兩條 Queue 中，裡面都會各自有一份 message 存在！\n因此即使 Producer 只傳送了一個 message 給 Exchange，Exchange 也會透過「複製」的方式，在每一條 Queue 中都添加同樣的 message，這樣子就可以確保所有的 Consumer 都可以接收到這個 message 了！\n也因為 Publish/Subscribe 模式的特性非常好用，因此通常在實作「訂閱」的情境下，就會採用 Publish/Subscribe 模式來實作。\n像是你在 Facebook 上追蹤了某個粉專，只要該粉專發文，你就會收到貼文通知，這就是一種 Publish/Subscribe 的應用，即是「粉專主（Producer）」發送一個貼文給 Exchange，而「每一位粉絲（Consumer）」都可以收到這個貼文的通知。\n而如果以前面提到的例子來說的話，假設今天除了「數據分析系統」想要取得訂單數據之外，其他的系統如「物流系統」、「倉管系統」也都想要取得訂單數據的話，那麼這時就可以採用 Publish/Subscribe 的方式來實作，這樣子「訂單系統（Producer）」一樣是丟出一份訂單數據給 Exchange，而「每一個想要獲得通知的微服務」就都可以從 Queue 中取得到這筆訂單數據了！\n4. Routing 模式 # Routing 模式也是一個用到 Exchange 的模式，他所使用的是 Exchange 的 direct 設定。\n在 Routing 模式中，當 Producer 將 message 丟給 Exchange 時，Producer 同時要在這個 message上面添加一個 routing key，因此 Exchange 就會根據這個 routing key，將 message 丟到指定的 Queue 上（Exchange 和 Queue 之間要用什麼 routing key 綁定，需要先行設置)。\nRouting 模式雖然使用上沒有 Publish/Subscribe 模式來的常用，但是他的重點在於 「可以多重綁定」，也就是同一個 routing key 可以綁到多條 Queue 上，因此就可以拿來實作收集 Log 的 Queue。\n像是在下圖中，我們可以將 info、error、warning 這三個 routing key，綁到記錄一般 Log 的 Queue 上，然後再將 error 這個 routing key，再綁定到另一條記錄 Error Log 的 Queue 上，這樣子就可以實作出一份帶有全部 Log 的 Queue、以及一份只有 Error Log 的 Queue 了。\n不過，雖然使用 Routing 模式可以很容易的實作出上述的收集 Log 的 Queue，但是因為他的重要邏輯都寫在 Exchange 上，所以如果對 RabbitMQ 或是 Exchange 不太熟練的話，就會不知道要去哪裡改這些設定（Exchange 的設定要進到 RabbitMQ 的系統中改）。\n因此在實作上，建議大家是慎用這個模式，除非團隊中的每個人真的都很熟悉 RabbitMQ，不然的話用這個模式可能會導致後期維護不易。\n5. Topi","date":"2024-10-22","objectID":"f6fde12fda103096a161745f55199a4a","title":"RabbitMQ 介紹（二）- RabbitMQ 用法介紹","url":"https://kucw.io/blog/2020/11/rabbitmq/"},{"categories":["其他技術分享"],"content":"RabbitMQ 是目前使用上最廣泛的 Message Queue，但是在了解 RabbitMQ 之前，必須先了解 Message Queue 的概念和特性，才會比較好上手。\n因此此系列文會先從 Message Queue 開始介紹，接著介紹 RabbitMQ 的用法、如何在電腦上安裝 RabbitMQ、以及如何在 Spring Boot 中實作 RabbitMQ。\n如果你對 RabbitMQ 的其他文章有興趣，也可以直接點擊下列連結跳轉到該文章：\nRabbitMQ 介紹（二）- RabbitMQ 用法介紹 RabbitMQ 介紹（三）- RabbitMQ 安裝教學 RabbitMQ 介紹（四）- 在 Spring Boot 中實作 RabbitMQ 而在這篇文章中，我們就會先來介紹什麼是 Message Queue，所以我們就開始吧！\n目錄 什麼是 Message Queue？ 例子：郵差送信 Message Queue 的具體應用場景 Message Queue 的優缺點 Message Queue 的優點 Message Queue 的缺點 結語 什麼是 Message Queue？ # Message Queue 是一種系統設計的技術，大陸是翻譯成「消息隊列」，台灣這邊好像沒有準確的譯名，所以通常大家還是直接用 Message Queue 來稱呼。\n而 Message Queue 的用途，就是「信箱」的概念，因此 Message Queue 就擁有了「非同步處理」以及「功能解耦」的兩大優點。\n上面的描述看起來可能有點抽象，所以我們可以直接透過一個例子，來了解 Message Queue 的概念。\n例子：郵差送信 # 舉例來說，假設你家沒有郵箱的話，那麼今天郵差如果想要送信給你，郵差就只能夠來你家按門鈴時，然後你就得從沙發上爬起來、走到樓下開門、並且當著郵差的面簽收這封信，這樣子你才能夠拿到這封信。\n所以在沒有信箱的時代，你和郵差「必須同一時間」出現在家裡大門口，這樣子郵差才能把信件親手交給你。\n但是，如果我們使用了「信箱」的話，狀況就不一樣了！\n如果你家有安裝信箱的話，那麼郵差想要送信給你時，就只要直接把信投遞到你家的信箱裡面，然後郵差就可以繼續去處理其他事情了，完全不用等你下樓來開門！而你也可以等到之後有空時，再自己下來收這封信。\n因此在有了信箱之後，你和郵差就「不需要同一時間」出現在家裡大門口，而是郵差先把信件投遞到信箱中，並且將這封信暫存在信箱裡，等到你之後有空時再來收信。\n因此「信箱」的概念，就是 Message Queue 的用途，也就是能夠 「暫存消息的傳遞」，進而達到兩個服務之間的非同步處理！\nMessage Queue 的具體應用場景 # 透過上面的例子，先大概了解「Message Queue = 信箱」這個概念之後，接著我們也可以用更具體的真實情境，來介紹 Message Queue 的用途。\n舉例來說，假設現在在電商網站中有兩個微服務：訂單系統、數據分析系統，那麼當使用者下了一筆訂單之後，這時候訂單系統就必須 call 數據分析的 api，將使用者購買了哪些商品傳給數據分析系統，讓他背後去進行大數據分析。\n所以在沒有使用 Message Queue 之前，微服務的架構會像是下面這樣子：\n這個架構也沒有說不好，但是想像一下，假設今天數據分析系統出現問題，導致 call api 請求超時、或是直接出現 500 錯誤，那這樣子就會導致使用者沒辦法成功下訂單：\n但是對於使用者而言，他根本不想要使用數據分析系統！他只是想要下訂單而已，但是卻會因為數據分析系統出現問題，導致使用者被連累著也出現問題。\n所以這個時候，就可以透過引入「Message Queue」這項技術，來解決這個問題！\n假設我們在「訂單系統」和「數據分析系統」之間，添加了一組 Message Queue 的話，那麼步驟就會變成下面這樣：\n使用者下單 訂單系統處理訂單的邏輯 訂單系統將這筆訂單的數據，傳送到 Message Queue 裡面來暫存 回傳下單的結果給使用者 所以當我們使用了 Message Queue 之後，在使用者下訂單的過程中，完全不會受到數據分析系統的干擾，而是可以專心的執行下訂單的操作了。\n而數據分析系統也可以在自己有空時，主動到 Message Queue 中查看是否有尚未處理的訂單數據，如果有的話，就抓取這一筆訂單數據來處理。並且他也可以慢慢的去分析這筆訂單數據，不用為了要搶快、要盡快回覆使用者，而逼迫自己要分析的非常迅速。\n所以透過 Message Queue 的設計，我們就可以將「訂單系統」和「數據分析系統」的功能給拆分開來，讓他們不需要同一時間處理完所有步驟，而是可以分批步驟慢慢執行，進而達到「非同步處理」以及「功能解耦」的效果了！\n所以最終的架構圖就會如下圖所示：\nMessage Queue 的優缺點 # 了解了 Message Queue 的具體使用情境之後，接著我們也可以來探討一下使用 Message Queue 的優缺點。\nMessage Queue 的優點 # 使用 Message Queue 有四大優點：\n非同步處理（Asynchronous Processing） 功能解耦（Decoupling） 易於橫向擴展（Horizontal Scalability） 流量速率限制（Rate Limiting） 其中關於「非同步處理」和「功能解耦」的部分，在上面的例子中已經有介紹到這一塊了。\n而至於「易於橫向擴展」和「流量速率限制」這部分，因為比較複雜，大家有興趣的話，可以再參考 ByteByteGo 的相關介紹。\nMessage Queue 的缺點 # 而至於使用 Message Queue 的缺點，則有：\n系統變得更複雜、增加維護成本（因為我們多添加了一個新功能到微服務架構中，勢必得多花一份心力去維護他） 上游無法知道下游的執行結果為成功 or 失敗 消息可能會丟失、或是會重複傳遞 也因為 Message Queue 在使用上不是萬能的，所以大家在引入 Message Queue 進來之前，一定要仔細思考：「這項技術是否適用於目前的架構？他是否真的能解決目前你遇到的問題？你是否願意多維護一份功能？」，只有當 Message Queue 真的能解決你的問題時，才有引入他的意義。\n而這也是大家想要邁向資深工程師的必經過程，即是有能力評估某一個功能的優缺點，並且決定是否要採用此項技術。\n不過老實說 Message Queue 是真的滿好用的啦🤣，而且也真的很常見，所以多了解一點是絕對不會虧的！\n結語 # 這篇文章我們先介紹了 Message Queue 是什麼、並且也比較了 Message Queue 的優缺點，希望能讓大家對 Message Queue 的用途有更多的了解。\n如果你對 RabbitMQ 的其他文章有興趣，也可以直接點擊下列連結跳轉到該文章：\nRabbitMQ 介紹（二）- RabbitMQ 用法介紹 RabbitMQ 介紹（三）- RabbitMQ 安裝教學 RabbitMQ 介紹（四）- 在 Spring Boot 中實作 RabbitMQ 如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2024-10-21","objectID":"8676a9b63fbd39ca3d7f8df16e889f25","title":"RabbitMQ 介紹（一）- 什麼是 Message Queue？","url":"https://kucw.io/blog/rabbitmq1/"},{"categories":["其他技術分享"],"content":"哈囉大家好，我是古古。身為一個工程師，在工作上多多少少都得和 PM 打交道，可能有的人會好奇 PM 的工作內容是什麼、或是不知道怎麼跟 PM 相處比較好。\n所以這篇文章，我們邀請到了 PM 大大來為我們工程師們解惑一下，分享 PM 的工作內容是什麼、以及工程師如何和 PM 共事，所以我們就開始吧！\n前言 # Hello 我是 Eileen，目前任職於 Hahow 好學校的產品經理，約擔任 PM 經歷 4 年，同時也是 Taiwan Product Managers 臉書社團的管理員，近一年舉辦了許多產品經理及軟體產品從業者的小聚，希望透過這篇文章跟大家分享及交流。\n也容許我在開始前打個預防針，產品經理的日常幾乎沒有正確答案，只有最合適的答案，因此在不同團隊中可能有所差異。本篇文章僅能代表我自身體驗及所觀察到周遭產品經理的經驗！那就讓我們開始吧～\n目錄 前言 1. Project Manager（PJM）和 Product Manager（PDM）的差異 2. PM 的工作範圍到哪裡？如果遇到灰色模糊地帶的事情，如何權責劃分？ 3. PM 都是如何規劃一個專案的發想到執行？有沒有推薦的課程或是文章可以參考？ 4. 想知道 PM 的職責會幫忙拆分 User Story 嗎？ 5. PM 是如何評估專案工時的？ 6. 請問 PM 面對比較陌生的開發專案，例如：要修改很陳舊的專案，接手的工程師都不知道要怎麼抓開發時間，PM 要怎麼抓時程？ 7. RD 後期真的都會轉 PM 嗎？網路上看到不少案例 8. 當 PM 說「他不懂技術」、或是「不要跟他說技術」時，該怎麼辦？ 9. RD 和 PM 之間應該如何和平相處呢？有時候和 PM 溝通都會很無力\u0026hellip; 總結（by 古古） 1. Project Manager（PJM）和 Product Manager（PDM）的差異 # 這題問題是我在擔任 PM 第一年，最常被 PM 前輩問到的 101 問題，「你覺得產品經理跟專案經理有什麼不一樣」？網路上有許多文章說明產品經理與專案經理的差異，以下先列出普遍對於專案經理及產品經理職務上的要求差異。\n產品經理（PDM）：\n主要負責產品的整體策略和方向 關注產品的長期發展和市場需求 決定產品功能和優先順序 與各個部門協調，確保產品符合公司目標和用戶需求 專案經理（PJM）：\n負責具體專案的執行和時程管理，以確保專案按時完成 管理專案資源和團隊協調 控管專案中的風險和問題 簡單而言，產品經理（PDM）專注於「價值交付」，專案經理（PJM）則專注於「專案交付」。\n舉例而言，如果是一個串流影音平台，產品經理可能會負責與營運、業務行銷單位討論預計推出新的影片類型、預計在市場上的定位及策略進而思考對應功能（如互動式影片、離線觀看等），以滿足用戶需求並提升平台競爭力。而專案經理則可能負責管理這些新功能的開發進度，確保各個團隊（如開發、設計、內容）能夠協調一致，按時完成專案。\n雖說如此，在實際的日常中，這兩個職稱的界限可能會有所模糊。許多公司的PM職位可能是複合型的，例如：在多數公司並沒有額外招募專案經理，而是由產品經理身兼專案經理的角色。或雖然職稱掛產品經理，卻實際上被期待作為客戶經理、業務的角色。\n各家 PM 實際要負責的範圍五花八門，我和幾位 PM 朋友也常開玩笑說「工程師不做的事就是 PM 的守備範圍」，因此當在分工有疑惑時，建議大家可以參考自己公司的 PM 的 JD（職缺敘述） 或直接跟自己主管、或 PM 聊聊！\n2. PM 的工作範圍到哪裡？如果遇到灰色模糊地帶的事情，如何權責劃分？ # 延續上題，由於各個產品團隊所包含的成員不同，例如是否擁有 UI designer、UX Researcher、數據分析師、測試工程師等，影響了 PM 所扮演的角色。而當 PM 所需擔任的角色越多，時間固定情況下，可以做到的顆粒度，與勢必有所影響。\n我自己認為 PM 的工作在一個專案中（Epic 或 Story），為了避免認知不同造成誤會或浪費工程資源，至少須確保以下內容有被定義：\n目標定義： 開發這個功能是為了達成什麼目的？我們的專案規模是多大？是 POC 或是一個長期使用的功能？了解目標也可以幫助工程師想到更好的解法及考慮維運性。 功能想像對焦： 不管是流程圖、PRD（規格文件）、wireframe、設計稿、技術開發文件，目的都是用來對齊團隊成員對於功能的想像跟期待，最終期待是所有人能夠具備共識。 驗收條件定義： 驗收的內容包含但不限於：開發是否如設計稿、是否使用者正向流程及負向流程都被考量、Bug 的優先序定義等，甚至如果公司內有多個團隊，本次上線是否對於其他產品或功能造成影響，都是 PM 在功能發布前須考慮的內容。 即使如此我想大家還是有很多疑問，常見的灰色地帶，例如：帶領會議究竟是技術主管還是 PM 負責？PM 到底要不要畫出流程圖？萬一沒有設計稿，工程師要自己做嗎？ 答案是「都可以」（被揍）XD\n在不同的公司、不同團隊甚至同一個團隊的不同專案都有可能有所差異，這一切都要取決於 「團隊所擁有的資源」。例如：我們有一位 App 工程師主管為了讓使用者體驗更好，在沒有設計資源的情況下，主動發起了 Dark mode 的導入，再請設計師做 design review ，反轉了由 PM 主導需求的角色。\n因此，如果遇到灰色地帶時，最好的方式就是在 retro 當中提出，甚至工程團隊也可以發起討論會議，了解彼此的期待及難處。希望能讓大家理解，讓開發順利推動是所有專案成員的責任，大家的付出都會幫助團隊運作更為順暢！\n3. PM 都是如何規劃一個專案的發想到執行？有沒有推薦的課程或是文章可以參考？ # 古古補充：這段比較多專有名詞，如果覺得有點複雜可以先跳過這題\n在目前的公司，我們透過公司層級定義的 OGSM（年度目標）來制定產品部門及組別層級的 OGSM。這包含了目標、策略（如何實現）以及衡量指標（如何確認完成）。\n舉例來說，今年的一個目標是能夠檢驗和衡量使用者的學習成效。經過對國內競品、市場分析、國外領導產業的研究，以及團隊腦力激盪後，我們選擇了透過「測驗」功能來量化學習效果作為策略。考慮到 Hahow 現有的技術架構、使用者和創作者的使用習慣，我們發現要提供「測驗」服務，需要完成 A、B、C 三項任務。為了在目標期限內完成這項策略，我們還定義了這三個專案各自的目標、時程安排，以及可用的工程資源和時間。\n接下來，PM 會根據這些資訊進行更深入的競品調查和使用者研究，以了解「測驗功能」（Epic）需要滿足哪些使用者情境（Story）及其具體流程。這個階段也就是撰寫「產品規格文件」（PRD，Product Requirement Document）的必要環節。由於這個專案預計運用 AI 技術，我們還額外進行了工程 POC 來評估新技術的可行性。確認可行後，我們將任務交給產品設計師進行 Wireframe 和設計稿的繪製，並反覆討論使用者體驗和操作介面。在這個過程中，我們也邀請工程師評估設計流程的開發可行性。\n最後，我們進入 Scrum 流程中的 Pre-refinement、Refinement 和 Planning 會議，將任務交給工程團隊進行 Task 的切分和開發實作。但這還不是終點。為了確保測驗功能上線後能達到目標使用人數並驗證假設，我們還需要規劃數據埋點，以及與行銷團隊討論產品行銷策略。上線後，PM 還需要負責數據分析和後續迭代規劃。\n每一個大型專案上線都像生了一個孩子，從發想到執行的過程總是充滿驚喜（嚇）。也幸好團隊中擁有產品設計師、工程師和測試工程師各自的專業，透過不斷的討論，才能確保產出最佳的解決方案。\n","date":"2024-10-15","objectID":"f7dc5e167f6e5ee522fdafbd5c7e288c","title":"工程師如何和 PM 共事？PM 親自來解答！","url":"https://kucw.io/blog/pm-for-developer/"},{"categories":["其他技術分享"],"content":"Git 可以說是工程師使用上最頻繁的技術，這篇文章會分享兩個我覺得 Git 的好用技巧給大家，分別是：\nCherry-Pick： 擷取某個 branch 中的某個 commit git reset HEAD^： 撤銷最新的 commit 所以我們就開始吧！\n目錄 Git 好用技巧之一：Cherry-Pick（擷取某個 branch 中的某個 commit） Git 好用技巧之二：git reset HEAD^（撤銷最新的 commit） 結語 Git 好用技巧之一：Cherry-Pick（擷取某個 branch 中的某個 commit） # 如果說 Git 有什麼隱藏的特殊技能的話，Cherry-Pick 絕對是我心目中的第一名！\nCherry-Pick 的用途，是「擷取某個 branch 中的某個 commit」，這個聽起來是有點抽象，所以下面就透過一個實際的例子來示範 Cherry-Pick 的用法。\n像是目前在 master branch 中，在 resources 資料夾底下只有一個 application.properties 的檔案：\n如果這時候，我們切到 test 這個 branch 上，然後在 resources 底下新增一個 123.txt 的檔案，並且成功 commit 的話，結果就會像下圖一樣，在「步驟 3」的地方新增了一個 123 的 commit 出來。\n這時如果再接著新增一個 456.txt 的檔案，並且也成功 commit 的話，結果就會和下圖一樣，在「步驟 2」的地方新增了一個 456 的 commit 出來。\n所以到目前為止，在 test branch 就新增了兩個 commit：\n一個是 123 的 commit（包含 123.txt 檔案） 另一個則是 456 的 commit（包含 456.txt 檔案） 而 master branch 中則是維持原樣，沒有任何新增的 commit，resources 底下也只有 application.properties 一個檔案。\n這時候，神奇的需求來了！！如果現在有人要你切回到 master branch，並且要求你將 test branch 中的 123.txt 的檔案移植到 master branch 的話，你該怎麼辦？\n在還不會使用 Cherry-Pick 時，第一直覺反應是將 test merge 進 master，並且想辦法把 456 的 commit 刪掉，這樣子就能夠只保留 123 的 commit。\n這種做法雖然也能夠解決問題，但是當 commit 數一多的時候，就會變的很混亂，無法確保自己是否有保留到正確的 commit。\n所以這時候，就是 Cherry-Pick 上場的時候了！\n當我們在 master branch，想要擷取 test branch 中的某個 commit 時，只要在 IDE 上點擊右鍵（在這個例子中，就是對 123 這個 commit 點擊右鍵），然後選擇 Cherry-Pick：\n點擊之後，就可以看到在 master branch 的 resources 資料夾中，就多出了一個 123.txt 的檔案！並且在 master branch 中，也多出了一個新的 commit 123，所以這也就表示，我們就成功的將 test branch 中的 commit，偷到 master branch 中了！\n因此大家以後如果有「擷取別的 branch 中的某個 commit」的需求時，就可以使用 Cherry-Pick 幫助我們非常方便的做到！\n補充：Cherry-Pick 的「偷」的動作不是真的偷過來，而是將 test branch 中的 123 commit 中的內容，複製同樣的一份到 master branch 上，所以 master branch 的 123 commit 和 test branch 中的 123 commit，兩者之間是沒有任何關係的，在 Git 的線圖上，他們會是兩個獨立的 commit。\n雖然 Cherry-Pick 的使用情境看起來真的很少見，但老實說我在實際的工作中真的有用過一兩次，而且那次幫助真的滿大的，超級感謝發明 Cherry-Pick 的人XD。\n那時候我是在某條 branch（ex: TICKET-112）上開發，但是後來因為各種原因所以要棄用這條 branch，換到另一條 branch 開發（ex: TICKET-917），這時候我就在新的 branch 上使用 Cherry-Pick 去拉有用的 commit 過來，真的很好用！！\nCherry-Pick 就是屬於一年不開張、開張吃一年的指令，建議大家可以先把他的情境記下來，等到將來有一天真的也遇見這種神奇的狀況時，再回頭來查看他的使用方法就好～\nGit 好用技巧之二：git reset HEAD^（撤銷最新的 commit） # git reset HEAD^ 是我個人愛用的 Git 指令，他可以用來撤銷最新的 commit，但是不會刪除該 commit 中的內容，所以適合用在「上一個 commit 沒寫好，想要重新 commit」的情境上。\n舉例來說，目前在 test branch 中，有兩個 commit：\n一個是 123 的 commit（包含 123.txt 檔案） 另一個則是 456 的 commit（包含 456.txt 檔案） 如果這時候我發現「啊！我少添加了一個 789.txt 的檔案」，那麼我就可以先執行 git reset HEAD^ 的指令，先將 commit 456 給撤銷（注意此時 456.txt 的檔案仍舊存在，只是變回尚未 commit 的狀態）：\n此時我就可以自由添加想要的程式（ex: 789.txt），然後後續就可以將 456.txt 和 789.txt 一起加到 commit 裡，將他們存放在同一個 commit 中。\n之所以會特別介紹 git reset HEAD^，是因為我在工作上還滿常用這個指令的。根據以往的經驗，我發現將多個相關的檔案儲存在同一個 commit 裡面，這樣後續如果要排查程式的話，放在同一個 commit 中的檔案會比較好查。\n像是在 IntelliJ 這個 IDE 中，點擊「456 + 789」的 commit，就會在右側呈現這一次 commit 中的檔案有哪些，所以就可以透過這個線索，去查詢和他有相關的改動為何。\n所以在實作上，我個人會偏好盡量把相關的程式 commit 在一起，也算是幫助自己後續排查問題時比較好查詢這樣。\n不過老實說 commit 的頻率和檔案數，這個真的就很看大家自己的喜好，所以大家就自由參考，或是說如果有什麼更好的做法，也歡迎在下方留言！\n補充：如果想要在 IntelliJ 中達到 git reset HEAD^ 的效果，可以直接在該 commit 上點擊「Undo Commit」，這樣子就可以得到和 git reset HEAD^ 一樣的「撤銷 Commit」的效果了！\n結語 # 這篇文章我們有分享了 Git 中的好用技巧：Cherry-Pick 和 git reset HEAD^ 給大家，雖然這兩個指令的使用頻率不是很高，但是這兩個都是我自己用過，真心覺得好用的指令～\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85","date":"2024-10-08","objectID":"b2173f3817c7d0d77e13397a7327a048","title":"Git 的好用技巧介紹 - Cherry-Pick 和 git reset HEAD^","url":"https://kucw.io/blog/git-tip/"},{"categories":["其他技術分享"],"content":"對於軟體工程師來說，「撰寫單元測試」已經是一個必須具備的技能，撰寫好的單元測試不僅能夠確保程式運行正確，也能夠降低後期重構、維護的成本，可以說是一舉多得！\n而在單元測試越來越重要的情況下，就誕生了 「TDD（測試驅動開發）」 的開發流程，因此這篇文章我們就來介紹一下，TDD 到底是什麼、以及要如何使用 TDD 來開發程式吧！\n目錄 什麼是 TDD（Test-Driven Development）？ 傳統的開發流程 TDD 的開發流程 步驟 1：只宣告功能的方法名稱，不實作內部程式 步驟 2：撰寫單元測試，並且單元測試運行失敗（紅燈） 步驟 3：實作主功能程式 步驟 4：單元測試運行成功（綠燈） 小結：TDD 開發流程 vs 傳統開發流程 TDD（Test-Driven Development）總結 結語 什麼是 TDD（Test-Driven Development）？ # TDD 的全稱是 Test-Driven Development，中文翻譯為「測試驅動開發」，而 TDD 的核心理念，就是 貫徹「先寫測試、再寫開發」的精神。\n也因為 TDD 的核心理念「先寫測試、再寫開發」比較抽象，所以以下我們就直接透過一個例子，來比較一下「傳統的開發流程」和「使用 TDD 的開發流程」的差別在哪裡。\n傳統的開發流程 # 一般我們在開發程式時，比較常見的流程為：\n接到需求 開始寫程式，實作該功能出來 最後撰寫單元測試，確保功能運行正確 所以在傳統的開發流程中，當我們拿到需求時，基本上就是先動手寫程式，先去實作該功能出來，而等到功能都實作的差不多之後，最後再補足單元測試，確保程式運行正確，這樣子就算完成這個功能的開發了。\n舉例來說的話，假設我們現在要實作一個「計算機的加法功能」，那麼我們就可以先寫出下圖左的程式（以 Java 為例），去實作加法功能出來。\n而當我們實作完左邊的加法功能時，就可以再去撰寫右邊的單元測試，使用單元測試去驗證我們所寫的程式是否正確，因此到最後，我們就實作完「主功能程式」和「單元測試」的程式了。\n所以上述這樣子的做法，就是最常見的傳統開發流程，也就是照著「先寫開發、再寫測試」的步驟，去完成一個功能的開發。\nTDD 的開發流程 # 而在了解了傳統的開發流程之後，接著我們就可以來介紹 TDD 的開發流程是什麼了！\n步驟 1：只宣告功能的方法名稱，不實作內部程式 # 當我們使用 TDD 時，首先第一步，就是 「只宣告功能的方法名稱，但卻不實作他的內部程式」。\n所以舉例來說的話，假設我們想要實作「計算機的加法功能」，那我們第一步，就只能夠先去宣告這個 add() 方法，但是不能夠去實作這個功能內部的程式。\n這一步對於習慣傳統開發流程的工程師來說，其實就是非常反人類的一步了😂，人生從來沒遇過只需要宣告方法名稱、但是不需要實作程式的要求！\n但是沒關係，建議大家就先接受 TDD 就是這樣的流程，所以當我們使用 TDD 時，第一步就是只宣告方法名稱，但是不實作他的內部程式。\n步驟 2：撰寫單元測試，並且單元測試運行失敗（紅燈） # 宣告完方法之後，接著也是很反人類的第二步來了。在第二步中，我們就要 「先根據這個方法的用途，憑空寫出他的單元測試出來」。\n舉例來說的話，因為我們想要實作的是「計算機的加法功能」，所以邏輯上，這個功能應該要能夠正確執行加法的操作才可以。因此我們可以直接在右邊撰寫一個單元測試，並且在這個單元測試中，就去測試 add() 方法的行為是否正確。\n所以在這個單元測試中，當我們去 call add(1, 2) 方法時，就「預期」他的返回結果應該要是 3 才對（因為抽象的邏輯意義上，add() 方法就是要去執行加法操作，所以 1+2 要等於 3 才對），程式如下圖右所示。\n所以在 TDD 的第二步中，我們就會直接根據這個方法的抽象邏輯，直接去撰寫單元測試出來，去「預期」這個方法的行為為何。\n不過當然啦，因為我們目前在 add() 方法中還沒有實作任何一行程式，所以當我們運行這一段單元測試的程式時，單元測試就會運行失敗，因此產生「紅燈」的結果（雖然此時單元測試運行失敗，但是這也是 TDD 的開發流程一環，因此這裡出現紅燈是正常現象）。\n老實說步驟二是大家一開始在接觸 TDD 時，最容易碰壁的一步，因為通常我們在實作程式時，會一不小心就往下鑽研去思考實作細節，但是卻忽略了「這個方法本身的用途」為何。\n因此在使用 TDD 開發程式時，我們就得換個角度來思考，在真正實作程式的細節之前，反而是要先從大方向去規劃每一個方法的用途為何，這樣子後續在實作程式時，才能夠寫出功能拆分完整、職責分明的程式了！\n步驟 3：實作主功能程式 # 在我們經歷了第二步最難熬的一關之後，後續的步驟就比較容易了！\n因為在第二步中，我們已經寫好測試 add() 方法的單元測試了，所以在第三步中，我們就可以回頭去實作 add() 方法中的邏輯，將這個功能真正的實作出來。\n步驟 4：單元測試運行成功（綠燈） # 而在我們實作完 add() 方法中的程式之後，這時候我們再回頭去運行單元測試，理論上就要能夠運行成功了！因此當我們運行單元測試時，單元測試就會顯示「綠燈」，表示單元測試成功通過。\n所以透過 TDD 的開發流程，我們最終也是可以實作完「主功能程式」和「單元測試」的程式了！\n小結：TDD 開發流程 vs 傳統開發流程 # 所以透過這兩個流程也可以發現，不論我們使用的是「傳統的開發流程」、還是「TDD 的開發流程」，我們最終所實作完的程式，其實是一模一樣的，就是會包含「主功能程式」和「單元測試」這兩部分，因此不論我們使用哪一種方式來開發，最終的產出都是一樣的。\n只不過 TDD 相對於傳統的開發流程而言，就是提供了另外一種開發思路給我們，讓我們在真正實作程式的細節之前，能夠先從大方向去規劃每一個方法的用途為何，進而寫出功能拆分完整、職責分明的程式了！\nTDD（Test-Driven Development）總結 # 所以總結上述的介紹的話，現在我們回頭來看 TDD 的定義，就可以更了解 TDD 的核心理念是什麼了。\nTDD 的全稱是 Test-Driven Development，中文翻譯為「測試驅動開發」，也因為 TDD 是透過「撰寫單元測試」去驅動「程式的開發」，所以 TDD 的核心理念，就是貫徹「先寫測試、再寫開發」的精神。\n而 TDD 的開發流程，也可以總結成下面這張圖：\n所以大家如果有興趣想嘗試 TDD（測試驅動開發）的話，就只要照著上述的步驟實作，就可以體驗 TDD 的開發流程了！\n結語 # 這篇文章我們有去介紹了 TDD（Test-Driven Development，測試驅動開發）的核心理念是什麼，並且也介紹了 TDD 的開發流程，了解要如何透過「撰寫單元測試」去驅動「程式的開發」。\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n補充：我開設的 Spring Boot 零基礎入門、Spring Security 零基礎入門、GitHub 免費架站術 已在 Hahow 平台上架啦！輸入折扣碼「HH202506KU」即可享 85 折優惠。\n","date":"2024-10-01","objectID":"c9d0981b02ac0e6e301d5523d31d1d43","title":"TDD 是什麼？認識 Test-Driven Development（測試驅動開發）","url":"https://kucw.io/blog/test-driven-development/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，正如之前在創刊號中有提到，雖然我一開始是意外踏入自媒體，但現在我是認真的要經營《古古的後端筆記》這個個人品牌，剛好現在也滿流行 Build in Public 的創業模式，所以就想來彙整一下到目前所經營的成果，寫個月報出來總結一下，也當作給自己的一個記錄。\n補充：還沒看過創刊號的人，也可以先查看一下 電子報創刊號，了解一下前情提要。\n2024.8～9 月粉絲追蹤數、電子報訂閱人數 # 自從電子報創刊號（2024/8/6）以來，到目前為止（2024/9/24）的粉絲成長人數如下：\n初始人數 2024/9/24 人數 總成長人數 Facebook 粉專追蹤數 2480 2687 +207 電子報訂閱人數 326 1195 +869 Threads 粉絲追蹤數 29 535 +506 IG 粉絲追蹤數 96 130 +34 雖然從這張圖上可以看到人數成長的很快，兩個月電子報就破 1000 人訂閱了，但我覺得這是因為我本身已經有經營 Facebook 粉專、有開過線上課程，所以目前已經有一定的粉絲數了，因此我是可以到 Facebook 社團宣傳、或是到課程中宣傳，將一部分粉絲數轉化為電子報訂閱者的。\n所以說到底，現在人數會漲得這麼快，完全就是吃老本😂，目前我已經把 Backend 社團的宣傳量能、自己線上課程的宣傳量能都用完了，下一步真的就是開始拼內容，只有製作出更有價值的內容，才能靠內容獲取大家的芳心💪。\n但有一點讓我滿意外的是，就是 Threads 粉絲的人數漲得也太快了吧！不知道大家有沒有玩過 Threads，我自己也是為了做自媒體才去下載 Threads 來用。\n據說 Threads 上的台灣用戶大概有 350 萬人左右，佔全球第 7 名，我自己是覺得根本原因是因為台灣人沒有被 Twitter（現 X 平台）佔據市場，所以在 「純文字的熱點討論」 這一塊，就被 Threads 給吃下來了。\n如果大家閒著沒事滑滑 Threads 的話，我是覺得可以比 Facebook 看到更多同溫層以外的討論，雖然也稱不上每一篇都很有收穫啦，但是就看看同溫層以外的世界也滿有趣的。\n然後有一點觀察也滿有趣的，就是 IG 的粉絲數真的是慘不忍睹XDD，一下子就被 Threads 超車到看不到車尾燈，我想這應該還是「內容呈現形式」所導致的差異。\n以 Facebook、Threads 和電子報來說，他們全部都是以 「文字」 的形式來傳播，但是 IG 則是以圖片、短影音來傳播，也因為後端技術這種比較硬的知識，大部分都還是以文字為主，所以我這個主題跟 IG 的粉絲受眾調性不同，因此在 IG 的粉絲數才一直起不來吧。\n後續應該會再觀察一段時間，如果 IG 流量真的不太好，現階段就直接放生吧XD，把時間拿去做更重要的事！\n本月學習內容：SEO 優化 # 既然是要認真經營自媒體，我覺得這對於我來說完全是一個新的領域，而我踏入新的領域的第一步，就是 「先認真看一堆書」，至少要把這個領域的基本知識搞懂、知道自媒體運作的原理，這樣子後續自己在實戰時，才不會像無頭蒼蠅一樣到處亂飛。\n所謂 「知識就是力量」，真的是非常好的一句話！\n這個月我看了《SEO白話文：贏得免費流量，創造長期營收的「SEO行銷指南」》這本書，作者是邱韜誠 Frank Chiu。\n會先挑 SEO 這個主題下手，是因為既然我的文章調性是以長文、部落格為主，那麼把 SEO 做好，可以說是一個 「長期有效」 的事。\n所謂的 SEO，全稱是 Search Engine Optimization（搜尋引擎優化），而 SEO 簡單的說的話，就是盡量讓你的網站在 Google 中排名變高，就是這麼簡單暴力。\n舉例來說，大家現在到 Google 上搜尋「Java lombok」的話，那就可以看到我所寫的「Java - 五分鐘學會 Lombok 用法」文章，出現在搜尋排名的第 2 名。\n而排名越靠前，就表示被使用者點擊的機率越高，因此我所寫的文章就越可以被大家看見、進而提升《古古的後端筆記》的影響力。\n所以在我不斷精進自己的後端技術能力的同時，我也得維護好 SEO，盡量讓自己寫的文章靠前，這樣子才能夠達到長期有效的成長。\n當然啦，SEO 效益這麼高，一定是所有人都搶著要把排名拉高🥹，而我現在也就只有一篇 Lombok 是排名比較靠前的，而且這也只是剛好運氣好賽到（當時根本不知道什麼是 SEO）。\n所以有關 SEO 的部分，後續我仍舊會投入練習經營，雖然目前也不知道成效怎樣，但是至少知道 SEO 的玩法規則之後，再上戰場跟其他網站廝殺，希望成功率可以高一點吧🤣。\n本月撰寫的電子報主題、文章 # 這邊記錄了我這兩個月內撰寫了哪些電子報和文章，大家如果對於其中的內容有興趣的話，也可以前往查看。\n《古古的後端筆記》創刊號 iThome 鐵人賽 - 得《優選》獎項的寫作心法 RESTful API 設計指南，3 個必備條件缺一不可！ 一文搞懂 Http Status Code，詳細解析 200、301、401、403、500、503 Spring Boot - 監控工具 Actuator AI 名詞和概念介紹 - NLP、LLM、RAG Vertical Scaling 和 Horizontal Scaling 介紹 Docker 是什麼？他和 VM 的差別在哪裡？ Nginx 是什麼？認識反向代理、負載平衡 如何準備面試？後端工程師的求職全攻略 2024.8～9 月報總結 # 綜合以上的數據的話，雖然到目前為止都只是吃老本，但是老實說看到這個粉絲數的成長我真的超開心的🥹😭，超級感謝大家的支持和訂閱，沒想到電子報訂閱可以破 1000 人，我真的是又驚又喜！\n後續除了努力撰寫後端知識分享的文章之外，也會多花一些時間學習自媒體、行銷這方面的知識，我覺得創業者就是得當個 「多面手」，什麼事情都必須要會，才能夠解決大大小小的各種問題。\n老實說，雖然在學習的過程中可能會有很辛苦的地方、也會有很累的地方，但是能夠學習其他領域的知識，其實我還滿興奮的😆，這可是坐在辦公室寫程式接觸不到的！所以後續希望也能多揭露一些自媒體的幕後祕辛給大家～\n另外，創業的路上不是自己一個人閉門造車就好，多聆聽用戶的反饋也是非常重要的！所以大家如果有什麼想法，也歡迎隨時寄信給我（service@kucw.io），每一封信我都會看的。\n這條創作之路仍在繼續前行中，期待下個月的月報！\n如果你對後端筆記有興趣，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，一起變強💪\n","date":"2024-09-24","objectID":"934bbacd668676b9f89c09d91d8ac9d1","title":"軟體工程師的自媒體之路 - 2024.8～9 月報","url":"https://kucw.io/blog/as-a-content-creator/monthly-report-202408-09/"},{"categories":["職涯相關"],"content":"在軟體工程師的世界中，大家可能常常聽到「漲薪水就是要靠跳槽」，但是，在跳槽之前，做好面試的準備是非常重要的！所以這篇文章就會來分析一下後端工程師在求職時，有哪些注意事項要準備。\n目錄 後端工程師的求職全攻略 履歷部分 履歷長度要濃縮成一頁 剛轉職後端，沒有過往的工作經驗該怎麼辦？ 履歷不要只描述產品本身，而是要描述「你做了什麼」 履歷不要有形容詞 海投履歷，渣男/渣女式求職法 先用中文寫履歷，再一口氣翻譯成英文 履歷小結 面試部分 面試前的心態準備 如果我是面試官… 1. 會做事 2. 能跟面試官溝通順暢 3. 認真負責任 面試小結 後端工程師的求職全攻略總結 結語 後端工程師的求職全攻略 # 在求職時，主要會分成兩個流程：「履歷審查」和「面試」，雖然這句話聽起來很像廢話沒😂，但是在求職時，一定要分清楚自己目前是處在哪個階段。\n如果你是海投了 100 家公司，但是連一封面試邀請信都沒有收到，那你就是「履歷」的部分有問題，需要回頭調整履歷呈現方式；相反的，如果你是滿手面試邀請信，但是沒有一家順利拿到 offer，那就是「面試」的部分有問題。\n所以大家在求職時，除了要好好準備面試大魔王之外，履歷的準備也是非常重要的！這兩項是相輔相成的，缺一不可！\n所以這篇文章也會分別從「履歷」和「面試」這兩個不同的角度，分別來介紹在這不同的階段該如何進行準備。\n補充：本篇文章主要是針對 Junior（初階）後端工程師所撰寫的求職攻略，Senior（資深）的部分會稍微提到一點，但是不會深入太多，所以大家如果是資深工程師、或是主管大大等級的話，這篇文章可能幫助不大🥹。\n履歷部分 # 履歷長度要濃縮成一頁 # 在撰寫履歷時，首先一定要把自己的豐功偉業濃縮成一頁，常見的履歷就真的是只有一頁而已（可能 HR 們也來不及全部看完，只會看一頁），所以想盡辦法把自己做過的事情濃縮成一頁是非常重要的。\n剛轉職後端，沒有過往的工作經驗該怎麼辦？ # 如果是剛轉職比較沒有過往工作經驗的話，那麼會建議可以放一些你自己實作的小專案，如果是轉職的話，通常是會跟著資策會、或是線上課程學習，所以就可以把那些學習過程中所實作的專案，當成是你的求職作品集這樣，比起什麼都沒放會好很多！\n至於後續有累積越來越多工作經驗之後，就可以把作品集的部分拿掉了，把履歷的空間留給你最新的工作經驗。\n履歷不要只描述產品本身，而是要描述「你做了什麼」 # 在撰寫履歷時，建議不要只描述產品、作品集本身，而是要描述「你做了什麼」。\n舉個例子來說的話：\n❌ 我使用 Spring Boot 實作了一個電商網站，裡面包含會員功能、訂單功能、商品功能。 ✅ 我使用 Spring Boot 實作了一個電商網站，裡面包含會員功能、訂單功能、商品功能，其中用到的技術有 Spring Boot、RESTful API、Spring MVC、Spring JDBC、單元測試。 在上面的例子中，上面的 ❌ 錯誤寫法，就只是在描述這個電商網站本身有什麼功能這樣，其實對於面試官來說，你實作的產品是什麼不重要，重點是你在實作的過程中，使用到了哪些技術，這個才是最重要的！\n所以在撰寫履歷時，建議可以改成下面的 ✅ 正確寫法，描述你在實作電商網站的過程中，使用到了哪些技術，像是 Spring Boot、RESTful API、Spring MVC…等等，這樣子面試官後續才可以透過這些關鍵字，延伸去考察你是否了解這些技術的運用。\n履歷不要有形容詞 # 在撰寫履歷時，還有一個很重要的重點，就是在履歷中 「不要出現形容詞」！！！\n舉例來說的話：\n❌ 我實作了一個具有高吞吐量、高擴展性的搶票功能，並且撰寫易於維護的程式，有效減少後期維護成本。 ✅ 我實作了一個能負擔 1000 人同時在線搶購的搶票功能，其中使用了限流器中的令牌桶算法（Token Bucket），確保伺服器不會因為過量的人流導致崩潰。並且我使用 JUnit 撰寫單元測試，單元測試覆蓋率為 80%。 透過上面這兩個例子，可以看到上面的 ❌ 錯誤寫法，用了非常多很厲害的形容詞，像是「高吞吐量」、「高擴展性」、「易於維護」…等，但老實說，這些形容詞對面試是一點幫助都沒有的，所以透過上面那句話所得到的資訊，就只有「你實作了一個搶票功能」而已。\n而至於下面的 ✅ 正確寫法，就提供了很多資訊，像是面試官就能知道你實作了一個能負擔 1000 人的搶票功能（對後端來說，能同時負擔 1000 人和 10 萬人是不同等級的系統架構），然後後面也呈現了你所用到的技術，像是限流器、令牌桶算法、JUnit 單元測試…等等。\n因此面試官之後在面試時，就可以根據你這份履歷中所提到的關鍵字，具體的問你「那你了解限流器是什麼嗎？請解釋一下你為什麼採用令牌桶算法」…等等，等於是埋了許多關鍵字給面試官問這樣。\n所以在撰寫履歷時，建議改成正確的寫法，就是 「在履歷中盡量不要出現形容詞」，反而是要好好的把你的實作給 「量化」 出來，這才是最重要的！\nPS: 這時候有的人可能會有疑問，啊我都離職了，要怎麼取得到當初我實作的系統數據？沒錯，你拿不到的（如果你拿得到，公司的資安就完了）。\n所以履歷其實是要在工作的過程中隨時更新的，建議是當你做完一個大案子的時候，就可以回頭拉一下數據、更新一下你的履歷，這樣子後續要求職的時候，才不會苦無找不到數據來佐證自己的情況出現🥹。\n海投履歷，渣男/渣女式求職法 # 抱歉這一個小節的標題有點聳動，但是這是我真實的想法😂。\n在投履歷時，除了那種「你上了也完全不想去的公司」不用投之外，其餘的公司，但凡你有一點點興趣，建議履歷就是直接給他大膽地投下去，等到你真的上了之後，再考慮要不要去。\n很多人在投履歷時，可能會陷入一個迷思，就是「我已經正在面試某幾家公司了，我還要繼續投履歷嗎？」，這個答案，一定是 Yes！ 在你真的拿到 offer 入袋之前，有幾家你就投幾家，每天照三餐投，反正就是有幾家就投幾家就對了！\n這樣子海投履歷的好處，就是 「你能掌握主動權」，舉例來說：\n假設你今天突然面試到一半被刷掉了，這時候就不需要特別擔心，因為你還有備胎 假設你今天同時得到 2 家公司以上的 offer，那太棒了，你們 2 家公司去打架吧，看誰給的錢高、待遇好、發展性好，我就去哪家 所以對於求職者來說，手上掌握越多 offer 通常是越好的，這代表你在市場上有競爭力，所以大家都搶著要你，所以你就越有底氣和 HR 談更好的薪資待遇，因此在求職時，如果時間允許的話（畢竟面試也是需要時間準備），還是會建議大家可以海投履歷，為自己掌握更多的主動權。\nPS：之所以說海投履歷是渣男/渣女式的求職法，是因為在還沒有正式交往（還沒有正式簽 offer）之前，你都還有許多備胎（其他公司）可以選擇；同樣的，公司也是除了你之外，還養著無數的備胎求職者，所以這就是一個比賽誰備胎比較多的世界，根本就是逼人當渣男/渣女😂。\n所以建議大家，在找工作時，就先放下你善良的感情觀，用渣男/渣女的方式來求職，讓自己成為許多公司搶著要的對象，才能成為真正的贏家！\n先用中文寫履歷，再一口氣翻譯成英文 # 這點我真的是恨不得早點發現🥹，在一剛開始求職時，我也常常遇到履歷寫不出來的情況出現，本來還怪說是不是自己英文太爛了才寫不出來，但是後來當我嘗試用中文寫寫看時，嗯.…我發現我還是寫不出來XDD。\n這就像是英文作文一樣，拿不到高分時，以為是自己英文不夠好、名言佳句背不夠多，才寫不到高分，後來才發現原來是自己中文不好，用中文寫的文章結構就很糟了，這樣就算翻譯成英文，仍然沒辦法變成一篇好作文，進而拿到高分的。\n所以就建議大家，一開始可以先用中文寫履歷，等到全部寫完之後，再叫 ChatGPT 幫我們翻譯成英文，最後","date":"2024-09-17","objectID":"8e2404cbfc70a6fe672eabf550114f95","title":"如何準備面試？後端工程師的求職全攻略","url":"https://kucw.io/blog/backend-developer-interview/"},{"categories":["其他技術分享"],"content":"Nginx 可以說是在大型的微服務架構中，必備的核心功能之一，所以這篇文章我們就來介紹一下 Nginx 到底是什麼，以及介紹一下反向代理、負載平衡的概念吧！\n目錄 什麼是 Nginx？ 在 Nginx 被發明出來之前：單體式架構 微服務架構所碰到的問題 Nginx 的反向代理 Nginx 介紹 反向代理（Reverse Proxy） 負載平衡（Load Balancing） Http Cache Nginx 總結 補充：什麼是正向代理（Forward Proxy）？ 結語 參考資料 什麼是 Nginx？ # Nginx 是一個輕量級的 Web 伺服器，通常用在以下三種情境：\n反向代理（Reverse Proxy） 負載平衡（Load Balancing） Http Cache 不過在我們開始了解 Nginx 的這三種特性之前，要先回頭來介紹一下 Nginx 最一開始要解決的問題是什麼，了解了 Nginx 要解決的問題之後，我們才能夠延伸去介紹「反向代理」、「負載平衡」、以及「Http Cache」的概念。\n補充：Nginx 的唸法是「Engine X」（中文發音近似於「安金 欸可斯」），也就是「引擎 X」的意思，這裡開頭的 N 字母就是使用到英文語法中常見的縮寫。像是 How are you 可以簡寫成 How r u、而 Airbnb 其實是 AirBed \u0026amp; Breakfast 的簡寫（n 的發音近似於 \u0026amp;），所以在這裡就是用到了英文的語法，將「Engine X」縮寫為「Nginx」。\n在 Nginx 被發明出來之前：單體式架構 # 在我們一開始學習後端程式時，我們就是寫好一個後端程式（ex: Spring Boot 程式），然後運行他，接著我們就可以用 Postman 這類的工具，去發起一個 API call，去請求我們電腦上所運行的後端程式，而這個就是最簡單的 單體式架構（Monolithic），也就是同時間我們只會有一個後端程式存在。\n不過當有越來越多使用者，慕名前來使用我們的後端服務時，這時候就會導致流量上升，使得一台 Server 沒辦法支撐住這麼高的流量。\n所以這時候，為了確保 Server 能負擔這麼高的流量，我們就有兩個選擇：\n使用 Vertical Scaling（垂直擴展），為這台 Server 升級 cpu、ram…等設備，讓他從一台普通的「2 核 cpu、4 GM ram」的電腦，變成是一台「64 核 cpu、128 GB ram」的超級電腦。 使用 Horitontal Sclaing（水平擴展），也就是多新增幾台 Server 出來，大家人多勢眾，一起對付更高的流量。 如果這時我們選擇 Vertical Scaling（垂直擴展）的話，就只是從一台普通的 Server，進化成一台超級電腦而已。這個選擇對整體的架構沒有影響，仍舊是只有一個後端程式在運行，所以此時依舊是單體式的架構，因此不需要 Nginx 上場。\n但如果我們選擇的是 Horitontal Sclaing（水平擴展）的話，那麼我們在同一時間，就會有非常多台的 Server 聚集在一起，準備聯合起所有 Server 的力量，一起對付高流量。所以這時候，整個後端的架構，就會從「單體式架構（Monolithic）」變成「微服務架構（Microservices）」，也就是同時間有多個後端程式在運作。\n所以這個時候，我們就會遇到微服務架構所要面對的問題，因此就需要 Nginx 上場了！\n補充：如果想更了解 Vertical Scaling（垂直擴展）和 Horizontal Scaling（水平擴展）的介紹，可以參考 Vertical Scaling 和 Horizontal Scaling 介紹 文章的介紹。不過這邊只要先知道 Horizontal Scaling 的概念是新增許多台 Server 出來，就可以繼續閱讀本文對 Nginx 的介紹了。\n微服務架構所碰到的問題 # 在微服務的架構中，因為同時間會有多個後端 Server 在運作，所以這時候對於前端而言，最最最大的問題，就是他不知道要去 call 哪一台 Server 的 API（對這個聽起來有點笨，但是前端就真的不知道啊！）。\n像是在下面這張圖中，同時間有 3 台後端 Server 在運作，這時候前端就會很困惑，他現在到底是要請求哪一台 Server？或是當流量又升高之後，這時候我們添加到 10 台 Server 來處理時，這時候要怎麼通知前端，現在有 10 台 Server 可以接受前端的請求？\n所以為了解決這個問題，Nginx 就被發明出來了！\nNginx 的反向代理 # Nginx 其中的一個核心功能，就是 「反向代理（Reverse Proxy）」，而所謂的反向代理，就是讓 Nginx 一個人擋在所有後端程式的最前面，由 Nginx 作為統一的守門人。\n所以當前端想要來請求後端的 API 時，前端就會改成請求 Nginx，由 Nginx 決定這一次的請求要分配到哪一台後端 Server 上，而這個行為，就是反向代理（Reverse Proxy）了！\n所以有了 Nginx 之後，Nginx 就可以作為所有後端 Server 的守門人，因此今天當前端來請求後端的 API 時，前端就必須改成去 call Nginx，由 Nginx 將這個請求「轉發」給任一台後端 Server（轉發規則由 Nginx 說了算），達到反向代理的效果。\n所以透過 Nginx 的反向代理，我們就可以統一的處理所有前端傳遞過來的請求了！\nNginx 介紹 # 透過上面的例子大概了解 Nginx 的概念之後，我們也可以回頭正式來介紹一下 Nginx 的用途。\nNginx 是一個輕量級的 Web 伺服器，通常用在以下三種情境：\n反向代理（Reverse Proxy） 負載平衡（Load Balancing） Http Cache 而透過上述的例子，現在我們也了解了反向代理的概念是什麼：\n反向代理（Reverse Proxy） # 所謂的「反向代理」，其實就只是 Nginx 一個人擋在所有後端 Server 的最前面，由 Nginx 作為後端 Server 的守門人，因此所有前端傳遞過來的請求，都一定都要先經過 Nginx，再經由 Nginx 後續去轉發給內部的後端 Server。\n而使用反向代理的好處非常多，因為在反向代理中，是由 Nginx 一個人扛住所有對外的連接，所以我們只需要在 Nginx 中設定好 Https 的憑證即可，因此內部的後端 Server 就不需要處理網路相關的憑證問題。\n又像是防火牆的問題，我們只要統一的在 Nginx 處理好 DDoS 之類的駭客攻擊就好，就不需要每一個後端 Server 都去處理，也大大的節省了資訊安全的維護成本。\n如果大家有用過 AWS、GCP…這類雲端服務的話，其實 AWS 中的 API Gateway，就會擋在 Amazon EC2 前面（EC2 就是 AWS 中提供 VM 的服務），由 API Gateway 統一處理所有對外的溝通。所以如果你的 Server 是部署在 AWS 上的話，那麼可以直接選擇 AWS 的 API Gateway 來負責反向代理的部分，因此就不用自己架一台 Nginx 來維護了。\n圖片來源： Amazon API Gateway 負載平衡（Load Balancing） # 只要講到反向代理，那麼就一定會提到 負載平衡（Load Balancing），他們是必定會一起出現的一對名詞。\n在前面我們有提到，Nginx 會作為所有後端 Server 的守門人，透過反向代理的機","date":"2024-09-10","objectID":"593611e9a7391a7aae30598c88a8b605","title":"Nginx 是什麼？認識反向代理、負載平衡","url":"https://kucw.io/blog/nginx/"},{"categories":["其他技術分享"],"content":"作為軟體工程師，大家可能多多少少聽過什麼是 Docker，但是卻不了解 Docker 實際要解決的問題是什麼、以及他和 VM 之間的區別。\n所以這篇文章就會來介紹一下，到底什麼是 Docker、什麼又是 VM，並且他們兩個之間的差別又是什麼，那我們就開始吧！\n目錄 什麼是 VM？ 使用 VM 的注意事項 補充：常見的 VM 例子 什麼是 Docker？ 有了 VM 之後，我們還需要 Docker 嗎？ VM 和 Docker 總結 結語 什麼是 VM？ # 在了解 Docker 之前，一定要先了解什麼是 VM 才可以，所以我們先來介紹一下 VM 是什麼。\nVM 的全稱是 Virtual Machine，中文翻譯為「虛擬機」，而 VM 要解決的問題，就是要達到「硬體資源的共享」。\n在很久以前 VM 還沒被發明出來的時代，這時候如果我們想要在一台電腦上運行程式，那我們首先要做的，就是要先帶著你的錢錢，先到 3C 商店買一台實體的電腦回家之後，你才能夠在那台電腦上面運行你所寫的程式。\n所以在 VM 被發明出來以前的時代，我們所有的程式，就都是直接運行在實體的主機上。\n不過隨著網路的發展，大家就漸漸發現，每次在運行程式之前，都還得要提前去買一台實體的電腦，這真的是太麻煩了！所以大家就開始思考：「我們是否能夠讓一台實體的電腦，分身成無數台電腦？」，這樣子我們就再也不用提前去購買實體的主機了，只要按一下按鍵就可以生成一部新的電腦出來。\n而正是這個想法，就造就了 VM（虛擬機）的出現了！\n所謂的 VM，就是將「一台實體的主機」，拆分成「許多個作業系統」的技術。 所以舉例來說的話，假設我們有一台超強的實體主機（64 核 cpu、128 GB ram），那麼我們就可以透過 VM 的技術，將這台實體主機「拆分」成是三台電腦，而每一台電腦，我們就可以自由的選擇我們想要灌哪個作業系統。\n所以像是在下圖中，在 VM 虛擬機 1 這台電腦上，我們可以灌 Linux 這個作業系統；而在 VM 虛擬機 2 這台電腦上，我們可以灌 Windows 這個作業系統…等等，所以當我們使用了 VM 之後，我們就只要購買一台實體的主機，就可以拆分成許多台小電腦出來，因此就達到了 「硬體資源的共享」 了！\n使用 VM 的注意事項 # 在使用 VM 時，因為 VM 本質上是去共享一台實體主機，所以所有 VM 的運算資源，都是從實體主機上面瓜分出來的。\n所以像是在上面的例子中，假設實體主機的運算能力是 64 核 cpu、128 GB ram，那麼 VM 1 可能只會分到 2 核 cpu、4G ram，VM 2 可能會分到 4 核 cpu、8G ram…等等，每一台 VM 都可以依照自身的需求，去和實體的主機請求適量的運算資源。\n也因為 VM 的運算資源都是從實體主機上面瓜分出來的，所以一台實體主機能創建的 VM 也是有限的，沒辦法無限的創建下去，所以萬一 VM 的需求越來越多的話，仍舊是會需要我們提前去購買新的實體主機回來的。\n補充：常見的 VM 例子 # 其實大家平常在用的雲端服務，就是 VM 最常見的用法。像是我們可以用滑鼠點一點，就能夠在 AWS 中去租用一台 EC2 的 VM 出來。\n像是在下圖中，AWS 預設就是租用 t2.micro 的 VM 給我們，所以這一台 VM 就是擁有 1 核 cpu、並且有 1G 的 ram，而運行這台 VM 的實體主機，就是 AWS 提前採購好的實體主機。\nOK 所以到這邊為止，我們已經了解到什麼是 VM 了，所謂的 VM，就是要達到「硬體資源的共享」。\n那麼在了解了 VM 的概念之後，接著我們可以來了解一下，什麼是 Docker？他和 VM 又有什麼差別？\n什麼是 Docker？ # 如果說 VM 的目的，是為了要達到「硬體資源的共享」，那麼 Docker 的目的，則是要達到「應用程式的隔離」。\n大家可以想像一個情境，假設你今天上網看到了一段爬蟲程式，然後你想把他載下來使用，但是這段程式要求要使用 Python 2.x 的環境才能夠運行，所以你為了運行這段程式，你就努力的在你的電腦上安裝好 Python 2.x 的環境，最後就成功的運行了這段程式。\n但是，當你好不容易處理完環境的問題之後，假設你又要執行另一段網頁程式，他所需要的是 Python 3.x 的版本，這時候你又得大費周章，努力的去研究要怎麼將 Python 2.x 更新到 Python 3.x、以及怎麼樣同時在你的電腦上安裝 Python 2.x 和 Python 3.x 的開發環境。\n這時候，你可能心已經累了，你明明只是想去運行網路上的那段程式而已，但是你卻得花非常多的心力，去處理架設開發環境的設定。\n所以為了解決這個問題，Docker 就被發明出來了！\n所謂的 Docker，他的目的是要達到「應用程式的隔離」，所以在你的電腦中，就會有很多個「容器」存在，每一個容器都是一個箱子，裡面會放置你想運行的程式、以及該程式需要的開發環境，並且每一個箱子之間不會互相干擾。\n所以像是在下圖中，在 VM 1 這個 Linux 的作業系統裡面，我們就可以創建許多個 Docker 容器出來，而每一個 Docker 容器，我們都可以安裝不同的開發環境、並且運行不同的程式，這樣子就可以達到開發環境之間的隔離，進而節省我們反覆架設開發環境的時間。\n所以舉例來說的話，我們就可以將「爬蟲程式 + Python 2.x 的開發環境」，放置在 Docker 容器 1 裡面，而我們也可以將「網頁程式 + Python 3.x 的開發環境」，放置在 Docker 容器 2 裡面。因此當我們之後想要執行爬蟲程式的時候，就直接去運行 Docker 容器 1 就好，而如果我們想要運行的是網頁程式，那就是改成運行 Docker 容器 2。\n所以透過 Docker 容器的概念，我們就可以達到「應用程式之間的隔離」了！\n有了 VM 之後，我們還需要 Docker 嗎？ # 在了解了 VM 和 Docker 的概念之後，接著我們也可以來討論一下一個很常見的問題，那就是：有了 VM 之後，我們還需要 Docker 嗎？\n在看完了上面的介紹之後，大家心裡可能會有一個疑惑，就是上述的 Python 環境的問題，我們創建兩個 VM 出來，不就也可以解決了？譬如說我們可以創建一個 VM 1，裡面安裝 Linux 的作業系統，並且安裝 Python 2.x 的環境，然後我們也可以再創建 VM 2，裡面也是安裝 Linux 的作業系統，但是改成安裝 Python 3.x 的環境，這樣不是也能解決上面的開發環境的問題嗎？\n確實透過創建兩個 VM 的方法，是可以解決 Python 的環境問題沒錯，不過相較來說，Docker 的解法會更加輕量化。\n因為對於傳統的 VM 來說，每一個 VM 都需要安裝一個作業系統（ex: Linux），然後才能在這個作業系統上面安裝 Python 的開發環境。但是在作業系統中，其實是包含非常多的功能的（ex: 操作介面、驅動程式、網路管理…等等），所以每當我們創建一個 VM 出來，就必須要一起創建這些我們根本用不到功能出來，所以我們就會浪費不少的運算資源，在維護這些不必要的功能上。\n而 Docker 之所以可以更加輕量化，是因為他是在同一個作業系統底下，去創建許多的容器出來，然後再在每一個容器中，各自去安裝需要的開發環境。所以這些容器，他們就會共享同一個作業系統的所有功能，所以我們就不需要重複去安裝驅動程式這類的功能，因此就可以節省許多的運算資源，降低 cpu 的負擔了。\n所以 Docker 之所以這麼好用，就是因為他除了能夠達到「應用程式的隔離」之外，同時他","date":"2024-09-03","objectID":"05ae48f286c2740f347cd181b9dd8c16","title":"Docker 是什麼？他和 VM 的差別在哪裡？","url":"https://kucw.io/blog/docker-vs-vm/"},{"categories":["其他技術分享"],"content":"你有沒有曾經好奇過，在雲端的世界裡 Server 是如何進行擴展的呢？\n這篇文章我們就來介紹一下雲端服務中兩種常見的擴展方式：Vertical Scaling 和 Horizontal Scaling 吧！\n目錄 什麼是 Vertical Scaling 和 Horizontal Scaling？ 什麼是 Vertical Scaling（垂直擴展）？ 什麼是 Horizontal Scaling（水平擴展）？ 小結 Vertical Scaling（垂直擴展）的優缺點、以及適用的情境 Vertical Scaling 的優缺點 Vertical Scaling 的適用情境 Horizontal Scaling（水平擴展）的優缺點、以及適用的情境 Horizontal Scaling 的優缺點 Horizontal Scaling 的適用情境 Vertical Scaling 和 Horizontal Scaling 總結 結語 什麼是 Vertical Scaling 和 Horizontal Scaling？ # 想像一下，假設現在你正在經營一間新創公司，一開始的流量很小，所以就只要到 AWS 或是 GCP 上面，隨便租用一台 VM（虛擬機）當作你的 Server，就可以直接應付這些使用者的流量了。\n但是，當你的業務慢慢擴張時，這時候當初租用的那台 Server 已經不夠用了，那麼這時候，你該怎麼辦？所以為了讓 Server 能夠應付更高的流量，這時候就是 Vertical Scaling 和 Horizontal Scaling 出場的時候了！\n什麼是 Vertical Scaling（垂直擴展）？ # 所謂的 Vertical Scaling（垂直擴展），就是表示「直接在該 VM 上加一堆 cpu 和 ram，讓這台 VM 變成一台超級強大的 VM」，讓他能夠對付更高的流量。\n所以在 Vertical Scaling 中，你就只會有一台 VM，只是你把這台 VM 從一台小型的「2 核 cpu、4 GM ram」Server，變成是一台「64 核 cpu、128 GB ram」的超級電腦而已。\n另外英文口語中的「Scale Up」，就是表示 Vertical Scaling（垂直擴展）的意思。\n什麼是 Horizontal Scaling（水平擴展）？ # 而所謂的 Horizontal Scaling（水平擴展），則是表示「多新增幾台小型的 VM 出來，大家人多勢眾，一起對付更高的流量」。\n所以在 Horizontal Scaling 中，你就會從「只有一台小型的 VM」，變成是「擁有很多台小型的 VM」，所以你就可以聯合這些小型的 VM 們，一起去對付更高的流量。\n注意在 Horizontal Scaling 中，你所擁有的一定是「很多台小型的 VM」，而不是「很多台超級電腦」，因為 Horizontal Scaling 的定義就是水平擴展，即是聯合許多小型的 VM，一起去克服龐大的流量。所以假設今天其中一台 VM 倒下了、或是流量又撐不住了，那麼就趕快再招一個小型的 VM 來加入戰場，而不是為戰場中的 VM 們升級裝備（添加 cpu、ram），這個是在了解 Horizontal Scaling 的定義時，一定要特別注意的一點。\n另外在英文口語中的「Scale Out」，則是表示 Horizontal Scaling（水平擴展）的意思。\n小結 # 所以到這邊先小結一下的話，Vertical Scaling 是「為一個 VM 不斷增強他的添加 cpu、ram，將他變成一台超級電腦，以應付更高的流量」，而 Horizontal Scaling 則是「添加許多小型的 VM，一起聯合起來對付更高的流量」，而他們兩種方式所要解決的問題，就都是 克服更高流量的問題。\n而在了解了 Vertical Scaling 和 Horizontal Scaling 的概念之後，接下來我們就可以來比較一下這兩種方式的優缺點，以及在什麼樣的情境下，更適合選擇哪一種 Scaling 機制了。\nVertical Scaling（垂直擴展）的優缺點、以及適用的情境 # Vertical Scaling 的優缺點 # 因為 Vertical Scaling 的方式，就是不斷為一台 VM 中添加 cpu、ram，將他變成一台超級電腦，來應對更高的流量，所以在 Vertical Scaling 的世界中，就只會有一台 VM 來處理請求，因此他的優點，就是維護和升級非常容易，所以我們只要管好這一台 VM 就好，不需要處理多台 VM 所帶來的分散式系統（Distributed System）的問題。\n不過 Vertical Scaling 的缺點，就是他的擴展是有限的，因為就算不斷的為一台 VM 添加 cpu、ram，在物理上總是有極限的，不可能真的無限擴展下去，所以 Vertical Scaling 的缺點，就是他的擴展是有限的，到一定程度之後一定得改成 Horizontal Scaling 擴展方式。\nVertical Scaling 的適用情境 # 其實在業務擴張的初期，Vertical Scaling 是非常好用的選擇，因為我們只要無腦的為那一台 VM 不斷的添加 cpu 和 ram 就好（錢給到位，什麼都給你加），不需要額外處理多台 VM 所帶來的分散式系統的問題。所以在業務擴張的初期，只要啟動資金足夠，Vertical Scaling 會是一個滿不錯的選擇。\n不過當業務漸漸擴展時，這時候用 Vertical Scaling 的效益就不太好、而且可能也無法應付大量的使用者流量，所以這時候就會需要一次大型的重建，將程式改為分散式系統，後續改為使用 Horizontal Scaling 來擴展。\n所以如果是去很早期的新創、或是自己做 Side Project 的話，就可以考慮先用 Vertical Scaling 來撐住，至少讓公司先活下去這樣，但是後續如果想要長期發展、或是想要進大公司的話，那就一定要了解 Horizontal Scaling 和分散式系統的概念，因為大公司的流量通常都不是一台 VM 可以抵擋得住的。\nHorizontal Scaling（水平擴展）的優缺點、以及適用的情境 # Horizontal Scaling 的優缺點 # 因為 Horizontal Scaling 的方式，是聯合一群小型的 VM 們，一起應對更高的流量，所以 Horizontal Scaling 的優點，就是擴展沒有極限，只要流量變大，那我就多添加一台小型的 VM 到群組裡，增加更多的戰力來應對更高的流量。\n不過 Horizontal Scaling 的缺點，就是他在維護上會比較麻煩，因為有了多台 VM 的概念，所以就必須要建立一個分散式系統（Distributed System），並且處理分散式系統所會帶來的相關問題。\n所以 Horizontal Scaling，他就是屬於前期需要投入許多的成本，去架設分散式系統、並且處理分散式系統所帶來的問題，但是一但建立起來之後，後期可以無限制擴張的一種擴展方式（用遊戲的術語來描述的話，他就是一個前期發育緩慢、但是大後期很強的角色）。\nHorizontal Scaling 的適用情境 # 因為要使用 Horizontal Scaling 的話，首先就需要建立分散式系統，所以確實會對開發帶來一定的成本，但是如果只要後續業務有成長的話，後期通常都會轉成 Horizontal Scaling。\n所以如果在開發時有餘裕的話，建議一開始在設計系統時，最好就直接朝「分散式系統」來設計，這","date":"2024-08-27","objectID":"6ec464750dc6d6c024767316bb8bf655","title":"Vertical Scaling 和 Horizontal Scaling 介紹","url":"https://kucw.io/blog/vertical-and-horizontal-scaling/"},{"categories":["其他技術分享"],"content":"最近實在是看到太多 AI 的專有名詞了（ex: NLP、LLM、RAG），雖然我不搞 AI，但是稍微理解一下這些名詞在幹嘛還是有必要的，不然搞得好像全世界都在追一場很好看的劇，但是自己卻完全跟不上🥹。\n所以這篇文章就會從頭來介紹一下 AI 的概念和邏輯（不需要任何程式背景也能看得懂），以及介紹常見的 AI 名詞，那麼我們就開始吧！\n目錄 什麼是 NLP（自然語言處理）？ 什麼是 LLM（大型語言模型）？ 什麼是 RAG（檢索增強生成）？ 結語 什麼是 NLP（自然語言處理）？ # 在談 LLM、RAG、GPT 這類的 AI 名詞之前，首先最最最重要的，是要先搞懂他們是要解決什麼問題。\n這一系列 AI 的名詞，他們最根本的，是要解決「自然語言處理」的問題，而這個白話來說的話，就是要讓這些 AI 們「能夠說人話」（我們平常說的語言就是自然語言）。\n像是在 AI 出現之前的時代，程式的運作是非常直覺的，工程師會寫一堆 if...else... 的判斷句，就是「當什麼事情發生時\u0026hellip;.程式就做什麼反應」。\n所以在以前的年代，工程師就會寫出 if 有人說 \u0026quot;你好\u0026quot;，程式就輸出 \u0026quot;Hello World\u0026quot; 這種程式，所以假設到時候，真的有人來說了 \u0026ldquo;你好\u0026rdquo;，那麼程式就會輸出 \u0026ldquo;Hello World\u0026rdquo; 這個字串，就是這麼簡單直覺暴力（所以這也是為什麼以前的客服機器人很笨，因為他就是只看得懂特定的句子，然後回覆固定的罐頭回答）。\n但是自從 ChatGPT 出現之後，世界就開始變得不一樣了！\n所謂的 ChatGPT，他是屬於一種生成式 AI，而他最大的不同點，就是「他會自己決定他要回覆什麼答案」。\n所以當我們使用了 ChatGPT 之後，工程師就再也不用寫出 if 有人說 \u0026quot;你好\u0026quot;，程式就輸出 \u0026quot;Hello World\u0026quot; 這種死板的程式了，工程師要做的，反而是告訴 ChatGPT 各式各樣的情境，讓 ChatGPT 自己去旁觀學習這樣。\n所以譬如說工程師可能就會輸入下面這兩段對話給 ChatGPT：\nA：你好 B：哈囉你好，你吃飽了沒？ A：還沒，正要去吃，你要一起嗎？ B：好啊！走吧 C：你好 D：心情不好，滾 C：別這樣嘛，走請你吃飯 D：你請客，謝謝 而當 ChatGPT 在「旁觀」完這兩段對話之後，他心中可能就會對於 \u0026ldquo;你好\u0026rdquo; 這兩個字，有自己的理解（這就像是人類在學習一樣，小時候都是透過旁觀父母的行為，決定自己的成長人格）。\n所以下一次當你對 ChatGPT 說 \u0026ldquo;你好\u0026rdquo; 的時候，他的反應可能會是：\n你：你好 ChatGPT：滾 又或是\n你：你好 ChatGPT：哈囉滾 所以對於 ChatGPT 會回答什麼，你是完全不會預先知道的，甚至就連養出他的工程師也不知道他會回什麼😂，因為實際上工程師所做的，就只是輸入一大堆片段的對話範例，讓 ChatGPT 自己去感悟理解而已。\n也因為對於每一個 ChatGPT 的模型而言，他們都是獨一無二的孩子，所以每一個孩子都會針對「同樣的對話範例」，會有「不一樣的理解」，而這個不斷拿範例對話去給 ChatGPT 旁觀學習的過程，就是俗稱的「訓練模型」。\n所以實際上 ChatGPT 能夠長成什麼樣子，初始的範例對話是影響非常大的（就跟父母的言行對於孩子的影響很大一樣），所以這也是為什麼 ChatGPT 剛上市的時候常常被吐嘈中文很奇怪，估計就是因為初始的範例對話數據很少中文，啊就父母不會講中文，孩子不會講中文也很合理吧🤣🤣🤣。 󠀠 所以到這邊先小結一下的話，就是 ChatGPT 所要解決的問題，就是要解決「自然語言處理（NLP）」的問題，而這個白話來說的話，就是要讓 AI「能夠說人話」。\n至於 ChatGPT 的解決方法，就是透過不斷的輸入初始的範例數據，去「訓練 ChatGPT 的模型」，進而養出一個獨一無二的孩子（ex: GPT-3、GPT-4、GPT-4o\u0026hellip;等），然後再來比較看看誰養的好這樣（天下父母心，我家的孩子就是要比別人家的好！）。\n什麼是 LLM（大型語言模型）？ # OK 在了解了 ChatGPT 要解決的是「自然語言處理」的問題之後，接下來要理解 LLM 和 RAG 就比較容易了。\n其實在「自然語言處理」這個領域中，有一個分類，叫做 LLM（Large Language Models），也就是「大型語言模型」，只要是透過前面所提到的「不斷輸入初始化的範例數據，去訓練一個語言模型出來」的流程，那這個被訓練出來的模型，就叫做「大型語言模型（LLM）」。\n所以其實 ChatGPT 在定義上，就是屬於 LLM 分類中的一種實作，只不過因為 ChatGPT 實在太紅了，所以他的傳播度才大於 LLM 這樣，但是在定義上，LLM 是比較廣泛的定義，但凡是被訓練出來的語言模型，就都可以是屬於 LLM 的一種。\n所以像是目前市面上常見的 Gemini、GPT-4、Claude\u0026hellip;等等，這些都是一個一個的大型語言模型（只是父母不同人而已，像是 Google、OpenAI\u0026hellip;等），所以這些語言模型，就都是屬於 LLM 的一種實作。\n所以簡單的說的話，LLM 就是「大型語言模型」，而市面上常見的 AI 產品，就都是屬於 LLM 中的一種實作，目的都是為了解決「自然語言」的問題（就是要讓 AI 說人話）。\n什麼是 RAG（檢索增強生成）？ # 而至於 RAG，就是比較新的概念了。所謂的 RAG，是一種結合 「大型語言模型」+「外部數據來源」 的技術。\n舉個例子來說的話，假設 Google、OpenAI 他們透過很大量的初始範例對話，生成了一個一個的語言模型 Gemini、GPT-4\u0026hellip;出來，而當你真的訂閱了這個模型來用的時候，你就會發現，怎麼 AI 所回答出來的內容，好像都是幾個月前的內容，就沒辦法回答最近兩天的內容這樣。\n譬如說 GPT-4 只能告訴你：「籃球比賽的規則是什麼」，但是他卻沒辦法告訴你：「昨天獲勝的 NBA 球隊是哪一隻」，因為在 GPT-4 的初始訓練數據裡面，並沒有昨天獲勝的 NBA 球隊，所以他就不懂，所以他就沒辦法告訴你到底是誰獲勝。\n而如果要解決這個問題的話，最直觀的想法，可能是「啊那就把昨天獲勝的 NBA 球隊也拿去訓練一下 GPT-4 不就好了？」，老實說拿著新數據重新去訓練 GPT-4 這條路是可行的，但是成本代價太高，所以基於成本考量，一般不太會這樣做。\n所以為了要解決這個問題，RAG 就出現了！ 󠀠 所謂的 RAG，可以把他想像成是一個補丁包，就是你訂閱了一個 GPT-4 來用之後，你可以再跟他說：「這裡有昨天的 NBA 比賽數據，啊你不用訓練自己沒關係，你把他當小抄，如果我來問你 NBA 的比賽情況，你就順便看一眼這個小抄，然後再回答我就好」。\n所以當我們在 GPT-4 這個模型之上，再去添加了 RAG 之後，就可以讓這個 GPT-4 在「不需要重新訓練模型」的情況下，也能夠「理解最新的客製化數據」，這樣子不僅可以省下許多成本，也可以達到取得到最新數據的目的了。\n也因為 RAG 他是一種結合「大型語言模型」+「外部數據來源」的技術，讓 GPT-4 就像是得到了一堆小抄一樣，可以知曉外部數據中的內容。所以目前 RAG 比較常見的用法，就是透過 RAG 的技術，將 GPT-4 和企業內部的數據結合在一起，讓 GPT-4 變身成為一個知曉企業內部數據的強大語言模型！\n有關 RAG 和語言模型的介紹，也","date":"2024-08-24","objectID":"82a229f7509edb56a080bfe52ee44686","title":"AI 名詞和概念介紹 - NLP、LLM、RAG","url":"https://kucw.io/blog/ai-intro/"},{"categories":["Spring Boot"],"content":" 本文使用的 Spring Boot 版本：3.3.2\nActuator 是 Spring Boot 所提供的監控功能，可以用來查看當前的 Spring Boot 程式運行的情況，像是可以查看當前運行的健康指標、查看 Spring Boot 所創建的 beans、以及獲取當前的 applicaiton.properties 的屬性的值。\n目錄 使用 SpringBoot Actuator Actuator 提供的常見的 api 開啟受保護的 Actuator 監控 api 的方法 結語 使用 SpringBoot Actuator # 如果要使用 SpringBoot Actuator 提供的監控功能，需要先加入相關的 maven dependency。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 只要加上了這個 maven dependency，Spring Boot 在運行時，就會自動開啟 /actuator/health 這個 api 給我們使用，因此當我們去請求 http://localhost:8080/actuator/health 這個 api 時，就可以查看當前 Spring Boot 程式的運行狀況。\n像是下圖中就顯示，當前的 Spring Boot 運行狀態為 UP，UP 即為正常運行的意思。\nActuator 提供的常見的 api # 除了上述自動開啟的 /actuator/health api 之外，Actuator 其實還提供更多樣化的 api，讓我們可以從不同的角度，去監控 Spring Boot 程式（不過因為安全因素考量，大部分的 api 都需要另外設定才能夠開啟，詳細的設定方式在下方介紹）。\n因為監控 api 眾多，以下僅列出常用的監控 api，所有監控 api 可查閱 Spring 官方文件\nHTTP 方法 Endpoint 描述 GET /actuator 查看有哪些監控的 api 有開放使用 GET /actuator/env 查看此 Spring Boot 程式載入了哪些 application.properties 的值（不過為了保護安全資訊，所以會自動碼掉帶有 key、password、secret 等關鍵字的 properties 的值） GET /actuator/flyway 查看 flyway DB 的 migration 資訊 GET /actuator/health 查看當前 Spring Boot 程式的運行健康指標，UP 即為正常運行 GET /actuator/heapdump 取得 JVM 當下的 heap dump，會下載一個檔案 GET /actuator/metrics 查看有哪些指標的數據可以看（ex: jvm.memory.max、system.cpu.usage），可再使用 /actuator/metrics/{metric.name} 分別查看各個指標的詳細資訊 GET /actuator/scheduledtasks 查看定時任務的資訊 POST /actuator/shutdown 唯一一個需要 POST 請求的 api，關閉這個 Spring Boot 程式 開啟受保護的 Actuator 監控 api 的方法 # 因為安全的因素，所以 Actuator 預設只會開啟 /actuator/health 這個監控的 api 讓我們使用，如果要開放其他的 api 的話，需要額外在 application.properties 中進行設定\n# 可以這樣寫，就會開啟所有的 api（但不包含 shutdown） management.endpoints.web.exposure.include=* # 也可以這樣寫，就只會開啟指定的 api，像是此處就只會再額外開啟 /actuator/beans 和 /actuator/mappings 這兩個 api management.endpoints.web.exposure.include=beans,mappings # exclude 可以用來關閉某些 api # exclude 通常會和 include 一起搭配使用，就是先去 include 全部進來，然後再 exclude 想要關閉的部分 # 像是此處就是關閉 /actuator/info 這個 api management.endpoints.web.exposure.include=* management.endpoints.web.exposure.exclude=info # 如果要開啟 /actuator/shutdown api 的話，需要額外再加這一行 management.endpoint.shutdown.enabled=true 除此之外，也可以改變 /actuator 的路徑，自定義成自己想要的 url 路徑\n# 這樣寫的話，原本內建的 /actuator/xxx 的 url 路徑，就都會變成 /my/xxx # 這樣做可以防止 Actuator 的監控 api 路徑被其他人猜到 management.endpoints.web.base-path=/my 結語 # 本篇文章介紹了 Spring Boot Actuator 的用法，以及列出的常見好用的監控 api 有哪些，提供給大家參考。\n如果你對後端技術有興趣的話，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，那我們就下一篇文章見啦！\n如果想了解更多 Spring Boot 的用法，也歡迎參考我開設的線上課程 Java 工程師必備！Spring Boot 零基礎入門 （輸入折扣碼「HH202506KU」即可享 85 折優惠）。\n","date":"2024-08-21","objectID":"d5a6783b07fef638dd2933a6e3f40b6b","title":"Spring Boot - 監控工具 Actuator","url":"https://kucw.io/blog/2020/7/spring-actuator/"},{"categories":["其他技術分享"],"content":"大家在 call API 時，是否曾經好奇過他所返回的 200、401、403、500\u0026hellip;等數字是代表什麼意思？其實這些 3 位數的數字，就是 Http Status Code（又稱 Http 狀態碼）。\n因此這篇文章我們就來詳細介紹一下，Http Status Code 的用途到底是什麼，以及常見的 Http Status Code 有哪些吧！\n目錄 什麼是 Http Status Code（Http 狀態碼）？ 常見的 Http Status Code 1xx：資訊 2xx：成功 3xx : 重新導向 4xx : 前端請求錯誤 5xx : 後端處理有問題 Http Status Code 總結 結語 什麼是 Http Status Code（Http 狀態碼）？ # 所謂的「Http Status Code」，中文是翻譯成「Http 狀態碼」，而他的目的，就是 「用一個簡短的值，快速的表示當前 API 的請求結果是什麼」。所以舉例來說的話，大家常見的「200」的回覆，其實就是表示這一次的 API 請求成功了，就只是這麼簡單的用途而已。\n而在 Http Status Code 的世界裡面，除了有「200」這個值可以使用之外，也是有其他的返回值可以使用的，而這些 Http Status Code 的返回值，我們是可以根據他們的 首位數字，去分成是五個大類：\n1xx : 資訊 2xx：成功 3xx：重新導向 4xx：前端請求錯誤 5xx：後端處理有問題 所以舉例來說的話，但凡是 2 開頭的 Http Status Code，不管你是 200、201、還是 202…等等，只要你是 2 開頭，那就是屬於「2xx」那一個大類，就都是表示「成功」的意思。\n又或是說 400、401、403…等等的這些值，因為他們都是 4 開頭，所以他們就都是屬於「4xx」那一個大類，所以就都是表示「前端請求錯誤」的意思。\n所以透過這個 Http Status Code，我們就可以快速的知道，這一次 API 的請求結果為何了！\n常見的 Http Status Code # 在我們了解了 Http Status Code 的用途之後，接下來我們就可以來看一下，常見的 Http Status Code 有哪些（以下分別介紹各大類常使用的值）。\n1xx：資訊 # 很神奇的是，明明 Http Status Code 有定義 1xx 這個分類出來，但是在實際的應用中，基本上很少很少會用到 1xx 的返回值，因此在這個大類裡面，沒有常見的 Http Status Code。\n2xx：成功 # 在 2xx 開頭的大類中，所有的 Http Status Code，就都是表示 「請求成功」 的意思。而在 2xx 的大類裡面，常見的 Http Status Code，就有 200、201、以及 202。\n200 OK # 首先是「200 OK」，200 這個 Http Status Code，這個估計是大家最最最熟悉的值XD，就是表示這一次的 API 請求成功了的意思。\n不過大家觀察一下的話，就可以發現，在 200 的後面，多了一個「OK」的英文單詞，而這個「OK」的英文單詞，其實就是每一個 Http Status Code 的專屬短語，就是簡單描述一下當前這個 Http Status Code 的用途是什麼。\n所以像是「200 OK」裡面的這個「OK」，就是表示 200 這個 Http Status Code，他的意義是代表請求成功的意思。\n不過老實說，這個短語其實幫助不大😂（我個人感覺啦），每次遇到不熟悉的 Http Status Code 時，還是得要乖乖的上網查詢他的用途，而那些常見的 Http Status Code（ex: 401、403、500），又已經熟悉到像呼吸一樣自然，不需要看短語就也可以直接回想起他的意思，所以這個短語的用途，大家就作為參考就好～\n201 Created # 在 2xx 的大類中，另一個常見的 Http Status Code 則是「201 Created」，201 所代表的，不僅僅是請求成功而已，他還有另一個含義，就是「有一個新的資源被成功創建出來了」。\n所以當 API 返回 201 的時候，就是表示這一次的請求成功、並且也有成功的去創建一筆數據出來，而也因為在 REST 風格中，POST 方法通常就是拿來表示對應到資料庫的 Create 操作，因此「201 Created」這個 Http Status Code，通常就是會用在 POST 方法的 API 返回值上（就是用來表示有一個數據被創建出來了）。\n補充：不熟悉 RESTful API 的話，可以參考這篇 RESTful API 設計指南 文章的介紹\n202 Accepted # 而在 2xx 的大類中，「202 Accepted」所代表的，是「這一次的請求已經被接受，但是尚未處理完成」這樣。\n所以當前端收到 202 的返回值時，前端就可以知道，後端已經有收到這一次的請求了，但是因為這個任務實在要做太久，所以後端就先返回 202 的 Http Status Code 給前端，用來表示「我已經開始做了！」的意思，所以當前端收到 202 之後，前端就可以先去處理其他的事情，而不用被這個 API 給 blocked 住這樣。\n3xx : 重新導向 # 看完了 2xx 的大類之後，接下來我們接著來看 3xx 的大類。\n在 3xx 開頭的大類中，所有的 Http Status Code，就都是表示 「重新導向」 的意思。而在 3xx 的大類中，常見的 Http Status Code，有 301 和 302。\n301 Moved Permanently # 所謂的「301 Moved Permanently」，就是表示這個 url 「永久的」 搬家了，注意這裡的關鍵字是「永久的」。\n所以假設當前端去請求某個 API，但是該 API 返回 301 時，那這時候就是表示，前端所請求的這個 API 的 url，他已經永久搬家了，而至於這個 API 搬去哪裡呢？通常就是會放在 Response header 的 Location，告訴前端說「我搬去了哪裡」。\n所以前端就可以再去請求 Response Header 中的這個新的 url，進而就可以真正的取得到新的 API 所返回的數據了。\n舉例來說的話，像是我之前在架設個人網站時，就有從 https://kucw.github.io，搬家到 https://kucw.io，所以現在如果請求舊的 https://kucw.github.io 的網站的話，就會返回 301 的 Http Status Code，告訴前端這個網站已經永久性的搬家了，並且會將新家的網址（也就是 https://kucw.io），放在 Response header 中的 Location 的裡面。\n302 Found # 如果說 301 是表示「永久的」搬家的話，那麼「302 Found」所表示的，就是 url「暫時性」的搬家。\n所以假設某個 API，他只是「暫時性」的搬家的話（ex: 這個 API 目前可能正在調整功能中），那麼就可以回傳 302 的值給前端，就是去告訴前端，你這一次先去請求這個臨時的 url 這樣。\n所以 302 和 301 一樣，就都是會將新的 url，放在 Response header 的 Location 裡面，告訴前端「我搬去了哪裡」，就讓前端可以再去 call Location 中所提供的網址，找到確切的新 url。\n不過老實說，對於後端工程師而言，通常是感覺不太到 301 和 302","date":"2024-08-20","objectID":"ee57f8d7c8ccc8bc453faec92a2fca6a","title":"一文搞懂 Http Status Code，詳細解析 200、301、401、403、500、503","url":"https://kucw.io/blog/http-status-code/"},{"categories":["其他技術分享"],"content":"RESTful API 可以說是在後端日常的開發中，很常見的一種設計 API 的方式，因此這篇文章我們就來介紹一下，到底要如何才能設計出一個正確的 RESTful API 吧！\n目錄 什麼是 RESTful API？ 如何設計出 RESTful API？ 1. 成為 RESTful API 的條件一：使用 Http method，表示要執行的資料庫操作 2. 成為 RESTful API 的條件二：使用 url 路徑，描述資源之間的階層關係 3. 成為 RESTful API 的條件三：Response body 返回 JSON 或是 Xml 格式 設計 RESTful API 的總結 補充：我們真的需要 RESTful API 嗎？ 結語 什麼是 RESTful API？ # 所謂的 RESTful API，他是一種設計 API 的風格，而他的目的，就是為了 「簡化工程師之間的溝通成本」。\n所以 RESTful API，他的目的其實非常單純，就只是為了簡化工程師之間的溝通成本，才提出了一系列的方法，去統一不同工程師所設計出來 API 的風格。\n不過在這邊要先強調一下，RESTful API 他只是一種設計風格而已，並不是強制的規定，所以如果你在設計上，完全不遵守 RESTful API 的建議，也是完全沒問題的！\n只不過因為 RESTful API 的設計風格，他非常的通用，所以有非常多工程師，都很喜歡使用 RESTful API 的設計方式，因此了解 RESTful API 的設計理念，還是會非常吃香的～\nOK 那所以，我們就來看一下，到底什麼是 RESTful API。\n那如果我們把「RESTful API」這兩個單字給拆解一下的話，就可以將他們拆解成是「RESTful」和「API」。\n那後面的 API 的部分就比較簡單，基本上就是設計出一個 url，然後讓前端可以去 call 這個 url，去和後端請求數據這樣，那不過前面的 RESTful，他就大有來頭了！\n其實 RESTful，他是一個形容詞，意思是「符合 REST 風格的」，所以 RESTful API 合在一起看的話，就是指「設計出一套符合 REST 風格的 API」。\n那 RESTful 在這裡，他其實是用到了英文的文法邏輯，在英文的文法中，如果想要把一個「名詞」，變成是「形容詞」的話，那就只要在後面加一個 ful 就好。\n所以像是在英文裡面：\nbeauty 是名詞（意思是漂亮），beautiful 是形容詞（意思是漂亮的） peace 是名詞（意思是和平），peaceful 是形容詞（意思是和平的） 所以同樣的道理，REST 是名詞，表示「REST 設計風格」，而 RESTful 是形容詞，就是表示「符合 REST 設計風格的」。\n所以當我們在說 RESTful API 的時候，我們實際上所表示的，就是指「你的 API 設計的很符合 REST 的風格」，或是更白話一點，就是表示「欸～你的 API 很 REST 哦😏」，所以這個 RESTful，他本質上是一個形容詞，用途就是用來形容目前這個 API，是不是符合 REST 這個設計風格。\n那所以當大家看到某個 API 很符合 REST 的設計風格時，就可以稱呼這個 API，他是一個 RESTful API，那就表示他是一個「很符合 REST 風格的 API」了！\n如何設計出 RESTful API？ # 當大家了解了 RESTful API 的概念之後，接下來我們就可以來探討一下，什麼是「REST 設計風格」，而我們又要如何將我們的 API，去設計成「符合 REST 風格」的 API 了。\n如果我們想要設計出一個「符合 REST 設計風格」的 API，也就是設計出一個 RESTful API 的話，那這個 API，就必須要滿足 3 個條件。\n1. 成為 RESTful API 的條件一：使用 Http method，表示要執行的資料庫操作 # 如果想要設計出 RESTful API 的話，那麼這個 API，他就必須要使用 Http method，去表示這個 API 的行為動作。\n舉例來說的話，目前在 Http 的世界中，就有非常多的 Http method 可以選擇，像是大家如果有安裝 Postman、或是 Chrome 的擴充功能 - Talend API Tester 這類的 api 請求工具的話，那麼在請求的時候，就有非常多種的 Http method 可以選擇（如下圖所示）。\n而要怎麼樣在設計 API 的時候，去選擇合適的 Http method，這個就非常的重要了！\n如果我們想要去設計出 RESTful API（也就是設計出符合 REST 風格的 API）的話，那麼這個 API，他就必須要 「使用 Http method，去對應到資料庫的 CRUD 操作」。\n這邊我們先題外話補充一下「資料庫中的 CRUD 操作」的概念，在資料庫中，CRUD 就是描述了資料庫中的四個基本操作，也就是：\nCreate（新增） Read（查詢） Update（修改） Delete（刪除） 也因為這四個操作，他們可以說是資料庫中最基本的數據操作，因此他們也常常合稱為「CRUD」，即是「增查改刪」（或是也有人稱「增刪改查」）。\n而在我們了解了資料庫中的 CRUD 的概念之後，如果我們想要去設計出 RESTful API 的話，那我們在設計 API 的時候，就必須要去 「使用 Http method，去對應到資料庫的 CRUD 操作」。\n在 RESTful API 的定義中，假設這個 API 想要執行的，是 Create（新增數據）的操作的話，那麼 REST 風格就會建議，要在 Http method 中使用 POST 方法來請求；而如果這個 API 想要執行的，是 Delete（刪除數據）的操作，那麼 REST 風格就會建議，在 Http method 中要使用 DELETE 方法來請求。\n所以在設計一個 RESTful API 時，基本上我們就是會去依據這個 API 想要執行的操作，去設計成要使用哪一個 Http method 來請求。\n所以簡單的說的話，如果大家都使用 REST 的風格來設計 API 時，那麼以後當我們看到某個 API 是使用 GET 方法來請求時，那我們就可以快速的知道，這個 API 是要去執行 Read（讀取數據）的操作；而假設這個 API 是使用 POST 方法來請求時，那我們也可以快速的知道，這個 API 是要去執行 Create（新增數據）的操作。\n所以假設我們想要設計出 RESTful API 的話，那麼第一個必要條件，就是 「使用 Http method，去對應到資料庫的 CRUD 操作」。\n2. 成為 RESTful API 的條件二：使用 url 路徑，描述資源之間的階層關係 # 而至於成為 RESTful API 的第二個條件，則是 「使用 url 路徑，描述資源之間的階層關係」，那這個聽起來可能有點抽象，所以我們就直接透過一個例子，來了解一下這個概念是什麼。\n假設現在我們有一個使用 GET 方法來請求的 API，即是 GET /users，那麼當我們第一眼看到這個 API 時，因為這個 API 是使用 GET 方法來請求，所以我們可以知道他是要去執行 Read（讀取數據）的操作，所以這個 GET /users 的含義，就是「取得所有 user 的數據」。\n那麼假設現在，我們又有一個使用 GET 方法去請求的 API，即是 GET /users/123，那麼這個 GET /users/123 的含義，就是「取得 user id 為 123 ","date":"2024-08-13","objectID":"52d8dbf5dc05a514a4da30669c51ff05","title":"RESTful API 設計指南，3 個必備條件缺一不可！","url":"https://kucw.io/blog/restful-api/"},{"categories":["自媒體經營"],"content":" 記錄經營《古古的後端筆記》個人品牌的重大事件\n最後更新時間：2024.12.16\n2024 年 # 2024.12.10：Threads 經營 4 個月破 5000 人追蹤（2024.8.27～2024.12.10） 2024.11.12：出版人生中的第一本書，Spring Boot 零基礎入門：從零到專案開發，古古帶你輕鬆上手（iThome鐵人賽系列書） 詳細記錄：2024.11 月自媒體活動：我出書了！！ 2024.8.6：夢開始的地方，《古古的後端筆記》個人品牌誕生，發送第一封電子報創刊號 ","date":"2024-08-10","objectID":"292478d572aba809ff5d18ba7620b2af","title":"軟體工程師的自媒體之路 - 創業大事記","url":"https://kucw.io/blog/as-a-content-creator/milestone/"},{"categories":["其他技術分享"],"content":"哈囉，我是古古！最近又到了 iThome 鐵人賽的開賽期（每年 9 月附近），在 2023 年時我也有報名參加過 iThome 鐵人賽，題目是《Spring Boot 零基礎入門》，當時有幸得到了《優選》的獎項（得獎名單），因此希望可以透過這篇文章，分享一些寫作心法給大家。\n不過要先在此申明一下，我不是什麼技術大神，並且也不是得到最高級別的《冠軍》的獎項，所以以下的分享，就單純只是我個人的參賽心得而已，不具有 iThome 鐵人賽任何的官方效力唷！\n目錄 iThome 鐵人賽官方得獎評判標準 道理我都懂，但是要如何選題？ 題目挑好了，然後呢？ 題目挑好了、大綱也寫好了，再來呢？ 文章段落 文章的易讀性 上面我都照做了，但是還是沒得獎怎麼辦… 結語 iThome 鐵人賽官方得獎評判標準 # 但凡是參加 iThome 鐵人賽的參賽者們，可能或多或少目的都是為了得獎去的，因為得獎可以出書啊哈哈哈哈！！！我能理解出書真的是很誘人的一個因素（我自己其實也是為了出書來的😆），所以假設我們今天參賽是以「得獎」為目標的話，那麼搞懂 iThome 官方的得獎判斷標準就很重要。\n以下內容擷取自 2024 年 iThome 鐵人賽的官方活動簡章：\n陸、 主題競賽評審要點： - 主題：主題規劃符合該組別的立意，並能充份切合所選參賽主題下，參賽者所訂定之議題 - 結構：30 篇文章組織良好、其所規劃結構足以引導讀者理解參賽者訂定之議題 - 內容：文章內容的技術或經驗具備專業性、豐富性、深入性 - 表達：透過適當文字、圖片、程式碼或影片等方式，讓人更容易理解 透過這份官方活動辦法也可以看到，想要得獎的話，必須要有 「組織良好」 的文章，並且該文章的內容，需要具備 「專業性」、「豐富性」、「深入性」，而且需要在文章中添加 「圖片」 或是 「程式碼」 的輔助，使人更容易理解。\n所以這個簡單的說的話，就是除了你的主題要寫的夠深、夠豐富之外，也需要寫出一個結構良好，並且有圖、有程式碼的文章。\n所以只要抓好這幾個重點的話，就能盡量提升得獎的機率，最終拿到出書的資格了！\n道理我都懂，但是要如何選題？ # 當我們了解了 iThome 鐵人賽官方的得獎標準之後，再來就可以進到選題的部分了！\n不得不說，選題的好壞真的影響很大，因為得獎的標準之一是 「專業性」 和 「深入性」，所以如果你今天挑的題目是「XXX 入門」的話，那麼在「深入性」上就會比較吃虧（因為你沒辦法真的寫得很深入）。\n舉例來說，我當時參賽的題目是《Spring Boot 零基礎入門》，因為我的定調是「入門」文章，所以其實這個選題，就注定了我在「深入性」這一點上，沒辦法寫的太詳盡。\n如果拿 2023 那一年「Software Development 區」的冠軍名單出來看的話：\n《為你自己學 Ru\u0026hellip;..st》— 高見龍 《圖解C++影像處理與OpenCV應用：從基礎到高階，深入學習超硬核技術！》— VincentYeh 就可以發現，得到冠軍的這兩位大大，他們寫的內容真的都是很有深度的內容，更別說其中一位參賽者，還是很有名的高見龍老師，所以這個廝殺真的是只有激烈可言😂。\n所以假設你想要得獎，那麼寫一個超級深入的硬派文章，確實可能會提升得獎的機率。不過老實說，我其實不是很建議大家一定要挑超級艱深的題目寫，因為這樣你只是為了寫而寫而已，就算這樣的文章得獎了，裡面也沒有靈魂的啊！！！\n所以會比較建議大家，你所寫的文章，盡量還是和你自己課業上所學、工作上有接觸的部分會比較好，因為自己熟悉的題目才是最好發揮的，而且就算挑的題目是「XXX 入門」的題目，只要掌握好文章結構的話，仍舊是能拿到《優選》或是《佳作》的獎項的（而且其實這兩個獎項也都可以出書的！）。\n所以就建議大家，不用太糾結《冠軍》這兩個字，建議還是挑你平常接觸最多、你最熟悉的那個主題來寫，這樣子寫起來才會快樂，得獎的時候成就感才會爆棚啊XDD（媽！這我寫的！）。\n題目挑好了，然後呢？ # 題目挑好之後，就可以「Welcome to 寫作地獄」了🤗。（我是指，Welcome to 快樂的寫作人生了）\n好啦說正經的，其實邊挑題目的時候，大家就可以開始思考：「我這篇文章是要寫給哪類人群看的？」，這個問題非常的重要，重要到會影響你整體的文章編排架構，所以一開始多花一點時間練習思考這個問題，是非常有意義的。\n舉例來說，像是我之前參賽的《Spring Boot 零基礎入門》主題，因為我的定調是入門文章：\n所以我預設的讀者人群，就是「完全沒有碰過（甚至沒聽過） Spring Boot」的人 而我希望這些人在看完我的文章之後，就「能夠使用 Spring Boot，實作出一個簡易的後端系統」 所以其實我在開始動筆寫第一篇文章之前，我就已經先決定了這 30 篇文章的走向，而我的目標，就是讓完全沒用過 Spring Boot 的人，在看完這 30 篇文章之後，就能夠去實作出一個簡易的後端系統這樣。\n所以圍繞著這個目標，我就可以開始思考，我要如何去介紹 Spring Boot 的基本用法、要如何去示範這些程式給大家看，然後就可以慢慢把 30 天的大綱給生出來了。\n所以總結來說的話，在我真的開始動筆寫第一篇文章之前，我其實就已經把這 30 篇文章的大綱都列好了。所以我會先列出說，我大概要講哪幾個部分、並且每個部分大概要佔幾篇文章的內容，所以就會像是下面這個樣子，就是先列出哪幾天要介紹哪些主題這樣：\nDay 1～4：Spring Boot 簡介、開發環境安裝 Day 5～12：Spring 框架的特性 - IoC 和 AOP Day 13～23：Spring MVC 介紹 Day 24～28：Spring JDBC 介紹 Day 29：實戰演練：打造簡易的圖書館系統 Day 30：Spring Boot 零基礎入門總結 如果要我說的話，「文章編排」這件事，可以說是整個 iThome 鐵人賽最難的地方，因為在這個環節中，你必須要去思考：「你想要介紹哪些內容？」、「你想要怎麼編排這些內容？」，而這個過程，等於是你要去逼自己，要想盡辦法的把你心目中零碎的知識片段，組織成一個完善的 30 篇文章架構。\n老實說這件事真的是最難的，所以一開始大家如果覺得很難排的話，是非常正常的，你在這個階段可能會覺得腦袋中東西很多、很混亂，不知道怎麼下手將他們呈現，有這些情況都是很正常的。\n所以我會建議大家，如果你真的覺得很混亂、不知道該怎麼下手，那你可以先跳過「文章編排」這一關，直接就動筆開始寫吧！等到你寫了一次、兩次，更有經驗之後，下一次參賽時，可以再回頭來挑戰「文章編排」這個大魔王。\n而等你真的克服這個大魔王之後，你就會發現自己的寫作技巧好像昇華了！以後不管是寫技術文件、或是寫其他的教學文章，都會寫得比以前還要順手，說不定等到那一天，就會換你在網路上分享寫作心法給其他參賽者了～（期待那一天的到來！）。\n題目挑好了、大綱也寫好了，再來呢？ # 當大家克服最難的大魔王「文章編排」之後，再來就是努力的寫稿子寫稿子\u0026hellip;loop，然後配上超級堅持不懈的精神，最終就可以順利完賽了！！\n不過其實寫稿子這件事，也是有一些心法可以分享給大家的，那就是要掌握好「文章的段落」，以及「文章的易讀性」。\n文章段落 # 以「文章段落」來說，其實當大家在閱讀這篇文章時，如果仔細觀察一下的話，就會發現我會使用 「H2（二級標題）」，為每個段落進行分段。\n所以像是在這篇文章中，一開始的段落標題就是「iThome 鐵人賽官方得獎評判標準」，這一段我們談的就是 iThome 的官方得獎標準；而像是目前這一段的標題就是「題目挑好了、大綱也寫好了，再來呢？」，談的就是文章寫作的內容","date":"2024-08-08","objectID":"ec2a594722d7de8181108e39fca0f7a4","title":"iThome 鐵人賽 - 得《優選》獎項的寫作心法","url":"https://kucw.io/blog/ithome-sharing/"},{"categories":["自媒體經營"],"content":"哈囉，我是古古，我的本業是軟體工程師，但是卻誤打誤撞走進了自媒體這條路。\n這個專欄裡面記錄了我作為一個軟體工程師，是如何踏入自媒體這條路，並且也記錄了我是如何在接觸了許多創作者之後，下定決心要認真嚴肅的看待自媒體這份事業。\n這份專欄不只是寫給我自己，也想寫給對自媒體有興趣的你，不管你的職業和身份是什麼，都希望這份專欄可以為你帶來一點幫助。\n現在時間是 2024.8.6，我在這裡寫下我的第一篇電子報《古古的後端筆記》的創刊號，作為我認真開始經營自媒體的起點，後續也會持續更新這份專欄，如果有興趣的話，也歡迎訂閱電子報：https://kucw.io/bio。\n踏入自媒體的起因 # 那！雖然有點突然，不過我還是先來個自我介紹吧XD\n我是古君葳，大家也可以叫我古古，我大學和研究所念的都是資工系（陽明交大資工系、台大資工所），然後在唸研究所的時候，我有申請去北京大學交換過半學期，所以也就剛好，在畢業之後，就在北京找過一份 Java 後端工程師的工作，這份工作大概是做了一年左右。\n然後在 2019 年初，大概是在 3 月附近，我因為家裡的事情，就決定回到台灣發展這樣，所以後續我就是加入 Garmin，負責開發 Java 後端程式，然後，就意外的開啟了這條自媒體之路了。\n所以如果回顧一下我的求學經歷、以及工作經歷的話，就也可以發現，在求學和工作的過程中，我真的是完全跟自媒體沾不上邊，就是一個照著社會安排的資工系學生的路線，穩穩地去做工程師的工作這樣。\n不過，我現在之所以能夠坐在這裡，寫著這個《古古的後端筆記》的創刊號，其中一個很大的轉折點，就是當時的我，接觸到了「線上課程」這件事。\n我當時在北京工作的時候，我身邊的同事、朋友，每一個人，真的不誇張，真的是每一個人，都是在談「你學了什麼課？」、或是「你最近買了什麼課？」，當然我的經驗不能代表所有人的經驗，不過這對於當時的我而言，完全就是文化衝擊XDD，就…沒想過在這個世界上，還有「線上課程」這麼好用的東西可以學習。\n比起實體課，不得不說我自己是更愛上線上課程，因為線上課程有字幕！！有動畫！！而且老師講廢話的時候還可以跳過（大家不要學🤣）！！還可以開 2 倍速看！！！完全就是超級高效率的學習方式！！！\n也就是因為當時在北京工作的時光中，我接觸到了不一樣的觀點和想法，所以也就間接導致我後續回台灣之後，會因緣際會投入到線上課程的教學。\n回台灣的發展期 # 所以在 2019 年回到台灣之後，我就進了 Garmin，就開始了 Java 後端工程師的工作了。\n不過在工作之餘，我就一個手癢，想說要不買點課來學吧XDD，但我一查了之後就發現，台灣怎麼好像….想要買課還買不到？可能是當時台灣的技術課程比較少人錄、也有可能是 Udemy 上面的英語課程就很實用了，反正就是當時在台灣的線上課程領域中，其實沒有多少選項可以選擇。\n那所以這時候我就想啦！如果我試著幫大家整理一下，把我所了解到的 Spring Boot 知識，都把他整理起來，然後錄製成課程，這樣子大家就有一個中文的課程可以考慮了嘛！所以大家就不用再去聽 Udemy 上面的印度老師的口音，然後很克難的，自己一個一個的去查這個英文單字的翻譯是什麼了（說的就是我🥹，有時候上課我都不知道是在學技術還是學英文\u0026hellip;）\n所以那時候我的想法很單純，就是想要結合我自己的工作經驗，幫大家整理出一個講中文、然後又是台灣用語的 Spring Boot 課程，就讓大家可以不用學的那麼克難這樣～\n但，事情的轉折點，就是這個「線上課程」！\n就是因為我錄製了線上課程，所以我開始有機會，去接觸到其他的創作者，去了解到他們是如何去進行創作的。然後我也慢慢的去了解到，原來身為一個創作者，除了要去創作你的原創內容之外（像是錄製線上課程），同時，你也必須要去經營你的粉絲、維護你的社群、打造你的影響力、進而讓你的個人品牌變得越來越茁壯。\n所以其實對於「自媒體」這條路而言，他並不只是「錄製線上課程」這麼單純而已，他要投入的，是更多角化的經營、是更嚴肅的態度，是要把這條自媒體的賽道，當成是一個事業來經營。\n所以在我了解到這個事實之後，我就開始思考，作為一個不小心踏進線上課程領域的工程師，我當初完全就是運氣好，所以才能夠走到這裡（再次感謝各位同學的支持🙏），但是，老天把我帶到了這裡，接下來我到底要不要認真嚴肅的，去看待這份自媒體的事業，就是我必須做出的決定了。\n那當然啦～我的決定就是「Yes！」，不然應該就沒有這個專欄文章了XDD，也因為我的決定是 Yes，所以接下來，我想要認真的，去經營自媒體這條路，我想試試看，到底可以將《古古的後端筆記》這個個人品牌，去成長到什麼樣的地步。\n所以接下來，就歡迎大家跟我一起，收看這個大型的真人秀吧！\n歡迎收看大型真人秀：軟體工程師的自媒體之路 # 打字打到這裡，我們終於可以來介紹《古古的後端筆記》的電子報了！\n簡單的說的話，這份電子報，就是我嘗試經營自媒體的第一步！不知道大家有沒有聽過國外一個很紅的後端技術分享的作者，就是 ByteByteGo，我之所以第一步會想要經營電子報，也是受到 ByteByteGo 影響，就覺得每週可以學習一點後端知識也很讚這樣！\n那既然我們都創刊了，夢想還是要有的，所以我就先來畫個餅，就希望可以把這份《古古的後端筆記》的電子報，做成是台灣的 ByteByteGo！\n不過當然我自己也知道，我本身的後端技術能力還不到頂尖，所以目前我離 ByteByteGo，仍舊是有一大段距離需要努力，但是，就希望可以透過這份電子報，每週輸出後端知識給大家，然後也藉著這份動力，能夠讓自己的後端技術可以更精進這樣。\n所以大家的訂閱，對我而言是一份責任、也是一份動力，所以大家如果後續對於電子報中的任何主題，有任何的想法的話，都歡迎直接回信給我，每一封我都會看的！電子報的優勢就是這樣，只要你回信，我就一定能收到，而且信件內容也不會公開，所以如果大家有什麼想對我說的話，都很歡迎大家回覆～\n結語 # 好啦！大概是這樣，總而言之這個大型的真人秀：軟體工程師的自媒體之路，仍舊會持續下去，我有打算把這個系列變成是一個專欄，就是會定期的跟各位老闆們回報一下，我目前的經營進度這樣XDD。最近不也挺流行 Build in Public 的創業模式嗎？那我就剛好也來把這份自媒體的創業之路，一起分享給大家～\n最後，我是古君葳（古古），是一個軟體工程師、同時也是一個自媒體的創業者，感謝大家的訂閱！就讓我們一起見證下去吧！\n如果你對後端筆記有興趣，也歡迎免費訂閱《古古的後端筆記》電子報，每週二為你送上一篇後端技術分享，一起變強💪\n","date":"2024-08-06","objectID":"ab09bb72d5b85a690065100ce1b0f361","title":"軟體工程師的自媒體之路 - 電子報創刊號","url":"https://kucw.io/blog/as-a-content-creator/1/"},{"categories":["Other tech"],"content":"當 Mac 升級到 Sonoma 14.0 之後，只要按下鍵盤左側的「中/英」鍵（也就是 Caps Lock 鍵），Mac 就會出現一個藍色的游標，提示你現在開啟了 Caps Lock\n要關閉這個藍色游標的話，只需要以下三個步驟\n開啟 Terminal 執行以下兩行指令（分別複製後執行） sudo mkdir -p /Library/Preferences/FeatureFlags/Domain sudo /usr/libexec/PlistBuddy -c \u0026#34;Add \u0026#39;redesigned_text_cursor:Enabled\u0026#39; bool false\u0026#34; /Library/Preferences/FeatureFlags/Domain/UIKit.plist 重新開機 這樣就可以關閉那個討厭的藍色游標了，天下太平啦！！\n","date":"2023-10-27","objectID":"ad544515bd017bcd60bbd400a6b361de","title":"關閉 MacOS Sonoma 的 Caps Lock 游標","url":"https://kucw.io/blog/2023/10/mac-sonoma-disable-caps-lock-indicator/"},{"categories":["Life"],"content":"說到夏天，當然就是陽光、沙灘、海！但是如何在炎炎夏日做好防曬措施，絕對是大家最關心的一項議題，本文會從兩個方向來分析如何挑選防曬乳\n挑選防曬乳之前，先了解太陽中的紫外線！ # 太陽中有很多紫外線，其中對我們影響最大的就是 UVA 和 UVB，而 UVA 和 UVB 這兩個紫外線，可以簡單分成兩部分：\n但凡涉及到美醜的，不論是變黑、變老、曬斑、長皺紋，都是 UVA 造成的，也就是 UVA for Aging（老化） 但凡是涉及到生病死亡的，不論是曬傷、皮膚癌、白內障，都是 UVB 惹的禍，也就是 UVB for Burn（灼傷） 所以到這邊大家可以先有個概念，假設你今天想要防曬的是美醜的部分，你就要挑選 UVA 防護比較高的防曬乳，假設你今天想要的是別那麼早死，那你則是要挑選 UVB 防護比較高的防曬乳\n如果是想要照顧美醜的同時、又別那麼早死，那就是挑選 UVA + UVB 皆有防護的防曬乳，不過想當然就是會比較貴啦！\n我不想那麼早死！如何挑選 UVB 防曬乳？ # 美醜可能不是每個人都在意，不過大家應該都不想早死，所以在談論美醜之前，我們先討論如何挑選避免早死的防曬乳，也就是如何針對 UVB for Burn 進行防曬\n要防曬 UVB 很簡單，就是看 SPF 的指數！\nSPF 50 是最棒、SPF 30 是次棒、SPF 15 是普通，而最猛的是 SPF 50 +++++，後面的 ++ 越多表示越厲害\n當然 SPF 這個指標本身的意義不是這麼簡單暴力，他的實際意思是「能夠延長多少不被曬傷的時間」，不過我是建議一般人也不用了解的那麼詳細，就挑數字最大、++ 最多的買下去就對了\n其實大家如果只是單純想防曬 UVB 的話，防曬乳真的是便宜到有剩，因為萬一你得了皮膚癌，最終還是要花費社會的醫療資源來救你，所以各國政府為了避免醫療人口太多，都會把 UVB 防曬乳的價格定的很便宜\n因此如果你今天有出遊計畫，卻又不知道怎麼挑選防曬乳，那就先去隨便買一條 SPF 50+ 的防曬乳來用就可以了！\n除了不那麼早死之外，我還想保持的漂漂亮亮的！如何挑選 UVA 防曬乳？ # 來，只要一講到美醜，這個砸的錢就沒有上限了🥲\n如果你是想要讓自己不會變黑、變老、曬斑、長皺紋，那就是要針對 UVA for Aging 進行防曬\n你知道，因為大家美不美醜不醜，政府都不會因此而增加醫療負擔，所以各國政府其實不是很 care 這件事，因此在 UVA 的防護上，其實沒有一個各國通用的標準可以參考\n現在市面上就有好幾種國際組織，分別對 UVA 的防護進行評斷，包含：\n日本的 PA 系列 英國的 Boots Star Rating 星星系列 歐盟的 PDD 系列 \u0026hellip;..等等，族繁不及備載 也因為每一個組織都用不同的標準來衡量 UVA 的防護，所以說實話，大家就挑自己喜歡的牌子來買就好，反正也沒有一個權威的公信力可以參考，況且美醜也是比較主觀的看法，所以大家只能多買多看，比較哪一種防曬乳是最適合自己膚質、並且荷包又買得起的\n不過如果真要我推薦的話，我會建議大家參考日系的 PA 標準，PA 後面越多 ++ 越好，所以 PA++++++++ 就是 UVA 防禦力爆表的防曬乳（當然價錢可能也爆表）\n會推薦日系的 PA 標準，是因為畢竟亞洲的人種還是比較像，在意的美醜價值觀也比較相似（要白！要透！），而歐美國家畢竟都是白人居多，白人天生就曬不黑啊XDD，所以如果大家有選擇障礙的話，建議可以先從日系的 PA+++ 的防曬乳開始挑起\n不過大家在挑 UVA 防護高的防曬乳時，記得同時也要看防禦 UVB 的 SPF 係數啊！！如果因為在意漂亮而忽略了身體健康，那就得不償失了😖\n總結 # 最後我列了一張表格，整理了一下文章中提到的資訊，提供給大家參考\nUVA UVB 由來 UVA for Aging（老化） UVB for Burn（灼傷） 影響範圍 美醜 生病死亡 造成的傷害 變黑、變老、曬斑、長皺紋 曬傷、皮膚癌、白內障 防曬指標 日本 PA、歐盟 PDD\u0026hellip;.等等 SPF 簡易挑選準則 PA+++++（++ 越多越好） SPF 50+++++（數字越大越好、++ 越多越好） 價錢 貴 便宜 ","date":"2023-05-03","objectID":"18eab7b5d98b5fa260b6fa09fd1924aa","title":"夏日防曬！五分鐘了解如何正確挑選防曬乳","url":"https://kucw.io/blog/2023/5/sun-and-ultraviolet/"},{"categories":["Life"],"content":"最近剛從宿霧玩回來，記錄一下 2023 宿霧 6 天 5 夜旅遊的行程規劃，也分享給有需要的大家\n出遊時間：2023.4.20 - 2023.4.25（6 天 5 夜） 重點行程：墨寶沙丁魚風暴、歐斯陸鯨鯊共游、薄荷島巧克力山+眼鏡猴 走法：逆時針（機場/宿霧市區 -\u0026gt; 墨寶 -\u0026gt; 歐斯陸 -\u0026gt; 薄荷島 -\u0026gt; 機場/宿霧市區） 人數：2 人 總花費：一人 28328（含機票+飯店+所有行程+按摩+各種喝爆+伴手禮） 行前準備 # 出發前 30 天內，線上申請 菲律賓電子簽證 假設 4/20 出發，就是 3/20 開始可以申請 記得印紙本出來，登機和入境要看 在台灣換好 美金現鈔 帶在身上 帶著美金去菲律賓當地再換披索，是換匯最佳解，有錢想省事的也可以在台灣直接台幣換披索 我們兩人 6 天在當地共花 800 美金（不含機票+飯店），剛剛好全花完沒有剩，供大家參考 列印 數位新冠疫苗證明 文件 登機要看 網路上買 Sim 卡，機場取貨 這個有許多選擇，也可以到當地再買，看自己需求 我們是直接買 KKday 上的 菲律賓 Globe Telecom 5G 網卡 強烈推薦帶一台 GoPro，水上活動好玩好拍很多 我們是自己有所以沒有額外租，不過如果要租的話，建議在台灣租，然後帶過去宿霧比較方便 旅平險 or 旅行不便險，依自己需求購買 Day 1 # 詳細資訊 當天重點行程 搭飛機+搭車至墨寶 當天住宿 Pig Dive Hostel（墨寶的青年旅館） 備註 我們在墨寶有許多行程和包車，都是請 Pig Dive 代訂的，老闆是台灣人會說中文，可以先訂住宿之後再跟他們聯繫幫忙代訂行程 搭乘 星宇航空 週四航班，桃園機場（早上 8:30 起飛） -\u0026gt; 宿霧（中午 11:25 降落）的班機 記得帶一支筆，在宿霧機場還需要填寫紙本入境卡，才能入境 聯絡包車司機，下午 2 點從機場出發前往墨寶 私人包車由 Pig Dive 住宿代訂 中間有請司機順路多停一站 SM Seaside City Cebu（要加一點錢），我們是在這邊換匯，1 美金 = 55 披索 大約傍晚 6 點抵達墨寶 Pig Dive Hostel，晚上自由活動 Day 2 # 詳細資訊 當天重點行程 Kawasan Falls 溯溪、墨寶沙丁魚風暴 當天住宿 續住 Pig Dive Hostel 備註 x 早上 7 點集合前往 Kawasan Falls 溯溪（私人包車，不過是搭乘嘟嘟車） 約莫中午 12 點半結束回到旅館，可休息或是附近晃晃 此行程由 Pig Dive 代訂 下午 2:30 集合前往沙丁魚風暴潛水 我們兩人皆有 OW 證照，所以是直接潛一隻水肺 約 4 點結束回到旅館 此行程也是請 Pig Dive 幫忙代訂 晚上也是在墨寶當地自由活動 Day 3 # 詳細資訊 當天重點行程 歐斯陸鯨鯊共游、搭船至薄荷島 當天住宿 Alona Austria Resort（薄荷島） 備註 x 早上 4 點集合前往 歐斯陸鯨鯊共游（私人包車） 車程大概 1 個半小時，所以約莫是快 6 點抵達歐斯陸 看完鯨鯊大概 8 點多結束 此行程也是請 Pig Dive 代訂（含包車+共游） 鯨鯊結束後請司機載我們到 Quartel Beach，此為搭船到薄荷島的港口 從歐斯陸到薄荷島，只有唯一一家船公司 APEKOPTRAVEL 可以選，船票可以事前線上訂，但是記得船票要印紙本出來，登船要看 因為抵達時間太早，所以我們有先在附近吃早餐 早上 11:30 搭船前往薄荷島 約莫下午 1 點抵達薄荷島 可以直接當地隨意攔一台車嘟嘟車前往飯店 飯店 check-in 後就自由活動，今天比較悠閒 Day 4 # 詳細資訊 當天重點行程 跳島 Island hopping（巴里卡薩島浮潛 + 處女島 or Frencesco島） 當天住宿 續住 Alona Austria Resort 備註 我們在薄荷島兩天的行程，都是找當地的導遊 Lutz Gen 幫忙安排的，他英文很好口音很不重，很推薦（可以叫他 Jay） 早上 6 點旅館接人，到 Alona beach 登船（私人包船） 補充一下 Jay 沒有買船，所以當天陪同的船夫會是其他當地人，雖然行程 Jay 會事先幫忙喬好，但介意的人可以自己斟酌 出海追海豚 Dolphin watching 巴里卡薩島浮潛 Balicasag snorkeling 可以加價租面鏡蛙鞋，我們有租 浮潛完可以在巴里卡薩島吃早餐，不吃也可以，不過我們有吃，浮潛完很餓 處女島拍照玩水 Virgin island 如果遇到漲潮，處女島會被淹沒到只剩下個牌子，這時船夫會問你要不要去隔壁另一個 Francesco 島，可自由選擇 如果兩個島都想去也可以，不過就要加錢，我們有加，想說都來了 大概下午 1 點回到 Alona beach，看你玩的速度，反正私人包船沒人趕，船夫就是捨命陪君子陪你到他下班 抵達 Alona beach 可以打電話給 Jay，他會安排人來接你回旅館 抵達旅館後，下午晚上一樣自由活動 Day 5 # 詳細資訊 當天重點行程 巧克力山 + 眼鏡猴（Chocolate hills + Tarsier） 當天住宿 Henann Resort Alona Beach 備註 這天的行程也是找 Lutz Gen 幫忙安排的 早上 8 點旅館接人（私人包車） 巧克力山 Chocolate hills 有興趣也可以加玩一項自費行程：巧克力山越野車探險 ATV，我們有玩，比起開越野車探險本身，反而是越野車導遊的拍照技術更狂一點XD 看眼鏡猴 Tarsier 人造森林拍美照 Man made forest 5 分鐘可完成的景點，還不錯 Loboc 河竹筏游船+ buffet 午餐 Loboc river cruise + buffet lunch 菲律賓式午餐不期不待不受傷害，游船倒是滿有趣，駐船歌手很會唱 因為我們早上多玩了一個越野車，所以我們大概下午 1 點才抵達游船的入口，實際吃午餐的時間為下午 1 點 ~ 下午 2 點半，建議中間可以帶一些小零食以防你餓了 聖母大教堂 Baclayon church 教堂壁畫很酷！台灣滿少看到這類的景點，推 歃血為盟紀念碑 Blood compact 5 分鐘景點，普偏廢，趕時間可跳過 大概下午 4 點回到旅館，一樣看你玩的速度，Jay 會全程開車陪同 晚上一樣是自由活動，我們盡情的享受在 Henann 中XD Day 6 # 詳細資訊 當天重點行程 買伴手禮、搭飛機回台 當天住宿 x 備註 x 早上 9:30 旅館接人，前往薄荷島港口 Tagbilaran Port 車程約 40 分鐘，約莫 10:10 抵達港口 這個車一樣是請 Jay 幫忙訂的 早上 10:40 的船回宿霧市區 約莫中午 12:40 抵達宿霧市區 船票可以事前先訂，船班公司是 OceanJet，可以直接用 Klook 代訂，但是記得船票要印紙本出來，登船要看 下船後我們直接搭計程車去 SM City Cebu 買伴手禮 裡面很大，要什麼有什麼，不用怕買不到 搭計程車記得說魔法關鍵字 \u0026ldquo;By Meter\u0026rdquo;（跳錶計價的意思），終於不用再被當盤子靴 下午 4 點抵達宿霧機場，搭乘 星宇航空 週二航班，宿霧（下午 6:55 起飛） -\u0026gt; 桃園機場（晚上 9:45 降落）的班機 總結 # 以上就是我們這一次宿霧的 6 天 5 夜行程安排，祝大家都能盡興享受海島的陽光沙灘海🏝😎\n","date":"2023-04-27","objectID":"d5bf1c6c98a641bd761f7ad7bd5101dd","title":"2023 宿霧 6 天 5 夜自由行分享（逆時針走法）","url":"https://kucw.io/blog/2023/4/cebu-plan/"},{"categories":["Other tech"],"content":"你是否曾經覺得，明明已經工作了 2、3 年，但是學的東西感覺都不是特別扎實？好像自己平常都只是東學一點、西學一點，程式出錯了就上網查解法，但出去面試被面試官問到技術問題時，你明明記得你看過的，卻不知道為什麼就是答不上來技術細節\n造就這一切的原因，其實都可以歸類成：你沒有建立過自己的知識體系\n好記性不如爛筆頭 # 俗話說「好記性不如爛筆頭」，這句話充分的強調了記筆記的重要性，更何況我們生活在這個知識爆炸的時代，你就算擁有再好的記憶能力，也沒辦法保證能將每一點的知識點都記得牢固\n而且說實在的，要能夠閉著眼睛默寫一段程式出來真的是滿難的，而且其實沒什麼意義，往後你會接觸到更多程式語言、更多框架，所以基本上是不可能把每一行程式全部背下來的\n因此平常比較用不到的知識點，其實不需要特別去記住，只要記在你的筆記裡就行，一但要用上時，只要去你的筆記中查詢，就可以快速找到你曾經學習過的知識，將它應用到你現在遇到的問題上了\n不過有的人可能會說：那這樣跟上網查解法有什麼不同？這個不同之處還是滿大的\n首先網路上的文章參差不齊，你不一定馬上就能找到你需要的解法，就算現在有 ChatGPT 可以輔助使用好了，但是要把關鍵字下的讓 ChatGPT 看得懂也是一門學問\n另外，因為這些文章中所提到的知識點，也沒有被你真正的放進你自己的知識體系，所以當你下次遇到同樣的問題時，你很有可能又忘記怎麼解了，所以你又得重新再次上網查詢解法，被同樣的問題重複絆倒許多次\n所以為什麼經常有大神會鼓勵大家要多寫技術 blog，其實也是相同的道理，你有寫 blog 就表示你有技術輸出，要寫出一篇技術文章，你就得去深入整理你對那個知識點的理解，而透過這樣的方式，無形中就是在為你自己建立知識體系\n因此當你下次又遇到同樣的問題的時候，你就會有印象「對！我曾經遇到過這個問題！」，所以你就可以回頭查看當初所寫的那篇文章，快速的回想起來要怎麼解決這個問題了\n記筆記的工具有哪些？ # 假設你看到這裡，已經認真決定要開始透過寫筆記，去建立自己的知識體系的話，那截至目前為止，記筆記的工具到底有哪些選擇？\n通常第一次筆記的工程師，我會建議選擇最容易取得的平台，也就是 Medium 寫作平台、或是 Notion 這類協作工具\n推薦這兩個平台的原因是因為想盡量降低大家寫作的門檻，因為寫筆記這件事，至少你得先寫起來，再來才是思考哪款筆記工具最好使用，所以在你真的「寫起來」之前，任何屠龍寶刀對你來說都是無用的\n而 Medium 和 Notion 這兩款工具，不僅 UI 介面舒適，並且也沒什麼惱人的廣告，作為第一個上手的記筆記工具還是很適合的\n當你寫筆記越寫越上手、以至於你已經培養了這個習慣之後，接著就可以慢慢找尋適合自己的寫筆記工具了\n工程師我會推薦可以學著使用 Markdown 的格式來記錄筆記，Markdown 是在程式領域裡面滿廣泛使用的一種文字格式，多學習是絕對沒有壞處的，而 Markdown 編輯器我個人最推薦 Typora，雖然需要付一次性買斷的費用，但是我認為非常值得，推薦使用\n總結 # 說了這麼多，最關鍵的因素還是得付出行動，其實有時候自制力差只是行動力不足，就像是想看書的人根本不用上網查：我該怎麼自律才能專注的看書，而是直接拿起書來看就可以了，所以踏出第一步真的沒有這麼複雜\n在變強的道路上總是需要付出時間和精力的，希望大家都可以找到最有效率的方法一起變得更猛💪\n","date":"2023-04-02","objectID":"88d5f52fc52bf3984760e9e39aba41fb","title":"工程師如何累積程式實力？","url":"https://kucw.io/blog/2023/4/rename-iphone-photo-by-date-copy/"},{"categories":["Other tech"],"content":"iPhone airdrop 到 Macbook 上的照片檔名為 IMG_0862.JPG，不利於照片整理，改命名成右邊這種 IMG_20220726_110204.JPG 格式會好整理很多\n想要大量修改圖片檔案名稱為右邊這種格式，可以透過 ExifTool 工具解決\nExifTool 使用教學（Mac 版） # 先到 ExifTool 官網下載安裝檔，並且安裝該 dmg 程式\n打開 Terminal，執行以下指令（最後一個參數值 ~/Downloads/image 為待轉換的照片資料夾，可自行更換路徑）\nexiftool -d \u0026#39;IMG_%Y%m%d_%H%M%S\u0026#39; \u0026#39;-Filename\u0026lt;${DateTimeOriginal}.%e\u0026#39; ~/Downloads/image 執行完成後，照片就會根據創建日期重新命名了，可喜可賀XD\nPS: 注意此指令會直接覆寫檔案檔名，不會備份照片，且此操作無法透過 Ctrl+Z 復原\n","date":"2022-08-05","objectID":"4b1c588264f06e087efb10267f055f4d","title":"重新命名 iPhone 照片檔名為日期時間（Mac版）","url":"https://kucw.io/blog/2022/8/rename-iphone-photo-by-date/"},{"categories":["Other tech"],"content":"想要提升學習效率，不管你是使用什麼工具，只有下面這三點最重要\n系統化的學習 循序漸進 大量練習 系統化的學習，讓你可以在腦中組織這些內容的關係圖，而不是像背單字一樣背一個忘一個\n循序漸進，讓你由淺入深慢慢拓展，沒有人一開始就跳過加減乘除去學微積分的對吧\n大量練習，讓這些知識成為你的肌肉記憶，就像呼吸一樣自然，右手寫左手答\n只要能夠掌握好這三點，不管是學習哪個領域，都可以加速你的學習效率\n什麼樣的學習管道最有效率？ # 要我說，對程式語言這個領域，剛入門的人，我建議是 「直接買付費的線上課程」 來學習\n很多人在初學程式語言的時候，都是先從網路上的免費資源開始學起，因為免費嘛！誰不愛\n但是你仔細觀察過可以發現，網路上的免費資源大部分都是比較碎片化的，因為這些文章，通常只會提到幾個知識點而已，那你這樣東學一點、西學一點，其實是很難打好程式的基礎的\n而線上課程就很好的解決這個問題\n你只要跟著課程的安排好好的上，一般是從最基本的程式語言語法開始教起，然後通常也會教你怎麼使用工具、有哪些快捷鍵可以按、怎麼樣 debug 最有效率\u0026hellip;之類的，這些東西都是真的要實際有實戰經驗的人才有辦法總結的，如果你是靠自己摸索，你可能只會停留在最基本的程式語法而已\n況且能夠開設線上課程、並且累積大量好評的老師，通常都是在各自的領域積累了好幾年的經驗、總結了許多想法，才能夠取得這樣的成績，你如果能跟著這些老師按部就班的學習，一定是比你一個人自己摸索著前進還要有效的多\n還是那句話「時間是最寶貴的資源」，你得把時間拿來創造價值，而不是拿來花在摸索上，所以最好的做法，就是直接買別人整理好的內容來學就對了\n除了線上課程之外，看書這種學習方式其實也是滿好的，因為寫書的作者在寫這本書時，目錄的安排也是循序漸進的從基礎開始教你，這其實就跟線上課程有點像，都是系統性的教你如何學程式語言\n只不過看書的門檻會稍微高一點，因為程式語言這種東西有點繁瑣，書其實沒有辦法真的把每一步過程寫在書上，這樣太瑣碎了，所以書通常是講一些比較抽象的概念，然後就會貼一段程式碼，再搭配上一些註解，讓你自己去體會這一段程式碼為什麼要這樣寫\n而相對比較起來，因為線上課程是影片，所以你就可以看著老師一行一行的把程式打出來，然後在自己的電腦上練習，更容易讓自己產生大量練習的機會\n所以書和線上課程，我會偏向選線上課程，用影片的形式更能清楚的呈現程式語言的教學，至於內容上，我倒是覺得沒有太大的差異，兩個都滿棒的\n所以要來排序一下的話，作為入門程式語言的學習方法，我最推薦的學習方式是上線上課程，第二名是看書，最不推的是網路上的文章，至於資策會的程式班雖然效率高，不過你要承擔的離職風險也高，所以這個我就不多做評論\n從現在開始改變 # 最近有人問我，非本科生半路轉職學程式，會不會找不到工作？\n現實是：本科生確實就是比非本科生有優勢，但是有優勢不代表非本科生就沒有機會\n如果宏觀一點來看的話，在台灣，唸到頂的台大資工，也只是能夠在台灣軟體業/半導體業佔盡優勢而已，出了國，比起別人滿手 CMU、MIT 的學歷，台大真的相對來說是還好而已\n但是在美國的 FAANG 大廠，也還是會有台灣工程師在裡面上班\n他們在學歷上有優勢嗎？相對沒有\n但是他們為什麼能錄取？因為想進這間公司，所以就會拼了命的學習\n所以我想說的是，優勢和錄取，不是佔絕對的比重，只要展現足夠的實力，大家都還是有入場券的\n所以別想那麼多啦吼，唸就對了！！！\n履歷就給他投下去，多面試面試練練手感，會更了解自己在市場上的價值～\n","date":"2022-04-30","objectID":"bb82974f07af54cf95dd7d6ffdc9400e","title":"自學程式，如何高效率的學習？","url":"https://kucw.io/blog/2022/4/efficient-learning/"},{"categories":["Spring Boot"],"content":"如果在 application.properties 直接寫上中文，則 Spring Boot 在使用 @Value 讀取時會產生中文亂碼\nname=小明 解決辦法： # 在 application.properties 裡面，將「中文」轉成「Unicode」，即可解決此問題（可使用此網頁進行轉換）\nname=\\u5c0f\\u660e ","date":"2021-07-25","objectID":"8b6f7ef874076648d6e16585b28e307e","title":"SpringBoot - 解決 application.properties 中文亂碼","url":"https://kucw.io/blog/2021/7/spring-chinese-properties/"},{"categories":["Intellij"],"content":"IntelliJ 為 JetBrains 公司旗下的一個產品，因此若要使用 IntelliJ 的折扣碼，則需要同時也創建一個 JetBrains 帳號\n1. 如何使用折扣碼，取得免費的 IntelliJ 訂閱？ # 點擊此連結，進入到 JetBrains 專屬的折扣碼兌換網頁，並填寫以下資訊\n接著會收到一封信，點擊裡面的連結驗證信箱\n接著會跳轉到 JetBrains 官網，去註冊一個 JetBrains 帳號\n填寫完成之後，就可以發現在你的 JetBrains 帳號底下，新增了一筆 IntelliJ IDEA Ultimate 的訂閱，這樣折扣碼就成功的兌換完畢了～\n2. 如何在 IntelliJ 中綁定 JetBrains 帳號？ # 若尚未開啟過 IntelliJ，則可以在第一次啟動時，在 JB Account 的地方，直接填上剛剛註冊的 JetBrains 帳號密碼\n若已使用試用版開啟過 IntelliJ，則可以點選上方的 Help，然後選擇 Register，就可以開啟 IntelliJ 的 license 管理視窗\n接著點擊 Add New license，就可以在這邊填上剛剛註冊的 JetBrains 帳號密碼了\n","date":"2021-04-25","objectID":"6ebeb54c5397f883d2c567a63dab9529","title":"IntelliJ - 折扣碼使用教學","url":"https://kucw.io/blog/2021/4/intellij-coupon/"},{"categories":["Java"],"content":"在了解 Java 8 新增的時間系列之前，我們需要先了解時間相關的知識\n1. 首先是要有時區的概念 # 在台灣的時區是 GMT+8，而在英國的時區為 GMT+0，所以在同一瞬間，在英國看見的時間是 2020/06/29 14:00:00，但在台灣看見的時間卻會是 2020/06/29 22:00:00\n2. 要有 Timestamp 時間戳的概念 # 在電腦的世界裡，有一個東西叫做 Timestamp（時間戳），這個 Timestamp 是一個整數，代表著從 UTC 1970 年 1 月 1 日 0 時 0 分 0 秒 起至現在的總秒數，UTC 代表的是英國格林威治時間，也就是 GMT+0（可以簡單的認為 UTC = GMT+0）\n所以說假設現在你人在英國格林威治，然後你現在看手錶上的時間是 2020/06/29 14:00:00，則這個當下的 Timestamp的值就為 1593439200（代表從 1970/1/1 00:00:00 到現在經過了 1593439200 秒），然後假設同一個瞬間你的朋友在台灣，因為台灣的時區為 GMT+8，所以你朋友在台灣看手錶的時間會是 2020/06/29 22:00:00，但是你朋友當下的 Timestamp 也會是 1593439200\n也就是說，雖然你和你朋友手錶上呈現的時間不一樣，但是你們兩個的 Timestamp 會是一模一樣的，因為 Timestamp 是唯一表示著 UTC 1970/1/1 00:00:00 起至現在的總秒數\n而在Java中，可以使用 System.currentTimeMillis()，取得當前的 Timestamp\n為什麼 Java 中舊版的 Date 不好 ? # 因為 Date 硬是把 時區 和 Timestamp 這兩個概念混合在一起了，這就是為什麼 Date 不好的關係\n通常我們 new 一個 Date object時，都是使用 Date date = new Date()，而這個 Constructor 其實是會去取得當前的 Timestamp，然後把 Timestamp 存放在 Date 內部的 fastTime 變量裡，所以 Date 內部存放的，實際上是 Timestamp 的值\npublic class Date { private transient long fastTime; //存放timestamp的值 public Date() { this(System.currentTimeMillis()); } public Date(long date) { fastTime = date; } } 雖然 Date 內部只存放了 Timestamp 的值，但是 Date 卻在 toString() 時，會去取得當前程式運行的時區，然後再把 Timestamp 換算成當地時區的時間印出來\npublic class MyTest { public static void main(String[] args) { // Date 內部明明是只存放 Timestamp 的值 1593439200，也就是 UTC 2020/06/29 14:00:00 // (因為 Date 的構造方法要輸入 ms 毫秒，所以要把 Timestamp 乘以 1000) Date date = new Date(1593439200L * 1000); // 但是卻會在 toString() 方法裡面再去取得當前程式運行的時區(我的時區是GMT+8) // 然後把 Timestamp 的值轉換成該時區的時間 // 所以這裡會輸出 Mon Jun 29 22:00:00 CST 2020 System.out.println(date); } } 所以 Java 為了改善 Date 混合 時區 和 Timestamp 的問題，在 Java 8 時提出了新的時間系列類，將 Timestamp 和時間分成兩組\nTimestamp組 : Instant 時間組 : LocalDate、LocalTime、LocalDateTime、ZonedDateTime Instant 用法 # 在 Java 8 新增的這一堆時間系列裡面，每個類都有工廠方法可以使用，可以直接用這些工廠方法來產生出一個新的 object\n所以我們就可以直接使用 Instant.of() 去創建一個 instant object\npublic class MyTest { public static void main(String[] args) { // 用當下的 Timestamp 來創建in1 Instant in1 = Instant.now(); // 自定義 Timestamp Instant in2 = Instant.ofEpochSecond(1593439200L); } } LocalDate、LocalTime、LocalDateTime 用法 # LocalDate : 只存日期 (2020/06/29)\n和Date不同，LocalDate內部用了三個Integer變量 year、month、day，分別存放日期的值，徹底將時間和Timestamp的概念區分開來 LocalTime : 只存時間 (14:00:00)\n也是用了三個Integer變量 hour、minutes、second來分別存放時間的值 LocalDateTime : 存了日期和時間 (2020/06/29 14:00:00)\nLocalDateTime內部其實用一個LocalDate變量存放日期，用另一個LocalTime變量存放時間，reuse的極致！ Local 這一系列的時間類型，都不帶有時區的資訊，所以說當你new一個LocalDateTime出來之後，不管你是在台灣運行程式還是在英國運行程式，輸出的值都一樣\npublic class MyTest { public static void main(String[] args) { // LocalDate LocalDate date = LocalDate.of(2020, 6, 29); // 創建一個 2020/06/29的LocalDate LocalDate date2 = LocalDate.parse(\u0026#34;2020-06-29\u0026#34;); // parse 字串成 LocalDate // LocalTime LocalTime time = LocalTime.of(14, 0, 0); // 創建一個 14:00:00 的 LocalTime LocalTime time2 = LocalTime.parse(\u0026#34;14:00:00\u0026#34;); // parse 字串成 LocalTime // LocalDateTime // 創建一個 2020/06/29 14:00:00 的 LocalDateTime LocalDateTime dateTime = LocalDateTime.of(2020, 6, 29, 14, 0, 0); // 或是也可以用 LocalDate 和 LocalTime 組合創建 LocalDateTime dateTime2 = LocalDateTime.of(date, time); // Local 系列可以和同樣也是 Java8 新增的 DateTimeFormatter 一起使用，讓時間 input/output 的更格式化 // Java8 新增的 DateTimeFormatter 和 joda-time 中的","date":"2020-06-30","objectID":"0b355dce319bfa272450b922780f0ea7","title":"Java - Java 8 新增的時間系列用法","url":"https://kucw.io/blog/2020/6/java-date/"},{"categories":["Spring Boot"],"content":"ObjectMapper 是一款非常好用的 json 轉換工具，可以幫助我們完成 json 和 Java 的 Object 的互相轉換\n什麼是 Serialize 和 Deserialize？ # Serialize : 將 Java Object 轉換成 json Deserialize : 將 json 轉換成 Java Object 在 Spring Boot 裡使用 ObjectMapper # ObjectMapper 是由 Jackson library 所提供的一個功能，所以只要在 maven 中加入 spring-boot-starter-web 的 dependency 就可以了\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; Json 和 Java Object、List、Map 的互轉 # 先定義一個 User class\npublic class User { private int id; private String name; // 省略constructor, getter, setter } 使用 ObjectMapper 完成 json 和 Java Object、List、Map 之間的互轉\nimport java.util.*; import com.fasterxml.jackson.core.type.TypeReference; import com.fasterxml.jackson.databind.ObjectMapper; public class MainTest { public static void main(String[] args) throws Exception { ObjectMapper objectMapper = new ObjectMapper(); // User Object 轉 json User user1 = new User(123, \u0026#34;John\u0026#34;); String json = objectMapper.writeValueAsString(user1); // json 轉 User Object User user2 = objectMapper.readValue(json, User.class); // List\u0026lt;User\u0026gt; 轉 json List\u0026lt;User\u0026gt; ulist = new ArrayList\u0026lt;\u0026gt;(); User user4 = new User(123, \u0026#34;John\u0026#34;); ulist.add(user4); String ujson = objectMapper.writeValueAsString(ulist); // json 轉 List\u0026lt;User\u0026gt; List\u0026lt;User\u0026gt; urlist = objectMapper.readValue(ujson, new TypeReference\u0026lt;List\u0026lt;User\u0026gt;\u0026gt;() {}); // Map\u0026lt;String, User\u0026gt; 轉 json HashMap\u0026lt;String, User\u0026gt; umap = new HashMap\u0026lt;\u0026gt;(); User user3 = new User(123, \u0026#34;John\u0026#34;); umap.put(\u0026#34;John\u0026#34;, user3); String mjson2 = objectMapper.writeValueAsString(umap); // json 轉 Map\u0026lt;String, User\u0026gt; Map\u0026lt;String, User\u0026gt; urMap = objectMapper.readValue(mjson2, new TypeReference\u0026lt;HashMap\u0026lt;String, User\u0026gt;\u0026gt;() {}); } } 如果想了解更多 Spring Boot 的用法，也歡迎參考我開設的線上課程 Java 工程師必備！Spring Boot 零基礎入門 （輸入折扣碼「HH202506KU」即可享 85 折優惠）。\n","date":"2020-06-12","objectID":"e2fcd66ccc91620642cafbcdbc233365","title":"SpringBoot - 使用 ObjectMapper 完成 json 和 Java Object 互相轉換","url":"https://kucw.io/blog/2020/6/java-jackson/"},{"categories":["Java"],"content":"Lombok 是一個 Java library，可以透過簡單的寫法自動生成 Java 的 code，像是 setter、getter、logger\u0026hellip;等，目的在消除冗長的 code 和提高開發效率\n例如當你在 Class 上加上了一個 @Getter 和 @Setter 之後，Lombok 就會自動幫你產生 getter 和 setter 出來，所以你就不用再自己去寫這些煩人的程式啦！\n之所以加個 @Getter 就可以幫我們自動生成所有變數的 getter，是因為 Lombok 參與了 Java 在 compile 階段生成 .class 檔的過程，Lombok 會幫我們自動寫一堆 getter，然後塞進 .class 檔，所以真正被編譯出來的 User.class 檔案，是包含完整的 getter 的\n所以簡單的說，Lombok 可以算是一種語法糖，只是在幫我們增進開發效率而已，實際上所產生出來的.class檔仍然是完全正常的！\n安裝 Lombok # 要在 project 中使用 Lombok，除了要在 maven 中加入 Lombok dependency，還要安裝 Intellij 的 Lombok 插件\n1. 加入 maven dependency # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.18\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 2. 在 Intellij 中安裝 Lombok 插件 # 補充：我使用的 Intellij 版本是 2019.3.3，可能會因為版本差異導致安裝方式有改變\n先點選左上角 Intellij IDEA -\u0026gt; Preferences\n然後點擊左邊的 Plugins，再點擊上面的 Marketplace tab，然後就可以在搜尋欄中輸入lombok，並且找到 Lombok 插件並安裝它\n補充：為什麼需要特地安裝 Intellij 的 Lombok 插件？ # 其實在 maven 加入 Lombok dependency 之後，使用 mvn clean package 就可以正常 build 過，在 Intellij 中點擊綠色按鈕也可以運行程式\n之所以還要特地安裝 Intellij 的 Lombok 插件，是因為如果不安裝 Lombok 插件的話，Intellij 就會沒辦法自動提示出 Lombok 產生的方法，所以就會發生你的 code 一片紅，但是運行卻可以通過的奇妙現象\n像是下面這段 code 中，因為對 Intellij 來說，code 裡並不存在 setter，所以沒辦法自動提示 setId()、setName() 等方法，但是又因為我們在 maven 中有加入 Lombok dependency，所以點擊第 13 行的綠色箭頭運行程式的話，是可以正常運行成功的\n所以 Lombok 算是侵入性很高的一個 library，只要團隊中有一個人用 Lombok 開發，那麼所有的人都必須得安裝 Lombok 插件才行，這樣才不會發生在 Intellij 中一打開 project 時，整片都是痛苦的紅字\nLombok 用法介紹 # Lombok 官網提供了許多好用的註解，但是「勁酒雖好，可不能貪杯」，你用了越多 Lombok 的進階用法，會讓整個團隊的學習曲線上升，反而會造成反效果，所以在此處只解釋最常見、並且最重要的註解使用方式，其他較少見的用法就不會涵蓋在此文章中\n此篇文章會介紹的 Lombok 方法為：\n@Getter/@Setter @ToString @EqualsAndHashCode @NoArgsConstructor、@AllArgsConstructor、@RequiredArgsConstructor @Data（最常用） @Value @Builder（常用） @Slf4j 1. @Getter/@Setter # 自動產生 getter/setter\n2. @ToString # 自動 override toString() 方法，會輸出所有變數的值\n3. @EqualsAndHashCode # 自動生成 equals(Object other) 和 hashcode() 方法，包括所有的「非靜態變數」和「非 transient 變數」\n如果某些變數不想要加進判斷，可以透過 exclude 排除（或是也可以反過來，使用 of 指定只要輸出某些字段）\n補充： 為什麼只有一個合在一起的註解 @EqualsAndHashCode，而不是兩個分開的註解 @Equals 和 @HashCode？\n因為在 Java 中有規定，當兩個 object equals 時，他們的 hashcode 一定要相同，反之，當 hashcode 相同時，object 不一定 equals。所以 equals 和 hashcode 要一起 implement，免得發生違反 Java 規定的情形發生\n4. @NoArgsConstructor、@AllArgsConstructor、@RequiredArgsConstructor # 這三個很像，都是在自動生成該 Class 的 constructor，差別只是在「生成的 constructor 的參數不一樣」而已\n@NoArgsConstructor : 生成一個沒有參數的 constructor\n@AllArgsConstructor : 生成一個包含所有參數的 constructor\n補充： 這裡要注意一個 Java 中的實作重點，當我們沒有指定 constructor 時，Java compiler 會幫我們自動生成一個沒有任何參數的 constructor 給該 Class，但是如果我們自己寫了 constructor 之後，Java 就不會自動幫我們補上那個無參數的 constructor 了！\n然而很多地方（像是 Spring Data JPA），都會要求每個 Class 一定要有一個無參數的 constructor，所以你在加上 @AllArgsConstructor 時，一定要記得補上 @NoArgsConstrcutor，不然就會出現各種意外的報錯，一定要記得！！！\n@AllArgsConstructor @NoArgsConstructor public class User { private Integer id; private String name; } @RequiredArgsConstructor : 生成一個包含「特定參數」的 constructor，特定參數指的是「那些有加上 final 修飾詞的變數們」\n補充一下，如果所有的變數都是正常的（即是都沒有用 final 修飾的話），那就會生成一個沒有任何參數的 constructor\n5. @Data # 懶人包，只要加了 @Data，等於同時加了以下註解：\n@Getter/@Setter @ToString @EqualsAndHashCode @RequiredArgsConstructor @Data 是使用頻率最高的 Lombok 用法，通常 @Data 會加在一個值可以被更新的 Object 上，像是日常使用的 DTO 們、或是 JPA 裡的 Entity 們，就很適合加上 @Data，也就是 @Data for mutabl","date":"2020-03-04","objectID":"4f394bcf6c466343c47b256aab455986","title":"Java - 五分鐘學會 Lombok 用法","url":"https://kucw.io/blog/2020/3/java-lombok/"},{"categories":["Spring Boot"],"content":"Mockito就是一種 Java mock 框架，他主要是用來做 mock 測試的，他可以模擬任何 Spring 管理的 bean、模擬方法的返回值、模擬拋出異常\u0026hellip;等，在了解 Mockito 的具體用法之前，得先了解什麼是 mock 測試\n1. 什麼是 mock 測試？ # mock 測試就是在測試過程中，創建一個假的對象，避免你為了測試一個方法，卻要自行構建整個 bean 的 dependency chain\n像是以下這張圖，類 A 需要調用類 B 和類 C，而類 B 和類 C 又需要調用其他類如 D、E、F 等，假設類 D 是一個外部服務，那就會很難測，因為你的返回結果會直接的受外部服務影響，導致你的單元測試可能今天會過、但明天就過不了了\n而當我們引入 mock 測試時，就可以創建一個假的對象，替換掉真實的 bean B 和 C，這樣在調用B、C的方法時，實際上就會去調用這個假的 mock 對象的方法，而我們就可以自己設定這個 mock 對象的參數和期望結果，讓我們可以專注在測試當前的類 A，而不會受到其他的外部服務影響，這樣測試效率就能提高很多\n2. Mockito 簡介 # 說完了 mock 測試的概念，接下來我們進入到今天的主題，Mockito\nMockito 是一種 Java mock 框架，他主要就是用來做 mock 測試的，他可以模擬任何 Spring 管理的 bean、模擬方法的返回值、模擬拋出異常\u0026hellip;等，他同時也會記錄調用這些模擬方法的參數、調用順序，從而可以校驗出這個 mock 對象是否有被正確的順序調用，以及按照期望的參數被調用\n像是 Mockito 可以在單元測試中模擬一個 service 返回的數據，而不會真正去調用該 service，這就是上面提到的 mock 測試精神，也就是通過模擬一個假的 service 對象，來快速的測試當前我想要測試的類\n目前在 Java 中主流的 mock 測試工具有 Mockito、JMock、EasyMock..等，而 SpringBoot 目前內建的是 Mockito 框架\n題外話說一下，Mockito 是命名自一種調酒莫吉托（Mojito），外國人也愛玩諧音梗。。。\n3. 在 SpringBoot 單元測試中使用 Mockito # 首先在 pom.xml 下新增 spring-boot-starter-test 依賴，該依賴內就有包含了 JUnit、Mockito\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 先寫好一個 UserService，他裡面有兩個方法 getUserById() 和 insertUser()，而他們會分別去再去調用 UserDao 這個 bean的 getUserById() 和 insertUser() 方法\n@Component public class UserService { @Autowired private UserDao userDao; public User getUserById(Integer id) { return userDao.getUserById(id); } public Integer insertUser(User user) { return userDao.insertUser(user); } } User model 的定義如下\npublic class User { private Integer id; private String name; //省略 getter/setter } 如果這時候我們先不使用 Mockito 模擬一個假的 userDao bean，而是真的去 call 一個正常的 Spring bean 的 userDao 的話，測試類寫法如下。其實就是很普通的注入 userService bean，然後去調用他的方法，而他會再去 call userDao 取得 DB 的 data，然後我們再對返回結果做 assert 檢查\n@RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { //先普通的注入一個userService bean @Autowired private UserService userService; @Test public void getUserById() throws Exception { //普通的使用userService，他裡面會再去call userDao取得DB的data User user = userService.getUserById(1); //檢查結果 Assert.assertNotNull(user); Assert.assertEquals(user.getId(), new Integer(1)); Assert.assertEquals(user.getName(), \u0026#34;John\u0026#34;); } } 但是如果 userDao 還沒寫好，又想先測 userService 的話，就需要使用 Mockito 去模擬一個假的 userDao 出來\n使用方法是在 userDao 上加上一個 @MockBean 注解，當 userDao 被加上這個注解之後，表示 Mockito 會幫我們創建一個假的 mock 對象，替換掉 Spring 中已存在的那個真實的 userDao bean，也就是說，注入進 userService 的 userDao bean，已經被我們替換成假的 mock 對象了，所以當我們再次調用 userService 的方法時，會去調用的實際上是 mock userDao bean 的方法，而不是真實的 userDao bean\n當我們創建了一個假的 userDao 後，我們需要為這個 mock userDao 自定義方法的返回值，這裡有一個公式用法，下面這段 code 的意思為，當調用了某個 mock 對象的方法時，就回傳我們想要的自定義結果\nMockito.when( 對象.方法名() ).thenReturn( 自定義結果 ) 使用 Mockito 模擬 bean 的單元測試具體實例如下\n@RunWith(SpringRunner.class) @SpringBootTest public class UserServiceTest { @Autowired private UserService userService; @MockBean private UserDao userDao; @Test public void getUserById() throws Exception { // 定義當調用mock userDao的getUserById()方法，並且參數為3時，就返回id為200、name為I\u0026#39;m mock3的user對象 Mockito.when(userDao.getUserById(3)).thenReturn(new User(200, \u0026#34;I\u0026#39;m mock 3\u0026#34;)); // 返回的會是名字為I\u0026#39;m mock 3的us","date":"2020-02-20","objectID":"53cbcd3e355d6042f045630039ff2319","title":"SpringBoot - 單元測試工具 Mockito","url":"https://kucw.io/blog/2020/2/spring-unit-test-mockito/"},{"categories":["Intellij"],"content":" 1. 什麼是 Debug 模式？ # 還記得以前不會使用 IntelliJ 的 debug 功能時，想要看某個 Variable 的值，都是在那行 code 的下面一行加上 System.out.println()，然後運行程式，把值 print 出來，如果要看另一個 Variable，我就再加一行 System.out.println()，所以到後來我的 code 就會長的像下圖這樣\npublic User getMaleUser() { List\u0026lt;User\u0026gt; userList = userDao.getUserList(); System.out.println(userList); // print userList，看一下userList裡面的內容長怎樣 // 從userList中取出男生，然後回傳 User resultUser; for (User user : userList) { if (user.getGender() == \u0026#34;男\u0026#34;) { resultUser = user; } } System.out.println(resultUser); // 再print resultUser，確認一下回傳的user到底是哪一個 return resultUser; } 可想而知，這樣做的開發效率是非常差的，每多看一個 variable 就要多增加一行 System.out.println()，而且每次改了之後，都要重新運行程式，讓程式再 print 出一次數據，我想想都覺得痛苦，難道 IntelliJ 就沒有一個能夠快速反應出現在這個 variable 的值是什麼的功能嗎？\n事實上，IntelliJ 是有提供的！當我們在運行程式時，改成使用 Debug 模式 運行就可以了！\n2. IntelliJ 的 Debug 模式 # 使用 IntelliJ 的 Debug 模式來運行程式的好處\n可以一行行的運行 code 可以馬上知道運行中的 variable 的值，甚至去改變它的值，從此不用再使用System.out.println()去 print variable 了 在 IntelliJ 的 Debug 模式中，有幾個比較重要的功能面板如下\n開啟 Debug 模式的開關，通常在寫 code 時都會改用 Debug 模式來跑程式，已經很少使用旁邊的 Run 模式了 設置 break point，在左邊行號欄單擊左鍵就可以設置 break point，再點一下則可以取消此 break point，能夠設置 break point 是 Debug 模式和 Run 模式最大的區別，而設置 break point 可以幫助我們一行行的運行 code，這也是為什麼推薦使用 Debug 模式而不是使用 Run 模式 Debug 按鈕，debug 的主要功能就對應著這幾個按鈕 Service 按紐，和 break point 有關係，主要搭配著 3. Debug 按鈕 一起使用 Variables，可以查看當前 break point 之前的 variable 的值 3. Debug 基本用法 # IntelliJ 的 Debug 模式基本用法主要對應著上述 3 和 4 的兩組按鈕\nDebug 按鈕，從左到右共 8 個按鈕 # Show Execution Point : 如果你在看其它行或其它頁面，點擊這個按鈕可跳轉到當前代碼執行的地方 Step Over : 一行一行的往下執行 code，如果這一行裡面有 call 其他方法的話，不會進入那些方法 Step Into : 如果當前行有 call 其他方法，可以進入該方法內部，一般用於進入自定義方法內，不會進入 library 的方法 Force Step Into : 強制進入方法內部，能進入任何方法，查看底層 source code 的時候可以用這個進入 library 的方法 Step Out : 退出方法，從進入的方法內退出到方法調用處，此時方法已執行完畢，只是還沒有完成賦值 Drop Frame : 回退 break point，很少用到 Run to Cursor : 運行到目前滑鼠點擊的位置，你可以將滑鼠點擊到你需要查看的那一行，然後使用這個功能，code 就會運行至那一行，而不需要在那一行上打 break point（前提是已經進入了 Debug 模式，就是已經停在某個 break point 上了） Evaluate Expression : 計算表達式，後面第五部分詳細說明 Service 按鈕，從上到下共 7 個按鈕 # Rerun : 重新運行程式，他會關閉程式後再重新啓動一次，不過很少用到，通常都會直接關掉，再手動開啟一次 Update \u0026rsquo;tech\u0026rsquo; application : 更新程式，就是執行當初定義的 update 選項，當 Debug 模式啟動後，再次點擊 debug 按鈕也會跳出此選項 Resume Program : 繼續執行程序，例如在第 20 行和 25 行有兩個 break point，而當前運行至第 20 行，按一下，則運行到下一個 break point（即第 25 行），再按一下，則運行完整個流程，因爲後面已經沒有 break point 了 Pause Program : 暫停程式，很少用，不是很重要 Stop : 連續按兩下關閉程式，有時候你會發現關閉程式再啓動時，說 port 被佔用，這是因爲沒完全關閉程式的原因 View Breakpoints : 查看所有 break point，後面第七部分詳細說明 Mute Breakpoints : 將所有 break point 變爲灰色並使它們失效，按 Resume Program 那個鍵（也就是第三個按鍵）可以直接運行完程式（因為所有打的 break point 都被設置為無效了），再次點擊這個 Mute Breakpoints 按鍵可以使所有無效 break point 變爲紅色有效 4. Debug 模式下的 variable 查看 # 在 Debug 過程中，跟蹤查看 variable 的值是非常必要的，有幾個方式可以查看當前 variable 的值\n參數所在行後面會顯示當前 variable 的值 滑鼠停到參數上，會顯示當前 variable 的資訊，點擊打開可以看到該 variable 的詳情 在下方的 Variables 窗口查看，這裡會顯示當前方法的所有 variable 的詳情 5. 計算表達式 Evaluate Expression # 在前面第三部分有提到一個計算表達式 Evaluation Expression 按鈕，可以使用這個按鈕在 debug 過程中計算某個表達式的值，或是直接改變某個 variable 的值，而不用再去重新改 code 然後再重啟程式\n假設在 Debug 模式下，想要快速比較當前 list.get(0).equals(\u0026quot;first\u0026quot;) 的結果，可以不用改 code，直接在計算表達式裡面運算，讓 IntelliJ 快速幫助我們計算出這個函式會回傳的值，非常方便，像是下面的 3. Result 部分，得到的就是使用計算表達式運行 list.get(0).equals(\u0026quot;first\u0026quot;) 的結果\n或是說想要更改 variable 的值的話，也可以透過計算表達式來改變，像是下面這個例子，對 code 來說，list 裡只會有 first、second 兩個字串，但是因為我們在計算表達式裡使用了 li","date":"2020-02-15","objectID":"dbbfa835138e137d396db1f7f0eeef2a","title":"IntelliJ - Debug 的使用方法","url":"https://kucw.io/blog/2020/2/intellij-how-to-debug/"},{"categories":["Intellij"],"content":"如果你想要 debug 某個 run 在 server 上的 SpringBoot 或是 Spring project 時，必須先配置好 remote debug，才能夠在本地打 break point，然後透過 remote debug 傳到 server 上，去對遠端 server 上的 project debug\n首先先運行起來在 server 上的 project # 如果是 SpringBoot project，需要在執行 build 出來的 jar 檔時，帶上 jvm 啟動參數\njava -agentlib:jdwp=transport=dt_socket,address=18090,server=y,suspend=n -jar myservice-0.0.1-SNAPSHOT.jar 如果是傳統的 Spring + tomcat war 檔 project，則是在 tomcat/bin/catalina.sh裡，加入 JAVA_OPTS 設定 jvm 啟動參數\n#!/bin/sh JAVA_OPTS=\u0026#34;-agentlib:jdwp=transport=dt_socket,address=18090,server=y,suspend=n\u0026#34; 接著在自己的電腦上開啟 tunnel # 如果自己的電腦是 Windows # 先下載 putty，下載完成之後打開他，然後點選 Tunnels\n在 Source port 填上本機的 port，這裡填 1993，但你可以挑一個自己喜歡的 port\n在 Destination上 填上 server ip 和 18090，其中 18090 要跟你剛剛在 server 上運行的參數 address 的值一樣\n填完之後按 Add，上面 Forwarded ports 就會出現你的設定值\n接著按左邊的 session 回到主頁面，在 Host Name 填上 server 的 ip\n最後再按右下角 Open 連線，就可以在 Windows 上開啟 tunnel 了\n如果自己的電腦是 Mac/Linux # Mac/Linux 開啟 tunnel 的方式比較簡單，只要運行以下指令就可以了\nssh -X -N -L 1993:your-server-ip:18090 your-server-ip 其中 18090 要跟你剛剛在 server 上運行的參數 address 的值一樣，而那個 1993 則是本機的 port，你挑一個自己喜歡的就可以了\n設定 IntelliJ # 首先先在 IntelliJ 上新增一個 Remote configuration\n在 host 的地方填入 localhost，而 port 的地方填入你剛剛開的那個本機 port，我剛剛在本機開的是 1993 port，所以我這裡就填 1993，填好按 OK 保存\n接著就可以運行剛剛設置好的 remote configuration 來進行 remote debug 了！\n如果連線有成功，IntelliJ 下方會顯示 Connected to the target VM...，這時候就可以打 break point 來對 server 上的 project debug 了\n","date":"2020-01-15","objectID":"e3643fcd360c677566425decfd24f191","title":"IntelliJ - Remote Debug","url":"https://kucw.io/blog/2020/1/intellij-remote-debug/"},{"categories":["Spring Boot"],"content":" 本文只講解 OAuth 2.0 的實作，有關 OAuth 1.0a 實作，可參考我的另一篇文章 使用 SpringBoot 實作 OAuth 1.0a 綁定 Twitter\n如果你想要在你的網站上取得某位 user 在 Github 上的資料，那麼就要使用 OAuth 2.0 將你的網站和 Github 做綁定，本文介紹如何使用 Spring Boot 來實現 Github OAuth 2.0 綁定\n首先，先進到 Github 的個人設定的 settings 頁面，點擊 Developer settings\n點擊 OAuth Apps，創建一個 OAuth App，這一步就是你的網站和 Github 的協商\n填入相關資料\n當 user 在 Github 按下授權按鈕之後，Github 會將 user 302 導到你指定的 Authorization callback URL，在此處就是 http://localhost:8080/callback，然後順便把 code 也傳給你 點擊創建之後，就會看到 Github 發給你的 OAuth 2.0 用的 Client Id 和 Client Secret 了\n接下來就是實現和 Github 綁定的 SpringBoot code，demo code 放在這裡\nGithub OAuth2 官方文件 : https://developer.github.com/apps/building-oauth-apps/authorizing-oauth-apps/\n假設你已經在 Github 上按下授權按鈕，然後你想要解綁的話，進到 Github 的 setting 頁面，點擊 Application，就可以看到剛剛綁的 oauth-test，在此處就可以刪掉你剛剛綁定的 token 了\n","date":"2019-12-31","objectID":"16b98e019ff016901add0131734d9875","title":"使用 SpringBoot 實作 OAuth 2.0 綁定 Github","url":"https://kucw.io/blog/2019/12/spring-oauth2-bind-github/"},{"categories":["Spring Boot"],"content":" 本文只講解 OAuth 1.0a 的實作，有關 OAuth 2.0 實作，可參考我的另一篇文章 使用 SpringBoot 實作 OAuth 2.0 綁定 Github\n如果你想要在你的網站上取得某位 user 在 Twitter 上的資料，那麼就要使用 OAuth 1.0a 將你的網站和 Twitter 做綁定，本文將介紹如何使用 Spring Boot 來實現 Twitter OAuth 1.0a 綁定\n首先先進到 Twitter developer 網站，將你的 Twitter 帳號申請為 developer 帳號，申請通過之後會收到 Twitter 寄來的 email，點擊 confirm 按鈕會進入到 Twitter developer dashboard get-started\n點擊 Create an app 創建一個 oauth app，這一步是你的網站和 Twitter 的協商\n填入相關資料，需要填寫 app name、Application description、website url、callback Url\n注意此處的 callback Url 要和到時後申請 request token 帶的 callback Url 一致，不然會被 Twitter 擋下來 另外也要注意他的 website url 強制需要 https 和 有域名 兩個條件，所以這時候就要使用 ngrok 來幫我們轉發 在這裡吐嘈 Twitter 一下，為什麼不管是申請個 Twitter 開發者帳號還是申請 Oauth app，都要寫一堆理由..\n填完相關資料點擊 create 後，就成功在 Twiiter 創建 Oauth app了\n點擊 Keys and tokens tab，就可以看到 Twitter 發給我們的 consumer key 和 secret 了\nTwitter 還算貼心，可以直接在開發者頁面一鍵新增 access token，幫你省去綁定的功夫，讓你可以快速使用這個 access token 去 call 他的 api 當然這個 access token 只會提供你自己的 Twitter 帳號的 access token，如果要實現所有人都可以綁定的話，還是需要自己寫 oauth 1.0a 綁定 code，這個 access token 只是讓你能夠快速測試而已 接下來就是實現和 Twitter 綁定的 SpringBoot code，demo code 放在這裡\nTwitter OAuth1.0a 官方文件 : https://developer.twitter.com/en/docs/basics/authentication/oauth-1-0a/obtaining-user-access-tokens\n","date":"2019-12-30","objectID":"3f1ce910313303c57791cf287f37cd9a","title":"使用 SpringBoot 實作 OAuth 1.0a 綁定 Twitter","url":"https://kucw.io/blog/2019/12/spring-oauth1a-bind-twitter/"},{"categories":["Other tech"],"content":"安裝環境 : CentOS Linux release 7.7.1908 (Core)\nsudo yum install python36-pip python3-devel gcc-c++ sudo pip3 install apache-superset superset db upgrade flask fab create-admin superset load_examples superset run -p 8088 ","date":"2019-12-20","objectID":"97299365eb45a8e5c7368181e33045af","title":"Superset 安裝教學","url":"https://kucw.io/blog/2019/12/superset-install/"},{"categories":["Elastic Search"],"content":"在 ElasticSearch 中，提供了兩種用來表示地理位置的方式，分別是 geo_point 和 geo_shape\n用緯度、經度表示的座標點使用 geo_point 字段類型(較常用)，geo_point 允許你找到距離另一個座標點一定範圍內的座標點、計算出兩點之間的距離來排序或進行相關性打分、或者聚合到顯示在地圖上的一個網格 另一種是使用 GeoJSON 格式定義的複雜地理形狀，使用geo_shape字段類型 geo_point 經緯度座標格式 # 在 mapping 定義時將字段類型定義為 geo_point\nPUT mytest { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34; }, \u0026#34;my_location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; } } } } 目前有6種經緯度座標格式可以插入一個座標點到 Elasticsearch中，此處只介紹幾種常用的，全部支持的格式請參考 官方文檔\n使用對象\nPOST mytest/_doc { \u0026#34;name\u0026#34;: \u0026#34;Pala Pizza as an object\u0026#34;, \u0026#34;my_location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.722, \u0026#34;lon\u0026#34;: -73.989 } } 使用數組 location : [ lon, lat ]\nPOST mytest/_doc { \u0026#34;name\u0026#34;: \u0026#34;Pala Pizza as an array\u0026#34;, \u0026#34;my_location\u0026#34;: [-73.989, 40.722] } 使用字符串 location : \u0026ldquo;lat, lon\u0026rdquo;\nPOST mytest/_doc { \u0026#34;name\u0026#34;: \u0026#34;Pala Pizza as a string\u0026#34;, \u0026#34;my_location\u0026#34;: \u0026#34;40.722, -73.989\u0026#34; } 使用Geohash\nPOST mytest/_doc { \u0026#34;name\u0026#34;: \u0026#34;Pala Pizza as a geohash\u0026#34;, \u0026#34;my_location\u0026#34;: \u0026#34;drm3btev3e86\u0026#34; } 通過地理座標點進行過濾 # 目前針對 geo_point 的查詢，ElasticSearch 提供 3 種地理座標點相關的過濾器，可以用來選中或者排除文檔\ngeo_bounding_box (矩形過濾) : 找出落在指定矩形框中的點，效率最高 geo_distance (圓形過濾) : 找出與指定位置在給定距離內的點 geo_polygon (自定義多邊型過濾) : 找出落在指定多邊形中的點 (注意這個過濾器使用代價很大) geo_bounding_box 矩形過濾 # 指定一個矩形的頂部、底部、左邊界、右邊界，然後過濾器只需判斷座標的經度是否在左右邊界之間，緯度是否在上下邊界之間就可以判斷這個座標點是否在矩形範圍內\n可以選擇使用 top_left + bottom_right 或是 top_right + bottom_left 或是 top + bottom + left + right\n具體實例 : 找出那些位在 lat 40.7~40.8，且lon -73 ~ -74的座標點 # GET mytest/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_bounding_box\u0026#34;: { \u0026#34;my_location\u0026#34;: { \u0026#34;top_left\u0026#34;: { \u0026#34;lat\u0026#34;: 40.8, \u0026#34;lon\u0026#34;: -74.0 }, \u0026#34;bottom_right\u0026#34;: { \u0026#34;lat\u0026#34;: 40.7, \u0026#34;lon\u0026#34;: -73.0 } } } } } } } geo_distance 圓形過濾 # 從給定的位置爲圓心畫一個圓，找出那些地理座標落在其中的文檔，distance支持的單位 : km、m、cm、mm\u0026hellip;\n圓形過濾計算代價比矩形過濾貴，爲了優化性能，ES 會先畫一個矩形框來圍住整個圓形，這樣就可以用矩形過濾先排除掉一些完全不可能的文檔，然後再對落在矩形內的座標點用地理距離計算方式處理\n在使用圓形過濾時，需要思考是否真的要這麼精確的距離過濾？通常使用矩形模型 bounding box是更更高效的方式，並且往往也能滿足應用需求，假設user想找1km內的餐廳，是否不行找了一個1.2km的餐廳給他？這200m真的差距有這麼大嗎？需要根據實際使用情況做選擇\n為了提高圓形過濾的性能，在計算兩點間的距離時，有多種犧牲性能換取精度的算法\narc : 最慢但最精確的計算方式 (默認的計算方式)，這種方式把世界當作球體來處理，不過這種方式的精度還是有極限，因爲這個世界並不是完全的球體 plane : plane的計算方式把地球當成是平坦的，這種方式快一些但是精度略遜，在赤道附近的位置精度最好，而靠近兩極則變差 具體實例 : 給定一個位置 (40.715, -73.988)，找出距離他1km以內的所有點 # GET mytest/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;geo_distance\u0026#34;: { \u0026#34;distance\u0026#34;: \u0026#34;1km\u0026#34;, \u0026#34;distance_type\u0026#34;: \u0026#34;arc\u0026#34;, \u0026#34;my_location\u0026#34;: { \u0026#34;lat\u0026#34;: 40.715, \u0026#34;lon\u0026#34;: -73.988 } } } } } } geo_polygon 自定義多邊型過濾 # geo_polygon 可以自定義一個多邊形，然後判斷這個座標點是否在該多邊形內\n在使用 geo_polygon 時，需要使用 points 數組自定義多邊形，需要把該多邊形的所有點列出來在此數組裡，順序不影響\n具體實例 : 找出那些位在指定多邊形內的座標點 # GET mytest/_search { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34; : { \u0026#34;filter\u0026#34; : { \u0026#34;geo_polygon\u0026#34; : { \u0026#34;my_location\u0026#34; : { \u0026#34;points\u0026#34; : [ {\u0026#34;lat\u0026#34; : 40, \u0026#34;lon\u0026#34; : -70}, {\u0026#34;lat\u0026#34; : 40, \u0026#34;lon\u0026#34; : -80}, {\u0026#34;lat\u0026#34; : 50, \u0026#34;lon\u0026#34; : -70}, {\u0026#34;lat\u0026#34; : 50, \u0026#34;lon\u0026#34; : -80} ] } } } } } } 按照距離大小進行排序 # 請求 # 注意，會需要在請求時決定距離的單位的原因是因為，這些用於排序的距離的值，會設置在每個返回的結果的sort元素中，方便我們取得他們實際上距離那個點","date":"2019-12-07","objectID":"585c17c0f875ad313182d1f3baa03609","title":"ElasticSearch - 地理座標點 geo_point","url":"https://kucw.io/blog/2019/12/elasticsearch-geo-point/"},{"categories":["Life"],"content":"最近突如其來的想說來減過肥吧，然後突然想到許多健身的人為了讓肌肉更突出，通常在一定時間的重訓後，就會開始為期三個月的減脂肪餐（就是很常見的雞肉餐）\n所以我突發奇想，要是減肥和減脂，他們的原理都是要減去人體內多餘的脂肪的話，那我走減脂那一套吃法，是不是會減的比較輕鬆又快樂？可以一直吃肉，然後又可以瘦下來，怎麼想都比蔬菜水果好減吧！\n詢問了我一個重訓多年的朋友之後，他給我的建議如下\n第一步：決定目標和手段 # 首先，你想減肥，具體有想要搭配重訓長肌肉嗎？還是只是想要減掉肥肉？\n要知道，減脂和減重是不同概念\n如果只是想要減掉肥肉，多做有氧（跑步游泳騎車之類的），就可以有能量消耗。然後不用特別吃肉，只要控制整體熱量 \u0026lt; 支出，兩個月就可以瘦個三五公斤\n減脂要吃肉是為了要維持肌肉量（搭配大量重量訓練），因為減重（包含減掉脂肪＋肌肉）的時候肌肉量一定會掉，但是減脂我們盡量只減脂肪，和少量肌肉（減脂還是會掉肌肉）\n所以如果你沒有想要搭配重量訓練（維持肌肉量），那你直接有熱量赤字就好了，不管是減脂還是減重，熱量赤字都是需要的\n第二步：如何達到熱量赤字？ # 什麼是熱量赤字？\n只要 每天的熱量消耗量 \u0026gt; 每天的熱量攝取量，就是熱量赤字 # 假設你每天消耗（運動＋能量消耗＋基礎代謝）2000大卡，那你就只能吃 1500 ~ 1700 大卡，才能達到熱量赤字\n通常每 7700 大卡熱量赤字會少一公斤，所以假如你一天少吃 400 大卡，你 7700 / 400 = 19.25 天，你就可以少一公斤。所以看看你要想少幾公斤，想要多少時間內減重，再去分配\n這也是為什麼要做有氧運動，就是為了要增加自己的 \u0026ldquo;每日消耗\u0026rdquo;\n前面說到 每日消耗 = 運動消耗＋消化食物消耗的熱量＋基礎代謝（呼吸之類的），所以如果你運動一天多500大卡消耗，你就一天多減500大卡\n可是運動會餓，所以可能又會想吃東西，不過減肥、減脂的人就是要控制自己不能吃太多\n然後要怎麼算自己一天吃 1500 大卡呢？就是要自己煮，或是找有營養標示的早、午、晚餐。所以有在重訓要減脂的人才會都自己煮，這樣才抓的到每日的吸收量是多少\n減脂的話要調配三大基礎營養素的比例：碳水化合物、脂肪、蛋白質。這三種的攝取量會依照你重訓的天數、強度、組數、每天或每月做不同變化\n結論 # 如果你只是想減肥，那就 \u0026ldquo;多做有氧＋每天熱量赤字\u0026rdquo; 就 ok 了\n如果你要減脂\u0026hellip;再跟我說\n","date":"2019-12-03","objectID":"ad134e3239e12813ace867d5f30bf277","title":"減肥和減脂有區別嗎？","url":"https://kucw.io/blog/2019/12/how-to-lose-weight/"},{"categories":["Java"],"content":"Stopwatch 是 Guava 提供，可以用來測量程式運行時間的工具（Guava 是 Google 開發的 Java library，是一個非常好用的工具包）\n常用方法 # Stopwatch.creatStarted() : 創建一個碼表，並且開始計時 stop() : 碼表停止計時 elapsed(TimeUnit unit) : 取得從開始到結束的時間 具體實例 # public class Main { public static void main(String[] args) { Stopwatch stopwatch = Stopwatch.createStarted(); doSomething(); stopwatch.stop(); long millis = stopwatch.elapsed(TimeUnit.MILLISECONDS); System.out.println(\u0026#34;time: \u0026#34; + millis + \u0026#34; ms\u0026#34;); } } ","date":"2019-11-18","objectID":"860a297a1e38020f3224599e6970bb92","title":"Java - 測量程式執行時間","url":"https://kucw.io/blog/2019/11/java-time-measurment/"},{"categories":["Java"],"content":"GraalVM 是 Oracle 發佈的下世代 jvm，2019.05 才發佈了第一個 release 版本，分別有 Community 版和 Enterprise 版\nGraalVM 三大特點 # High-performance modern Java : 使用 GraalVM 執行 Java 程式可以變得更快 Polyglot : 可以在 Java 裡面同時使用多種語言，像是 JavaScript、R\u0026hellip; Instant startup, low footprint : 直接把 Java program compile 成 machine code，執行起來體積更小、啟動更快 High-performance modern Java # High-performance modern Java 使用到了 Graal compiler 技術，Graal compiler 是一個 JIT compiler，並且是使用 Java 撰寫的，目的是拿來替換掉原本 HotSpot VM 的C2 compiler\n為了讓 Graal compiler 可以更彈性的被更新（總不能每發布一次 Graal compiler 更新就要重新 compile 整個 java），Oracle 加上了一層JVM Compiler Interface，簡稱 JVMCI，把原本那些應該由 C2 執行的請求抽象化成 interface，解耦 C1 compiler 和 Graal compiler 的連結，讓 Graal 可以更輕易的被更新\n雖然 Graal 是使用 java 寫的，難免會讓人聯想到性能會比不上 C2 compiler，但是在各種實驗之後，得到的數據顯示對於 Java program，Graal 和 C2 compiler 的能力幾乎不相上下（在已經預熱完畢的前提下），而對於 Scala program，Graal 更是達到 10% 以上的優化，這也是為什麼 Twitter 大規模的使用 GraalVM 替換掉原本的 HotspotVM\n不過在啟動時，GraalVM 會比 HotspotVM 還慢，原因是必須先將 Graal 編譯成 machine code，這個是無可避免的，只是當預熱完畢時，Graal 和 C2 compiler 的性能不相上下，並且根據 Graal 的版本不斷更新，這個數據只可能會更好\nPolyglot # GraalVM 最一開始被發明出來，就是為了讓 Java 可以在一次 runtime 中同時使用多種語言，從官網稱呼 GraalVM 為 High-performance polyglot VM，也可以發現 Oracle 對 GraalVM 定位是一個多語言 jvm\n為了實現 Polyglot，GraalVM 引入了一層 Truffle framework，只要實現了該語言的 interpreter，就可以在 Java program 中使用該語言\n目前官方支援的語言僅有 Python、R、JavaScript，後續會陸續增加\n具體實例\npublic class Main { public static void main(String[] args) { System.out.println(\u0026#34;Hello World from Java!\u0026#34;); Context context = Context.newBuilder().allowAllAccess(true).build(); context.eval(\u0026#34;js\u0026#34;, \u0026#34;print(\u0026#39;Hello World from JavaScript!\u0026#39;);\u0026#34;); context.eval(\u0026#34;python\u0026#34;, \u0026#34;print(\u0026#39;Hello World from Python!\u0026#39;)\u0026#34;); context.eval(\u0026#34;ruby\u0026#34;, \u0026#34;puts \u0026#39;Hello World from Ruby!\u0026#39;\u0026#34;); } } Hello World from Java! Hello World from JavaScript! Hello World from Python! Hello World from Ruby! Instant startup, low footprint # GraalVM 還有最後一項技術，就是 native image，他是在 compile time時，就將 Java program 直接 compiler 成 binary 的 machine code，讓這個程式可以像一般二進制的檔案被運行\nNative images compiled with GraalVM ahead-of-time improve the startup time and reduce the memory footprint of JVM-based applications.\nNative-image 帶來的好處是可以更快速的啟動一個 java program，以往如果要啟動 java 程式，需要先啟動 jvm 再載入 java code，然後再即時的 compile bytecode to machine code，非常耗時和耗 memory，而如果使用 native-image，可以取得一個更小更快速的鏡像，適合用在 cloud deploy\nnative image 之所以可以快速啟動，是因為他做了 Ahead-of-time compile，在 compile time 時，會把所有相關的東西，包含一個 Substrate VM，一起 compile 成 machine code，這個 SubstrateVM 是 GraalVM 才有的東西，他只包含最基本的 thread scheduling、垃圾回收，盡可能的縮小必要的 jvm 體積\n雖然 native image 感覺很猛，但是他也有不可抹滅的缺點，就是使用 native image 的程式，throughput 會下降，原因是因為 java 程式很大一部分的優化都在 JIT compiler 中\n還有另一個缺點是，native image 並沒有辦法動態的加載類（因為所有東西必須要在 compile time 就決定好），所以也沒辦法使用反射等相關機制，不過對於這個問題，GraalVM 也有提出相對應的解法，就是在 compile 時，把所有可能的類全部 compile 進來，所以反射機制還是可以支持的，不然的話，整個 Spring framework 就不能使用 native image 了\n目前 Spring 5 也打算開始支持 GraalVM native-image 的開箱即用設定，可以看到 serverless 的 java program 可能是之後的趨勢\n補充一下，serverless 就是指 Fuction as a Service，他的目的是希望 program 不用一直 run 著，當有請求來的時候，我快速啟動這個 program，然後請求走我就 shutdown 這個 program，不讓 program 一直啟動著，而是有需要的時候才開啟他，也就是說，FaaS 就是讓這個 program 像是 function 一樣，用完即走，因此 native-image 的快速啟動非常符合FaaS的需求\n安裝 GraalVM # 上官網下載 GraalVM community，如果電腦是 mac","date":"2019-10-09","objectID":"decafc112fb06a3d9050e870184fac34","title":"GraalVM 介紹 + 安裝教學","url":"https://kucw.io/blog/2019/10/java-graalvm/"},{"categories":["Java"],"content":" Iterator 是所有 Collection 類（List、Set\u0026hellip;.）們都可以使用的迭代器，而 ListIterator 則是專門為 List 類所設計的迭代器\nIterator 只支持 hasNext()、next()、remove() 三種操作，而 ListIterator 除了原本的 3 種之外，還支持了更多操作\n//Iterator接口 public interface Iterator\u0026lt;E\u0026gt; { boolean hasNext(); E next(); void remove(); } //ListIterator接口 public interface ListIterator\u0026lt;E\u0026gt; extends Iterator\u0026lt;E\u0026gt; { //繼承自Iterator的接口 boolean hasNext(); //後面是否有元素 E next(); //游標向後移動，取得後面的元素 void remove(); //刪除最後一個返回的元素 //ListIterator新增的接口 boolean hasPrevious(); //前面是否有元素 E previous(); //游標往前移動，取得前面的元素 int previousIndex(); //取得游標前的index int nextIndex(); //取得游標後的index void set(E e); //將當前元素改設成e void add(E e); //增加一個元素 } Iterator 和 ListIterator 的差別\niterator() 方法在所有集合類中都能使用，但是 listIterator() 只有 List 類能用 Iterator 只能 remove() 元素，而 ListIterator 可以 add()、set()、remove() Iterator 只能使用 next() 順序的向後遍歷，ListIterator 則向前 previous() 和向後 next() 遍歷都可以 還有一個額外的功能，ListIterator 可以使用 nextIndex() 和 previousIndex() 取得當前游標位置的前後 index 位置，Iterator 沒有此功能 如果想在遍歷 List 時邊做刪除，用 Iterator 和 ListIterator 都能辦到，但如果是想在遍歷 List 時 add 元素，則只能使用 ListIterator 去做，因為 Iterator 是不提供此接口的\n要注意的是，邊遍歷 List 邊使用 Iterator 和 ListIterator 的 add()、remove() 時，並不會影響當前 List 輸出結果，雖然他們修改的是同一個 List，但是迭代器故意將 add 和 remove 設計成就算執行了，也不影響當前迭代器的輸出結果\npublic class Main { public static void main(String[] args) { List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(1); list.add(2); list.add(3); ListIterator\u0026lt;Integer\u0026gt; it = list.listIterator(); while (it.hasNext()) { Integer x = it.next(); System.out.println(x); //雖然使用it.add(100)去新增一個元素，使得list實際儲存的是 [1,2,100,3] //但是此處的遍歷仍然只顯示[1,2,3]，這是迭代器故意這樣設計的 if (x == 2) { it.add(100); } } System.out.println(\u0026#34;list: \u0026#34; + list); } } 1 2 3 list: [1, 2, 100, 3] 另外，雖然 ArrayList 和 LinkedList 都支持 ListIterator，但通常只有在使用 LinkedList 時才會搭配 ListIterator\n因為 LinkedList 幾乎所有的時間消耗都是在去找到這個元素在哪，而找到此元素之後，對他進行修改是非常容易的事情（只要改指針就可以了），所以使用 ListIterator 的話，就可以節省下這個查找時間 而對 ArrayList來說，因為他查找的速度很快（底層是數組），因此使用 ListIterator 省下的查找時間非常少，所以對他來說，並沒有迫切的需要使用 ListIterator，使用 add(index, E) 也能達到同樣的效率 因此可以說 ListIterator 根本是為 LinkedList 發明的，ArrayList 只是順道實現而已，ArrayList 去實現只是為了設計成讓 List 接口的類們都能使用 ListIterator 而已 ","date":"2018-12-12","objectID":"fe39ba8e8bffa090a9917af29ecec97f","title":"Java - Iterator 和 ListIterator","url":"https://kucw.io/blog/2018/12/java-iterator-and-listiterator/"},{"categories":["Java"],"content":" 為什麼 Iterator 接口，只有 hasNext()、next()、remove() 方法，而沒有 add(E) 方法 ? 邏輯上來說，迭代器是一個一個去遍歷集合中的元素，而當前 iterator 停下的地方，就是迭代到一半的地方 如果當迭代到一半時調用 iterator.add() 方法，理論上來說，應該是要在當前這個元素 E1 後面新增一個元素 E2，使得下次遍歷此集合時，E2 一定會出現在 E1 後面，也就是 [\u0026hellip;.E1, E2, \u0026hellip;.] 假設 add() 方法是以這個語意為前提的話，那麼迭代器不提供此方法是很合理的，對於有序的集合（像是ArrayList）來說，在此元素後面新增一個元素是一個很簡單的事情，但是對於無序的集合（像是HashSet）來說，不能保證新插入的這個元素 E2 一定會在 E1 後面（因為還得計算 HashCode），如此就違反了 add() 的語意了，這也就是為什麼 Iterator 接口不提供 add() 方法 另一個說法是，在使用迭代器時，通常就是 \u0026ldquo;遍歷\u0026rdquo; 的場景，這種場景下很少會去使用 add() 方法，因此 Iterator 接口沒必要提供這個方法 ","date":"2018-12-06","objectID":"55704ca4817297acb07604b43d8b27fd","title":"Java - 為什麼 Iterator 不提供 add（E）方法 ？","url":"https://kucw.io/blog/2018/12/java-iterator-without-add/"},{"categories":["Java"],"content":" 所有的集合類（List、Set\u0026hellip;）都實現自 Collection 接口，而 Collection 接口又繼承於 Iterable 接口，因此可以說所有的集合類（List、Set\u0026hellip;）都實現了 Iterable 接口\n當某個類實現 Iterable 接口時，我們就能稱這個類是一個 \u0026ldquo;可數\u0026rdquo; 的類，也就是可以使用 iterator() 獲取一個迭代器 Iterator，然後使用這個 Iterator 實例去遍歷這個類，因此所有的 Collection 類都能夠使用迭代器 Iterator 來遍歷\nIterable 接口\npublic interface Iterable\u0026lt;T\u0026gt; { //當某個類實現Iterable接口的話，就能獲取到迭代器iterator，然後就能使用這個iterator去遍歷此類 Iterator\u0026lt;T\u0026gt; iterator(); } Iterator 接口\n如果某個類實現了 Iterable 接口，那麼他也需要創建一個內部類去實現一個 Iterator 類，讓調用 Iterable 接口中的 iterator() 時，能夠獲取到一個 iterator 實例\npublic interface Iterator\u0026lt;E\u0026gt; { //是否有下一個元素 boolean hasNext(); //取得下一個元素 E next(); //刪除最後一個獲取的元素，因此調用remove()前一定得先調用一次next() void remove(); } 至於此 Iterator 接口怎麼實現，就看各個集合實現類如何定義 \u0026ldquo;下一個元素\u0026rdquo;，像是 ArrayList 的下一個元素就是 element[index+1]，而 HashMap 的下一個元素則是 hash table 數組中儲存的下一個 entry\n另外可以想像 Iterator 像是一個游標一樣，一開始停在最前面，然後不停的往後走（只能向後移動），且此游標每次都是停在元素和元素的中間，當調用 next 時，迭代器就越過下一個元素，並返回剛剛越過的那個元素的引用\n使用迭代器 Iterator 遍歷 ArrayList\npublic class Main { public static void main(String[] args) { //ArrayList實現了Collection接口，因此他也實現了Iterable接口，所以他可以使用iterator迭代器來遍歷 List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(\u0026#34;hello\u0026#34;); list.add(\u0026#34;world\u0026#34;); //調用Iterable接口中的iterator()取得此ArrayList的迭代器實例 Iterator\u0026lt;String\u0026gt; its = list.iterator(); //使用Iterator接口的hasNext()、next()來遍歷此ArrayList集合實現類 while (true) { if (its.hasNext()) { String s = its.next(); System.out.println(s); } else { break; } } } } 而再進一步說，當某個類能使用迭代器 Iterator 來遍歷時，就能使用 java 提供的 foreach 語法糖來遍歷此類（foreach語法糖其實就是簡化的 iterator()）\nforeach 實際上會被編譯器編譯成使用迭代器 iterator() 去遍歷集合，因此能使用 foreach 的，都是得實現 Iterable 接口的集合類 Collection 們，像是 List、Set\n所以 Map 就沒有辦法直接使用 foreach（因為 Map 沒有實現 Iterable 接口），只有他的 map.entrySet()、map.keySet()、map.values() 這種返回一個集合類的方法，才能使用 foreach\npublic class Main { public static void main(String[] args) { List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); list.add(\u0026#34;hello\u0026#34;); list.add(\u0026#34;world\u0026#34;); //原代碼，使用語法糖的foreach for (String s : list) { System.out.println(s); } //實際上會被編譯成使用iterator去遍歷 for (Iterator\u0026lt;String\u0026gt; its = list.iterator(); its.hasNext(); ) { String s = its.next(); System.out.println(s); } } } 為什麼 Iterator 要額外使用內部類去實現，而不是 ArrayList 直接實現此接口 ?\n如果看過 Collection 類的源碼（以ArrayList為例），可以發現 ArrayList 類並不是由 ArrayList 去實現 Iterator 接口，而是 ArrayList 有一個內部類 Itr，專門去實現 Iterator 接口，而 ArrayList 的 iterator() 方法，只是去創建一個內部類 ArrayList.Itr 的實例而已\n//ArrayList不實現Iterator接口，反而是由他的內部類進行實現 public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; { //調用list.iterator()可以取得此list的迭代器 public Iterator\u0026lt;E\u0026gt; iterator() { return new Itr(); //實際上就是去創建一個內部類的實例 } //ArrayList中的內部類Itr，專門實現Iterator接口 private class Itr implements Iterator\u0026lt;E\u0026gt; { int cursor; //記錄當前迭代到哪裡 public boolean hasNext() { ... } public E next() { ... } public void remove() { ... } } } 要這樣設計是因為一個集合類可能同時有多個迭代器去遍歷他，而每個迭代器遍歷到集合的哪裡，是每個迭代器自己的事情，彼此不互相干涉，因此才需要額外使用一個內部類去實現迭代器的 Iterator 接口\n如此當需要用到 Iterator 來遍歷集合時，只需要調用 list.iterator()，就能取得一個全新的、不受別人影響的迭代器供自己使用，而迭代器彼此之間也不會互相干涉 至於為什麼要特別使用內部類來實現 Iterator 接口，而不是創建一個 Iterator 公共類來供所有集合一起使用，是因為迭代器需要知道集合的內部結構，他才能知道要怎麼去實現 hasNext()、next()、remove() 方法，而使用內部類才能無條件的取用外部類的所有信息（包含 private 的變量和方法），因此才需要將 Iterator 提取成接口，讓每個集合自己使用內部類去實現 Iterator 接口 為什麼 Iterator 接口，只有 hasNext()","date":"2018-12-05","objectID":"df46f721cab925e1e7bb583bcecc6677","title":"Java - Iterable 接口、迭代器 Iterator","url":"https://kucw.io/blog/2018/12/java-iterator/"},{"categories":["Java"],"content":" 在探討 Java 傳遞參數是 pass by value 還是 pass by reference 之前，需要先了解 Java 中 = 的具體細節\n賦值 = 的用法\n= 的意義是賦值，但是這個賦值用在 基本類型 和 對象類型 上會有非常大的差別\n如果 = 用在基本類型上，因為基本類型儲存了實際的數值，所以在為其賦值時，是直接將值複製一份新的過去 因此假設 a、b 都是基本類型，如果執行了 a=b，那麼就是將 b 的內容直接複製一份新的給 a，之後如果改變了 a 的值，也不會影響到 b 此處的基本類型，泛指 int、long、boolean\u0026hellip;.和其包裝型態 Integer、Long、Boolean\u0026hellip;.，只要是這些類型的變量，都適用基本類型的 = 規則 但如果 = 用在對象類型上，因為在使用對象操作時，實際儲存的其實是對象的引用，所以在為其賦值時，實際上只是把 \u0026ldquo;引用\u0026rdquo; 從一個地方複製到另一個地方 因此假設 c、d 都是對象類型，如果執行了 c=d，那麼 c 和 d 都會指向原本只有 d 指向的那個對象，而原本 c 的那個對象因為沒人引用了，所以會被垃圾回收清理掉 另外，只要是數組，不管你是基本類型 int[] 還是對象類型 Tank[]，一律存的都是引用，所以只要賦值了，也會互相影響 具體實例\nt1、t2是基本類型的 = 效果（不會互相影響）\nt3、t4是對象類型的 = 效果（因為存的是引用，所以會互相影響）\ni5、i6是基本類型的數組的 = 效果（因為存的也是引用，所以也會互相影響）\nclass Tank { int level; } public class Main { public static void main(String[] args) { Tank t1 = new Tank(); Tank t2 = new Tank(); t1.level = 1; t2.level = 2; System.out.println(\u0026#34;t1: \u0026#34; + t1.level + \u0026#34;, t2: \u0026#34; + t2.level); //此處只是基本類型的賦值，所以t1、t2仍舊指到兩個不同對象 t1.level = t2.level; System.out.println(\u0026#34;t1: \u0026#34; + t1.level + \u0026#34;, t2: \u0026#34; + t2.level); t1.level = 100; System.out.println(\u0026#34;t1: \u0026#34; + t1.level + \u0026#34;, t2: \u0026#34; + t2.level); System.out.println(\u0026#34;----\u0026#34;); Tank t3 = new Tank(); Tank t4 = new Tank(); t3.level = 3; t4.level = 4; System.out.println(\u0026#34;t3: \u0026#34; + t3.level + \u0026#34;, t4: \u0026#34; + t4.level); //此處是對象類型的賦值，所以是t3和t4都指到了同一個對象上 //而原本t3那個對象因為沒人引用了，所以會被垃圾回收清理掉 t3 = t4; System.out.println(\u0026#34;t3: \u0026#34; + t3.level + \u0026#34;, t4: \u0026#34; + t4.level); t3.level = 100; System.out.println(\u0026#34;t3: \u0026#34; + t3.level + \u0026#34;, t4: \u0026#34; + t4.level); System.out.println(\u0026#34;----\u0026#34;); int[] i5 = {5}; int[] i6 = {6}; System.out.println(\u0026#34;i5[0]: \u0026#34; + i5[0] + \u0026#34;, i6[0]: \u0026#34; + i6[0]); //因為數组存的是引用，所以i5和i6會指到同一個地方上 i5 = i6; System.out.println(\u0026#34;i5[0]: \u0026#34; + i5[0] + \u0026#34;, i6[0]: \u0026#34; + i6[0]); i5[0] = 200; System.out.println(\u0026#34;i5[0]: \u0026#34; + i5[0] + \u0026#34;, i6[0]: \u0026#34; + i6[0]); } } t1: 1, t2: 2 t1: 2, t2: 2 t1: 100, t2: 2 ---- t3: 3, t4: 4 t3: 4, t4: 4 t3: 100, t4: 100 ---- i5[0]: 5, i6[0]: 6 i5[0]: 6, i6[0]: 6 i5[0]: 200, i6[0]: 200 Pass by value or Pass by reference\n和 = 一樣，只要掌握好基本類型實際儲存的是 \u0026ldquo;值\u0026rdquo;、對象類型儲存的是 \u0026ldquo;引用\u0026rdquo;、數組不論什類型存的都是 \u0026ldquo;引用\u0026rdquo;，就能了解 Java 到底什麼時候是 pass by value，什麼時候是 pass by reference\n基本類型 pass by value，對象類型 pass by reference，而數組因為存的都是引用，所以也是 pass by reference\nclass Tank { int level; } public class Main { public static void main(String[] args) { Tank t1 = new Tank(); Tank t2 = new Tank(); t1.level = 1; System.out.println(\u0026#34;t1.level: \u0026#34; + t1.level); fooInt(t1.level); //基本類型pass by value System.out.println(\u0026#34;t1.level: \u0026#34; + t1.level); t2.level = 2; System.out.println(\u0026#34;t2.level: \u0026#34; + t2.level); fooTank(t2); //對象類型pass by reference System.out.println(\u0026#34;t2.level: \u0026#34; + t2.level); } public static void fooTank(Tank tank){ tank.level = 1000; } public static void fooInt(int level){ level = 5; } } t1.level: 1 t1.level: 1 t2.level: 2 t2.level: 1000 基本類型的List、Set、Map 也是 pass by value，對象類型的 List、Set、Map 是 pass by reference\nclass Tank { int level; } public class Main { public static void main(String[] args) { List\u0026lt;Tank\u0026gt; tankList = new ArrayList\u0026lt;\u0026gt;(); List\u0026lt;Integer\u0026gt; intList = new Arr","date":"2018-09-10","objectID":"53465f549ce0b7aa1d4dfdb5d88bb771","title":"Java - Pass by value or Pass by reference？","url":"https://kucw.io/blog/2018/9/java-pass-param/"},{"categories":["Elastic Search"],"content":" 閱讀本文需要先了解 Elastic Search 的 index 和 analyzer 的相關知識\nElasticSearch - index mapping（5.x以上）\nElasticSearch - 自定義 analysis\n在此之前，ES 所有的查詢都是針對整個詞進行操作，也就是說倒排索引存了 hello 這個詞，一定得輸入 hello 才能找到這個詞，輸入 h 或是 he 都找不到倒排索引中的 hello\n然而在現實情況下，用戶已經漸漸習慣在輸入完查詢內容之前，就能為他們展現搜索結果，這就是所謂的即時搜索（instant search），或是可以稱為 輸入即搜索（search-as-you-type） 雖然 ES 提供了一系列的前綴搜索 match_phrase、prefix、wildcard、regexp，然而這樣的查詢的性能非常差，要知道用戶每多輸入一個新的字母，就意味著要重新進行一次搜索，在實時的 web 系統中，100 ms 可能就會是一個難以忍受的延遲 因此為了加快 輸入即搜索 的查詢效率，可以改使用 edge n-gram 建立索引，如此可以避免使用前綴查詢，在建立索引時就進行優化，使用空間換取時間，讓查詢的速率增快 使用 edge n-gram 建立索引\n假設有一個詞 hello，普通建索引時，就是把這個詞 hello 放入倒排索引\n用戶輸入 h、he時會找不到索引（倒排索引中只有 hello），因此匹配失敗 而對於輸入即搜索這種應用場景，可以使用一種特殊的 n-gram，稱爲 edge n-grams\n所謂的 edge n-gram，就是指它會固定詞語開始的一邊滑動窗口，他的結果取決於 n 的選擇長度\n以單詞 hello 爲例，它的 edge n-gram 的結果如下\nh he hel hell hello 因此可以發現到，在使用 edge n-gram 建索引時，一個單詞會生成好幾個索引，而這些索引一定是重頭開始\n這符合了輸入即搜索的特性，即是用戶打 h、he 能找到倒排中的索引 h、he，而這些索引對應著的數據就是 hello\n具體實例\n建立索引時使用 edge n-gram 的 token 過濾器，為每個經過這個 token 過濾器的詞條們，都生成從頭開始的字符組合\n假設有一個輸入 QUICK! RUN!，分詞器會先將它分詞成兩個詞 quick 和 run，此時這些詞再一一的通過 edge n-gram token 過濾器，產生了 8 個索引 q、qu、qui、quic、quick、r、ru、run，接著存入倒排索引中 如此，任何詞條像是 quick、run，都能生成他們自己的 n-gram 另外要注意，要額外定義一個 search_analyzer 分析器，供查詢使用\n原因是因為我們為了要保證倒排索引中包含各種組合的詞，所以在建索引時才加入了 edge n-gram 過濾器，然而在查詢時，我們只想匹配用戶輸入的完整詞組，像是用戶的輸入 run 或 qu\n因此需要定義兩套分析器，一套是建索引的分析器（包含edge n-gram 過濾器），另一套是查詢使用的正常的分析器\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { //定義一個edge n-gram的token過濾器 //並設置任何通過這個過濾器的詞條，都會生成一個最小固定值為1，最大固定值為20的n-gram \u0026#34;my_autocomplete_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 1, \u0026#34;max_gram\u0026#34;: 20 } }, \u0026#34;analyzer\u0026#34;: { //自定義一個分析器，並使用自定義的edge n-gram過濾器 \u0026#34;my_autocomplete_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;my_autocomplete_filter\u0026#34; ] } } } }, \u0026#34;mapping\u0026#34;: { \u0026#34;my_type\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_autocomplete_analyzer\u0026#34;, //在索引時用，生成edge n-gram的每個詞 \u0026#34;search_analyzer\u0026#34;: \u0026#34;standard\u0026#34; //查詢用，只搜索用戶輸入的詞 } } } } } 讓非 text 字段也能使用 edge n-gram\n由於 edge n-gram 是一個 token 過濾器，他包含在 analyzer 分析器裡面，因此只有 text 類型的字段才能使用（其他類型的字段不會被分詞，所以不會使用到 analyzer，因此不能用 edge n-gram）\n但是可能會有一種情況是，有些精確值也希望能通過 edge n-gram 生成組合，這時就要搭配使用一個叫做 keyword 的分詞器\n注意，此 keyword 分詞器和 keyword 字段類型是不同的東西 keyword 分詞器主要的功用是，將輸入的詞條，原封不動的 output 出來，不對其內容做任何改變 因此可以利用這個特性，將精確值的字段類型改成 text，但是分詞器使用 keyword，如此就可以避免分詞的效果，又能使用 edge n-gram 具體實例\n將 postcode 這個本來是 keyword 類型的精確值，改成使用 text 類型並搭配 keyword 分詞器\n因此假設有一個輸入 ABC EF，先經過 keyword 分詞器分詞成 ABC EF（和輸入一模一樣），接著再經過 edge n-gram 生成 A、AB、ABC、ABC （有一個空格） 、ABC E、ABC EF\n如果是使用正常的分詞器，生成的 edge n-gram 會是 A、AB、ABC、E、EF，他們是有差別的\nPUT my_index { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;postcode_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;edge_ngram\u0026#34;, \u0026#34;min_gram\u0026#34;: 1, \u0026#34;max_gram\u0026#34;: 8 } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;postcode_index\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;postcode_filter\u0026#34; ] }, \u0026#34;postcode_search\u0026#34;: { \u0026#34;tokenizer\u0026#34;: \u0026#34;keyword\u0026#34; } } } }, \u0026#34;mapping\u0026#34;: { \u0026#34;m","date":"2018-08-19","objectID":"68949f08a1e67a8e22fff4a059d079f7","title":"ElasticSearch - 輸入即搜索 edge n-gram","url":"https://kucw.io/blog/2018/8/elasticsearch-instant-search/"},{"categories":["Elastic Search"],"content":" ES 的 bulk 語法允許在一個請求中進行多個操作（create、index、update、delete），也就是可以在一次請求裡做很多事情\n也由於這個關係，因此 bulk 的請求體和其他請求的格式會有點不同 bulk 的請求模板\n分成 action 和 metadata 兩部份 action : 必須是以下 4 種選項之一 index(最常用） : 如果文檔不存在就創建他，如果文檔存在就更新他 create : 如果文檔不存在就創建他，但如果文檔存在就返回錯誤 使用時一定要在 metadata 設置 _id 值，他才能去判斷這個文檔是否存在 update : 更新一個文檔，如果文檔不存在就返回錯誤 使用時也要給 _id 值，且後面文檔的格式和其他人不一樣 delete : 刪除一個文檔，如果要刪除的文檔 id 不存在，就返回錯誤 使用時也必須在 metadata 中設置文檔 _id，且後面不能帶一個 doc，因為沒意義，他是用 _id 去刪除文檔的 metadata : 設置這個文檔的 metadata，像是 _id、_index POST mytest/_bulk { action : { metadata } } { doc } { action : { metadata } } { doc } .... 具體實例\nbulk請求\nPOST mytest/_bulk //創建一筆數據 { \u0026#34;create\u0026#34; : { \u0026#34;_id\u0026#34;: 1 } } { \u0026#34;color\u0026#34;: \u0026#34;create black\u0026#34; } //創建一筆數據，因為id=1的文檔已經存在，所以會創建失敗 { \u0026#34;create\u0026#34; : { \u0026#34;_id\u0026#34;: 1 } } { \u0026#34;color\u0026#34;: \u0026#34;create black2\u0026#34; } //索引一筆數據 { \u0026#34;index\u0026#34; : { \u0026#34;_id\u0026#34;: 2 } } { \u0026#34;color\u0026#34;: \u0026#34;index red\u0026#34; } //索引一筆數據，但是index可以創建也可以更新，所以執行成功 { \u0026#34;index\u0026#34; : { \u0026#34;_id\u0026#34;: 2 } } { \u0026#34;color\u0026#34;: \u0026#34;index red2\u0026#34; } //索引一筆數據，不一定要設置id(index又能創建又能更新又不用設id，超好用) { \u0026#34;index\u0026#34;: {} } { \u0026#34;color\u0026#34;: \u0026#34;index blue\u0026#34; } //刪除一筆文檔，注意delete後面不接一個doc { \u0026#34;delete\u0026#34; : { \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } //找不到此id的文檔，刪除失敗 { \u0026#34;delete\u0026#34; : { \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34; } } //更新一筆文檔，注意doc格式不太一樣 { \u0026#34;update\u0026#34; : { \u0026#34;_id\u0026#34;: 1 } } { \u0026#34;doc\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;update green\u0026#34;} } //更新一筆文檔，但因為此id的文檔不存在，所以更新失敗 { \u0026#34;update\u0026#34; : { \u0026#34;_id\u0026#34;: 100 } } { \u0026#34;doc\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;update green2\u0026#34;} } bulk 的返回結果\n因為在 bulk 中，每個 action 的執行結果都是獨立的，所以有幾個 action，就會有幾個返回結果，返回結果如下\n最上面會有一個 errors，表示這一次 bulk 請求中，是否有 action 出錯了 因此寫代碼時可以先檢查 errors 這個值，如果是 false，表示這次 bulk 請求全部通過，就不用再一一去檢查是否有 action 出錯，但如果是 true，則必須去 items 一個一個檢查到底是哪個 action 出錯了 items 是一個 array，裡面則放著每個 action 對應的結果，上面的請求執行了 9 個 action，所以返回結果的 items 就會有 9 個\n返回結果會依照 action 的順序排好，因此 items 的第一個結果就是請求時第一個 action 的執行結果 { \u0026#34;took\u0026#34;: 145, \u0026#34;errors\u0026#34;: true, \u0026#34;items\u0026#34;: [ { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;mytest\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;status\u0026#34;: 201 } }, { \u0026#34;create\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;mytest\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;status\u0026#34;: 409, \u0026#34;error\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;version_conflict_engine_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;[1]: version conflict, document already exists (current version [1])\u0026#34;, \u0026#34;index_uuid\u0026#34;: \u0026#34;PfdgdTyiRgaCIM6uKRQAPQ\u0026#34;, \u0026#34;shard\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;index\u0026#34;: \u0026#34;mytest\u0026#34; } } }, { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;mytest\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;result\u0026#34;: \u0026#34;created\u0026#34;, \u0026#34;status\u0026#34;: 201 } }, { \u0026#34;index\u0026#34;: { \u0026#34;_index\u0026#34;: \u0026#34;mytest\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;_doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;2\u0026#34;, \u0026#34;result\u0026#34;: \u0026#34;updated\u0026#34;, \u0026#34;status\u0026#34;: 200 } } ... 5 RESULTS REMOVED ... ] } 使用 bulk 要注意的地方\n如果使用 127.0.0.1/_bulk，那麼就是在整個 ES 的範圍中插入數據，因此在 metadata 中要指定插入的 index，優點是可以一次插入多筆數據到不同的索引 而如果使用 127.0.0.1/mytest/_bulk，就不用在 metadata 再次指定要插入的 index，可以想像成是 _bulk API幫我們自動填好了 me","date":"2018-07-30","objectID":"220b7fd894d07bf549b71cc64510972a","title":"ElasticSearch - 批量操作 bulk","url":"https://kucw.io/blog/2018/7/elasticsearch-bulk/"},{"categories":["Intellij"],"content":" 有时候在升级 IntelliJ 時會遇到 Error: java: invalid flag: -version 解決辦法 打開 File -\u0026gt; settings，搜索 Java compiler 此時會發現右下角的每一個 module 裡的 Compilation options 中，都多加了一個 -version 把每個 module 的 -version 都刪除掉，就可以正常運行了 ","date":"2018-07-26","objectID":"f1e1fb9c42bde50826ddb391c8ef572e","title":"IntelliJ - 升級遇到的問題 Error: java: invalid flag: -version","url":"https://kucw.io/blog/2018/7/intellij-update-error/"},{"categories":["Linux"],"content":" tail -f catalina.log : 實時看log，會自動把新增的log直接顯示出來\n在實時日誌上打印顏色，給每個狀態給上不同的顏色，INFO 綠色、WARN 黃色、ERROR 紅色\ntail -f catalina.out | perl -pe \u0026#39;s/(INFO)/\\e[0;32m$1\\e[0m/g,s/(WARN)/\\e[0;33m$1\\e[0m/g,s/(ERROR)/\\e[1;31m$1\\e[0m/g\u0026#39; 只看 ERROR\ntail -f catalina.out | grep \u0026#34;ERROR\u0026#34; --line-buffered | perl -pe \u0026#39;s/(ERROR)/\\e[1;31m$1\\e[0m/g\u0026#39; 在 .bashrc 下加入這一段，可以讓 tail 輸出 log 時有顏色\nalias tail=\u0026#34;_tail_log\u0026#34; _tail_log() { \u0026#34;tail\u0026#34; $@ | perl -pe \u0026#39;s/(INFO)/\\e[0;32m$1\\e[0m/g,s/(WARN)/\\e[0;33m$1\\e[0m/g,s/(ERROR)/\\e[1;31m$1\\e[0m/g\u0026#39; } less : 通常用來翻找舊的日誌\n輸入 F，也可以實時滾動日誌，就像 tail -f 的效果一樣\n在 .bashrc 下加入這一段，可以讓 less 在找 log 時輸出顏色\nhighlight 整條 log\nalias less=\u0026#34;_show_log\u0026#34; _show_log() { awk \u0026#39; /ERROR/ {printf(\u0026#34;\\033[1;31m%s\\033[0m\\n\u0026#34;, $0)} /WARN/ {printf(\u0026#34;\\033[1;33m%s\\033[0m\\n\u0026#34;, $0)} !/(WARN|ERROR)/ {printf(\u0026#34;%s\\n\u0026#34;, $0)} \u0026#39; $1 | \u0026#34;less\u0026#34; -r } 只highlight INFO、WARN、ERROR 這種狀態\nalias less=\u0026#34;_show_log\u0026#34; _show_log() { awk \u0026#39; /ERROR/ {sub(/ERROR/, \u0026#34;\\033[1;31mERROR\\033[0m\u0026#34;)} /WARN/ {sub(/WARN/, \u0026#34;\\033[1;33mWARN\\033[0m\u0026#34;)} /INFO/ {sub(/INFO/, \u0026#34;\\033[0;32mINFO\\033[0m\u0026#34;)} {print} \u0026#39; $1 | \u0026#34;less\u0026#34; -r } multitail : 可同時開啟多視窗看 log，適合用在看部署在很多機器上的項目的 log\n-cS [color_scheme] : 可以選擇輸出的 log 的顏色，推薦使用 goldengate，也可自定義（修改/etc/multitail.conf）\n-s [column number] : 設定看 log 時會分成幾個縱列\nmultitail -s 2 -cS goldengate -l \u0026#39;ssh [ip] \u0026#34;tail -100f /example/logs/catalina.out\u0026#34;\u0026#39; -cS goldengate -l \u0026#39;ssh [ip] \u0026#34;tail -100f /example/logs/catalina.out\u0026#34;\u0026#39; multitail 開始運作後，點擊 b，可以選擇要 scroll 的檔案，點擊 q 退出\n","date":"2018-07-25","objectID":"b6986617d9c476e93be05d004f10a24c","title":"Linux - 查看 Log 的指令 tail、multitail、less","url":"https://kucw.io/blog/2018/7/linux-log-command/"},{"categories":["Elastic Search"],"content":" 聚合概念 aggs # ElasticSearch 除了致力於搜索之外，也提供了聚合實時分析數據的功能 如果把搜索比喻為大海撈針（從海量的文檔中找出符合條件的那一個），那麼聚合就是去分析大海中的針們的特性，像是 在大海里有多少針？ 針的平均長度是多少？ 按照針的製造商來劃分，針的長度中位值是多少？ 每月加入到海中的針有多少？ 這裏面有異常的針麼？ 因此透過聚合，我們可以得到一個數據的概覽，聚合能做的是分析和總結全套的數據，而不是查找單個文檔（這是搜索做的事） 聚合允許我們向數據提出一些複雜的問題，雖然他的功能完全不同於搜索，但他們其實使用了相同的數據結構，這表示聚合的執行速度很快，並且就像搜索一樣幾乎是實時的 並且由於聚合和搜索是使用同樣的數據結構，因此聚合和搜索可以是一起執行的 這表示我們可以在一次 json 請求裡，同時對相同的數據進行 搜索/過濾 + 分析，兩個願望一次滿足 聚合的兩個主要的概念，分別是 桶 和 指標 桶（Buckets）: 滿足特定條件的文檔的集合 當聚合開始被執行，每個文檔會決定符合哪個桶的條件，如果匹配到，文檔將放入相應的桶並接着進行聚合操作 像是一個員工屬於男性桶或者女性桶，日期 2014-10-28 屬於十月桶，也屬於 2014 年桶 桶可以被嵌套在其他桶裏面 像是北京能放在中國桶裡，而中國桶能放在亞洲桶裡 Elasticsearch 提供了很多種類型的桶，像是時間、最受歡迎的詞、年齡區間、地理位置桶等等，不過他們在根本上都是通過同樣的原理進行操作，也就是基於條件來劃分文檔，一個文檔只要符合條件，就可以加入那個桶，因此一個文檔可以同時加入很多桶 指標（Metrics） : 對桶內的文檔進行統計計算 桶能讓我們劃分文檔到有意義的集合， 但是最終我們需要的是對這些桶內的文檔進行一些指標的計算 指標通常是簡單的數學運算（像是min、max、avg、sum），而這些是通過當前桶中的文檔的值來計算的，利用指標能讓你計算像平均薪資、最高出售價格、95 % 的查詢延遲這樣的數據 aggs 聚合的模板 當 query 和 aggs 一起存在時，會先執行 query 的主查詢，主查詢 query 執行完後會搜出一批結果，而這些結果才會被拿去 aggs 拿去做聚合 另外要注意 aggs 後面會先接一層自定義的這個聚合的名字，然後才是接上要使用的聚合桶 如果有些情況不在意查詢結果是什麼，而只在意 aggs 的結果，可以把 size 設為 0，如此可以讓返回的 hits 結果集是 0，加快返回的速度 一個 aggs 裡可以有很多個聚合，每個聚合彼此間都是獨立的，因此可以一個聚合拿來統計數量、一個聚合拿來分析數據、一個聚合拿來計算標準差\u0026hellip;，讓一次搜索就可以把想要做的事情一次做完 像是此例就定義了 3 個聚合，分別是 custom_name1、custom_name2、custom_name3 aggs 可以嵌套在其他的 aggs裡面，而嵌套的桶能作用的文檔集範圍，是外層的桶所輸出的結果集 GET mytest/doc/_search { \u0026#34;query\u0026#34;: { ... }, \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;custom_name1\u0026#34;: { //aggs後面接著的是一個自定義的name \u0026#34;桶\u0026#34;: { ... } //再來才是接桶 }, \u0026#34;custom_name2\u0026#34;: { //一個aggs裡可以有很多聚合 \u0026#34;桶\u0026#34;: { ... } }, \u0026#34;custom_name3\u0026#34;: { \u0026#34;桶\u0026#34;: { ..... }, \u0026#34;aggs\u0026#34;: { //aggs可以嵌套在別的aggs裡面 \u0026#34;in_name\u0026#34;: { //記得使用aggs需要先自定義一個name \u0026#34;桶\u0026#34;: { ... } //in_name的桶作用的文檔是custom_name3的桶的結果 } } } } } 結果 { \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 8, \u0026#34;max_score\u0026#34;: 0, \u0026#34;hits\u0026#34;: [] //因為size設為0，所以沒有查詢結果返回 }, \u0026#34;aggregations\u0026#34;: { \u0026#34;custom_name1\u0026#34;: { ... }, \u0026#34;custom_name2\u0026#34;: { ... }, \u0026#34;custom_name3\u0026#34;: { ... , \u0026#34;in_name\u0026#34;: { .... } } } } terms桶 # 功能 : 針對某個 field 的值進行分組，field 有幾種值就分成幾組 terms 桶在進行分組時，會爲此 field 中的每種值創建一個新的桶 要注意此 \u0026ldquo;terms桶\u0026rdquo; 和平常用在主查詢 query 中的 \u0026ldquo;查找terms\u0026rdquo; 是不同的東西 具體實例 首先插入幾筆數據，其中 color 是一個 keyword 類型 { \u0026#34;color\u0026#34;: \u0026#34;red\u0026#34; } { \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34; } { \u0026#34;color\u0026#34;: [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;] } 執行 terms 桶聚合 GET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;my_name\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;color\u0026#34;, //使用color來進行分組 } } } } 結果 因為 color 總共有 3 種值，red、blue、green，所以 terms 桶為他們產生了 3 個 bucket，並計算了每個 bucket 中符合的文檔有哪些 bucket 和 bucket 間是獨立的，也就是說一個文檔可以同時符合好幾個 bucket，像是 {\u0026quot;color\u0026quot;: [\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;]} 就同時符合了 red 和 blue bucket \u0026#34;aggregations\u0026#34;: { \u0026#34;my_name\u0026#34;: { \u0026#34;doc_count_error_upper_bound\u0026#34;: 0, \u0026#34;sum_other_doc_count\u0026#34;: 0, \u0026#34;buckets\u0026#34;: [ //terms桶產生的buckets數組，裡面每個json對象都是一個bucket { \u0026#34;key\u0026#34;: \u0026#34;blue\u0026#34;, \u0026#34;doc_count\u0026#34;: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;red\u0026#34;, //表示color為red的文檔有2個，此例中就是 {\u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;} 和 {\u0026#34;color\u0026#34;: [\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;]}這兩個文檔 \u0026#34;doc_count\u0026#34;: 2 }, { \u0026#34;key\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34","date":"2018-07-23","objectID":"78294bb6c4280181d28a2712ad9dd904","title":"ElasticSearch - 聚合 aggs","url":"https://kucw.io/blog/2018/7/elasticsearch-aggs/"},{"categories":["Java"],"content":" ThreadLocal 是線程的局部變量， 是每一個線程所單獨持有的，其他線程不能對其進行訪問\nThreadLocal 支持泛型，也就是支持 value 是可以設置類型的，像是 ThreadLocal\u0026lt;Date\u0026gt; 就是設置 value 為 Date 類型 每個線程會有自己的一份 ThreadLocalMap 變量，去儲存這個線程自己想存放的 ThreadLocal 變量們，他內部儲存的是一個鍵值對 Map，其中 key 是某個 ThreadLocal，value 就是這個線程自己 set 的值，所以對於一個線程來說，一個 ThreadLocal 只能存一個值，而一個線程可以存放好多個 ThreadLocal 因此當調用 ThreadLocal tl 的 tl.get() 方法時，其實就是先去取得此線程的 ThreadLocalMap，然後再去查找這個 Map 中的 key 為 tl 的那個 Entry 的 value 值 ThreadLocal 常用的方法\nset(x) : 設置此線程的想要放的值是多少 get() : 取得此線程當初存放的值，如果沒有存放過則返回 null remove() : 刪除此線程的鍵值對，也就是如果先執行 remove 再執行 get，會返回 null ThreadLocal 通常用在 SimpleDateFormat，或是 SpringMVC 上\n因為 SimpleDateFormat 不是線程安全的，因此雖然可以每次要使用的時候重新 new 一個，但是這樣做會很浪費資源，所以如果使用 ThreadLocal 在每個線程裡都存放一個此線程專用的 SimpleDateFormat，就可以避免一直 new 的資源浪費，又確保線程安全 因為 SpringMVC 會對每個請求分配一個線程，可以在攔截器將此線程的用戶信息（ip、名字\u0026hellip;）使用 ThreadLocal 儲存，這樣在後續要用到用戶信息的地方時，就可以去 ThreadLocal 中取得，而且因為 ThreadLocal 可以隔離線程，因此每條請求對應的線程的用戶信息不會互相干擾 ThreadLocal 可能造成的內存洩漏\n在Java裡，每個線程都有自己的 ThreadLocalMap，裡面存著這個線程自己私有的 ThreadLocal 們，而 ThreadLocalMap 的 key 為 ThreadLocal 實例，value 為私有對象 T，即是透過 set() 設置的值\npublic class Thread implements Runnable { //Thread類裡的threadlocals存放此線程的專有的ThreadLocalMap ThreadLocal.ThreadLocalMap threadLocals = null; } public class ThreadLocal\u0026lt;T\u0026gt; { //根據線程，取得那個線程自己的ThreadLocalMap ThreadLocalMap getMap(Thread t) { return t.threadLocals; } static class ThreadLocalMap { //ThreadLocalMap的key是使用 \u0026#34;弱引用\u0026#34; 的ThreadLocal static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026gt; { Object value; //ThreadLocalMap中的key就是ThreadLocal，value就是設置的值 Entry(ThreadLocal k, Object v) { super(k); value = v; } } } } 可以創建許多個 ThreadLocal 對象，對每個 ThreadLocal 都設置不同的值\n像是以下的例子，在 main 線程中的 ThreadLocalMap，就有兩個 key-value 的映射，分別是 userIdThreadLocal -\u0026gt; 100、userNameThreadLocal -\u0026gt; hello\npublic class Main { public static void main(String[] args){ ThreadLocal\u0026lt;Integer\u0026gt; userIdThreadLocal = new ThreadLocal\u0026lt;\u0026gt;(); ThreadLocal\u0026lt;String\u0026gt; userNameThreacLocal = new ThreadLocal\u0026lt;\u0026gt;(); userId.set(100); userName.set(\u0026#34;hello\u0026#34;); } } 之所以 ThreadLocal 會發生內存洩漏，原因是因為只要線程活著，這個線程的 ThreadLocalMap 就會一直活著，而當初透過 ThreadLocal set() 的值，也就會在 ThreadLocalMap 中一直存在這個鍵值對不消失，所以該 ThreadLocal 和該 value 的內存地址始終都有這個 ThreadLocalMap 在引用著，導致 GC 無法回收他，所以才會發生內存洩漏\n為了解決這個問題，java 做了一個小優化，也就是存放在 ThreadLocalMap 中的 ThreadLocal，會使用 弱引用 來儲存，也就是說，如果一個 ThreadLocal 內存地址沒有外部強引用來引用他，只有這條 ThreadLocalMap 的弱引用來引用他時，那麼當系統 GC 時，這些 ThreadLocal 就會被回收（因為是弱引用），如此一來，ThreadLocalMap 中就會出現 key 為 null 的 Entry 們\n下圖中，實線表示強引用，虛線表示弱引用 這個弱引用優化只能使得 ThreadLocal 被正確回收，但是這些 key 為 null 的 Entry 們仍然會存在在 ThreadLocalMap 裡，因此 value 仍然無法被回收\n所以 java 又做了一個優化，就是在 ThreadLocal 執行 get()、set()、remove() 方法時，都會將該線程 ThreadLocalMap 裡所有 key = null 的 value 也設置為 null，手動幫助 GC\nThreadLocal k = e.get(); if (k == null) { e.value = null; // Help the GC } 但是根本上的解決辦法，還是在當前線程使用完這個 ThreadLocal 時，就即時的 remove() 掉該 value，也就是使得 ThreadLocalMap 中不要存在這個鍵值對，這樣才能確保 GC 能正確回收\n具體實例\n每個線程都可以在 ThreadLocal 中放自己的值，且不會干擾到其他線程的值\nclass Tools { public static ThreadLocal threadLocal = new ThreadLocal(); } class MyThread extends Thread { @Override public void run() { if (Tools.threadLocal.get() == null) { Tools.threadLocal.set(Thread.currentThread().getName() + \u0026#34;, \u0026#34; + Math.random()); } System.out.printl","date":"2018-07-16","objectID":"60c8acce13b54ab636e60a38252bb630","title":"Java - ThreadLocal 類的使用","url":"https://kucw.io/blog/2018/7/java-thread-local/"},{"categories":["Spring Boot"],"content":" Spring 中，使用注解 @Autowired 進行注入好，還是使用 xml 配置進行注入好？ 先講結論，使用注解 @Autowired 注入比較好 當時 Spring 開發的初衷是為了解決類與類之間的強耦合 new，所以當時提出了 xml 配置注入bean的方法，就是讓代碼只關注我需要什麼 service，但此 service 是由哪個實現類提供的我並不關心 使用 xml 的好處就是，實現類更換的時候並不需要去改動代碼，只要去改動 xml 配置，將注入的 bean 改成另一個實現類就可以了，如此可以達到類與類之間的松耦合 但是到了 Spring3.0 之後，他們開始提出了使用 @Autowired 注解來進行 bean 的注入 有的人可能會覺得，如果使用 @Autowired、@Qualifier 來注入，那麼假設我要改注入實現類的話，得去改 java 代碼中的 @Qualifier，那這樣還是得改代碼，那這樣使用 Spring 注入和使用 new，又有什麼差別？是不是還是使用 xml 比較好？ 事實上，使用注解確實會有這個問題沒錯，不過經過長時間的項目經驗下來，你會發現，我們其實很少會去改注入的實現類的（天天改服務還要不要命?） 而注解提供的好處卻是不少，像是簡化 xml 配置的冗長、使用注解比較直觀且容易、並且是類型安全的（compiler 可以掃描注解，判斷注入的類型是否正確，但他掃描不了 xml 文件) 因此就算使用注解 @Autowired 去改變注入的實現類比 xml 更困難，但他其他大量的優點足以掩蓋過這個缺點，這也是為什麼 Spring 覺得使用注解配置比使用 xml 配置更好的理由 所以到目前為止（Spring4.0），雖然 Spring 官方本身沒有明說拋棄 xml 配置，不過事實上 Spring 已經轉往注解配置方向前進了，SpringBoot 就是最好的例子 SpringBoot 中只有一個 properties 文件負責配置一些不可避免的設定，像是數據庫連接、mvc 模板配置\u0026hellip;.，除此之外沒有任何一個 xml 文件來定義 bean，全部都是使用注解來配置 注解 vs xml 優缺點比較 注解 優點 : 簡化配置、使用起來直觀且容易，提升開發效率、類型安全 缺點 : 改變實現類比 xml 困難 xml 優點 : 類與類間的松耦合，容易擴展、更換、對象間的關係一目了然 缺點 : 配置冗長，且還要額外多維護一份配置，類型不安全，compiler 無法幫忙校驗，運行期才會發現錯誤 ","date":"2018-07-09","objectID":"c13e8bdfb4e480a692a5adaf81b2c82c","title":"Spring Boot - 注解 vs XML 哪個好？","url":"https://kucw.io/blog/2018/7/spring-annotation-vs-xml/"},{"categories":["Elastic Search"],"content":" 閱讀本文需要先了解 function_score 的相關知識，請看 ElasticSearch - function_score 簡介\n很多變量都可以影響用戶對於酒店的選擇，像是用戶可能希望酒店離市中心近一點，但是如果價格足夠便宜，也願意為了省錢，妥協選擇一個更遠的住處\n如果我們只是使用一個 filter 排除所有市中心方圓 100 米以外的酒店，再用一個 filter 排除每晚價格超過 100 元的酒店，這種作法太過強硬，可能有一間房在 500 米，但是超級便宜一晚只要 10 元，用戶可能會因此願意妥協住這間房 為了解決這個問題，因此 function_score 查詢提供了一組 衰減函數（decay functions）， 讓我們有能力在兩個滑動標準（如地點和價格）之間權衡 function_score 支持的衰減函數有三種，分別是 linear、exp 和 gauss\nlinear、exp、gauss 三種衰減函數的差別只在於衰減曲線的形狀，在 DSL 的語法上的用法完全一樣 linear : 線性函數是條直線，一旦直線與橫軸 0 相交，所有其他值的評分都是 0 exp : 指數函數是先劇烈衰減然後變緩 guass（最常用） : 高斯函數則是鐘形的，他的衰減速率是先緩慢，然後變快，最後又放緩 衰減函數們（linear、exp、gauss）支持的參數 origin : 中心點，或是字段可能的最佳值，落在原點（origin）上的文檔評分 _score 為滿分 1.0，支持數值、時間 以及 \u0026ldquo;經緯度地理座標點\u0026rdquo;（最常用）的字段 offset : 從 origin 為中心，為他設置一個偏移量 offset 覆蓋一個範圍，在此範圍內所有的評分 _score 也都是和 origin 一樣滿分 1.0 scale : 衰減率，即是一個文檔從 origin 下落時，_score 改變的速度 decay : 從 origin 衰減到 scale 所得的評分_score，默認為 0.5（一般不需要改變，這個參數使用默認的就好了） 以上面的圖為例 所有曲線（linear、exp、gauss）的 origin 都是 40，offset 是 5，因此範圍在 40-5 \u0026lt;= value \u0026lt;= 40+5 的文檔的評分 _score 都是滿分 1.0 而在此範圍之外，評分會開始衰減，衰減率由 scale 值（此處是5）和 decay 值（此處是默認值0.5）決定，在 origin +/- (offset + scale) 處的評分是 decay 值，也就是在 30、50 的評分處是 0.5 分 也就是說，在 origin + offset + scale 或是 origin - offset - scale 的點上，得到的分數僅有 decay 分 具體實例一\n先準備數據和索引，在 ES 插入三筆數據，其中 language 是 keyword 類型，like 是 integer 類型（代表點贊量）\n{ \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 10 } { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 15 } 以 like = 15 為中心，使用 gauss 函數\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;functions\u0026#34;: [ { \u0026#34;gauss\u0026#34;: { \u0026#34;like\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;15\u0026#34;, //如果不設置offset，offset默認為0 \u0026#34;scale\u0026#34;: \u0026#34;5\u0026#34;, \u0026#34;decay\u0026#34;: \u0026#34;0.2\u0026#34; } } } ] } } } \u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 15 } }, { //因為改變了decay=0.2，所以當位於 origin-offset-scale=10 的位置時，分數為decay，就是0.2 \u0026#34;_score\u0026#34;: 0.2, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 10 } }, { \u0026#34;_score\u0026#34;: 0.0016, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } } ] 具體實例二\n假設有一個用戶希望租一個離市中心近一點的酒店，且每晚不超過 100 元的酒店，而且與距離相比，我們的用戶對價格更敏感，那麼使用衰減函數 gauss 查詢如下 其中把 price 語句的 origin 點設為 50 是有原因的，由於價格的特性一定是越低越好，所以 0 ~ 100 元的所有價格的酒店都應該認為是比較好的，而 100 元以上的酒店就慢慢衰減 如果我們將 price 的 origin 點設置成100，那麼價格低於 100 元的酒店的評分反而會變低，這不是我們期望的結果，與其這樣不如將 origin 和 offset 同時設成 50，只讓 price 大於 100 元時評分才會變低 雖然這樣設置也會使得 price 小於 0 元的酒店評分降低沒錯，不過現實生活中價格不會有負數，因此就算 price \u0026lt; 0 的評分會下降，也不會對我們的搜索結果造成影響（酒店的價格一定都是正的） 換句話說，其實只要把 origin + offset 的值設為 100，origin 或 offset 是什麼樣的值都無所謂，只要能確保酒店價格在 100 元以上的酒店會衰減就好了 GET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } \u0026#34;functions\u0026#34;: [ //第一個gauss加強函數，決定距離的衰減率 { \u0026#34;gauss\u0026#34;: { \u0026#34;location\u0026#34;: { \u0026#34;origin\u0026#34;: { //origin點設成市中心的經緯度座標 \u0026#34;lat\u0026#34;: 51.5, \u0026#34;lon\u0026#34;: 0.12 }, \u0026#34;offset\u0026#34;: \u0026#34;2km\u0026#34;, //距離中心點2km以內都是滿分1.0，2km外開始衰減 \u0026#34;scale\u0026#34;: \u0026#34;3km\u0026#34; //衰減率 } } }, //第二個gauss加強函數，決定價格的衰減率 //因為用戶對價格更敏感，所以給了這個gauss加強函數2倍的權重 { \u0026#34;gauss\u0026#34;: { \u0026#34;price\u0026#34;: { \u0026#34;origi","date":"2018-07-08","objectID":"a05f5941b38bf3fb5a3c5fa29c6fc461","title":"ElasticSearch - function_score（衰減函數 linear、exp、gauss 具體實例）","url":"https://kucw.io/blog/2018/7/elasticsearch-function_score-gauss/"},{"categories":["Elastic Search"],"content":" 閱讀本文需要先了解 function_score 的相關知識，請看 ElasticSearch - function_score 簡介\n在正常的查詢下，有相同評分的 score 的文檔會每次都會以相同次序出現，但是爲了提高展現率，在此引入一些隨機性可能會是個好主意，這能保證有相同評分的文檔都能有均等相似的展現機率\n但是，在隨機的同時，除了想讓不同的用戶看到不同的隨機次序之外，但也希望如果是同一用戶翻頁瀏覽時，結果的相對次序能始終保持一致，這種行爲被稱爲 一致隨機（consistently random） 因此 random_score 加強函數除了能隨機得到一個 0 ~ 1 的分數，也會使用一個 seed 值，來保障生成隨機的順序，當 seed 值相同時，生成的隨機結果是一致的 先準備數據和索引，在 ES 插入三筆數據，其中 language 是 keyword 類型，like 是 integer 類型（代表點贊量）\n{ \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 5 } { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 10 } 使用 random_score 生成隨機排序\n注意在 functions 裡，只有 weight 加強函數加了 filter，也就是說只有 like 數小於等於 5 的文檔，才會被 weight 加強函數加強\n而 random_score 加強函數沒有加 filter，表示所有的文檔都會被 random_score 隨機排序一遍\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} }, \u0026#34;functions\u0026#34;: [ { \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;like\u0026#34;: { \u0026#34;lte\u0026#34;: 5 } } }, \u0026#34;weight\u0026#34;: 2 }, { \u0026#34;random_score\u0026#34;: { \u0026#34;seed\u0026#34;: 100 } } ] } } } 當 random_score 的 seed 設為 100，其結果為\n\u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 1.2912227, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 1.1301208, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 0.9083528, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 10 } } ] 當 random_score 的 seed 設為 200，其結果為\n\u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 1.2040303, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 0.5595852, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 10 } }, { \u0026#34;_score\u0026#34;: 0.10916698, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 5 } } ] ","date":"2018-07-07","objectID":"20bb3a32640b2e5c56acc6a6bf67126d","title":"ElasticSearch - function_score（random_score 具體實例）","url":"https://kucw.io/blog/2018/7/elasticsearch-function_score-random_score/"},{"categories":["Elastic Search"],"content":" 閱讀本文需要先了解 function_score 的相關知識，請看 ElasticSearch - function_score 簡介\n一樣先準備數據和索引，在 ES 插入三筆數據，其中 language 是 keyword 類型，like 是 integer 類型（代表點贊量）\n{ \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 5 } { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 10 } functions 是一個數組，裡面放著的是將要被使用的加強函數列表，我們在裡面使用了 3 個 filter 去過濾數據，並且每個 filter 都設置了一個加強函數，並且還使用了一個會應用到所有文檔的 field_value_factor 加強函數\n可以為列表裡的每個加強函數都指定一個 filter，這樣做的話，只有在文檔滿足此 filter 的要求，此 filter 的加強函數才會應用到文擋上，也可以不指定 filter，這樣的話此加強函數就會應用到全部的文擋上\n一個文檔可以一次滿足多條加強函數和多個 filter，如果一次滿足多個，那麼就會產生多個加強 score\n因此 ES 會先使用 score_mode 定義的方式來合併這些加強 score 們，得到一個總加強 score，得到總加強 score之後，才會再使用 boost_mode 定義的方式去和 old_score 做合併\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} //match_all查出來的所有文檔的_score都是1 }, \u0026#34;functions\u0026#34;: [ //第一個filter(使用weight加強函數)，如果language是java，加強score就是2 { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34; } }, \u0026#34;weight\u0026#34;: 2 }, //第二個filter(使用weight加強函數)，如果language是go，加強score就是3 { \u0026#34;filter\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34; } }, \u0026#34;weight\u0026#34;: 3 }, //第三個filter(使用weight加強函數)，如果like數大於等於10，加強score就是5 { \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;like\u0026#34;: { \u0026#34;gte\u0026#34;: 10 } } }, \u0026#34;weight\u0026#34;: 5 }, //field_value_factor加強函數，會應用到所有文檔上，加強score就是like值 { \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;like\u0026#34; } } ], \u0026#34;score_mode\u0026#34;: \u0026#34;multiply\u0026#34;, //設置functions裡面的加強score們怎麼合併成一個總加強score \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34; //設置old_score怎麼和總加強score合併 } } } \u0026#34;hits\u0026#34;: [ { //go同時滿足filter2、filter3 //且還有一個加強函數field_value_factor產生的加強 //因此加強score為3, 5, 10，總加強score為3*5*10=150 \u0026#34;_score\u0026#34;: 150, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 10 } }, { //java只滿足filter1 //但是因為還有field_value_facotr產生的加強score //因此加強score為2, 5，總加強score為2*5=10 \u0026#34;_score\u0026#34;: 10, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { //python不滿足任何filter //因此加強score只有field_value_factor的like值 //就是5 \u0026#34;_score\u0026#34;: 5, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 5 } } ] 其實 weight 加強函數也是可以不和 filter 搭配，自己單獨使用的，只是這樣做沒啥意義，因為只是會給全部的文檔都增加一個固定值而已\n不過就 DSL 語法上來說，他也像其他加強函數一樣，是可以直接使用而不用加 filter 的\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match_all\u0026#34;: {} } }, functions: [ { \u0026#34;weight\u0026#34;: 3 } ] } } \u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 3, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;go\u0026#34;, \u0026#34;like\u0026#34;: 10 } }, { \u0026#34;_score\u0026#34;: 3, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;python\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 3, \u0026#34;_source\u0026#34;: { \u0026#34;language\u0026#34;: \u0026#34;java\u0026#34;, \u0026#34;like\u0026#34;: 5 } } ] weight 加強函數也可以用來調整每個語句的貢獻度，權重 weight 的默認值是 1.0，當設置了 weight，這個 weight 值會先和自己那個 {} 裡的每個句子的評分相乘，之後再通過 score_mode 和其他加強函數合併\n下面的查詢，公式為 new_score = old_score * [ (like值 * weight1) + weight2 ] 公式解析 : weight1 先加強 like 值（只能使用乘法），接著再透過 score_mode 定義的方法（sum）和另一個加強函數 weight2 合併，得到一個總加強 score，最後再使用 boost_mode 定義的方法（默認是 multiply）和 old_score 做合併，得到 new_score\nGET mytest/doc/_search { \u0026#34","date":"2018-07-06","objectID":"da4e2b503c1484ed0a654df41c72baa4","title":"ElasticSearch - function_score（weight 具體實例）","url":"https://kucw.io/blog/2018/7/elasticsearch-function_score-weight/"},{"categories":["Elastic Search"],"content":" 閱讀本文需要先了解 function_score 的相關知識，請看 ElasticSearch - function_score 簡介\n首先準備數據和索引，在ES插入三筆數據，其中 title 是 text 類型，like 是 integer 類型（代表點贊量）\n{ \u0026#34;title\u0026#34;: \u0026#34;ES 入門\u0026#34;, \u0026#34;like\u0026#34;: 2 } { \u0026#34;title\u0026#34;: \u0026#34;ES 進階\u0026#34;, \u0026#34;like\u0026#34;: 5 } { \u0026#34;title\u0026#34;: \u0026#34;ES 最高難度\u0026#34;, \u0026#34;like\u0026#34;: 10 } 先使用一般的 query，查看普通的查詢的評分會是如何\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES\u0026#34; } } } \u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 0.2876821, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 入門\u0026#34;, \u0026#34;like\u0026#34;: 2 } }, { \u0026#34;_score\u0026#34;: 0.20309238, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 進階\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 0.16540512, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 最高難度\u0026#34;, \u0026#34;like\u0026#34;: 10 } } ] 使用 function_score 的 field_value_factor 改變 _score，將 old_score 乘上 like 的值\n本來 \u0026ldquo;ES 最高難度\u0026rdquo; 的 score 是0.16540512，經過 field_value_factor 的改變，乘上了那個文檔中的like值（10）之後，新的 score 變為 1.6540513\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES\u0026#34; } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;like\u0026#34; } } } } \u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 1.6540513, //原本是0.16540512 \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 最高難度\u0026#34;, \u0026#34;like\u0026#34;: 10 } }, { \u0026#34;_score\u0026#34;: 1.0154619, //原本是0.20309238 \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 進階\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 0.5753642, //原本是0.2876821 \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 入門\u0026#34;, \u0026#34;like\u0026#34;: 2 } } ] 加上 max_boost，限制 field_value_factor 的最大加強 score\n可以看到 ES 入門的加強 score 是2，在 max_boost 限制裡，所以不受影響\n而 ES 進階和 ES 最高難度的 field_value_factor 函數產生的加強 score 因為超過 max_boost 的限制，所以被設為 3\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES\u0026#34; } }, \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;like\u0026#34; }, \u0026#34;max_boost\u0026#34;: 3 } } } \u0026#34;hits\u0026#34;: [ { \u0026#34;_score\u0026#34;: 0.6092771, //原本是0.20309238 \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 進階\u0026#34;, \u0026#34;like\u0026#34;: 5 } }, { \u0026#34;_score\u0026#34;: 0.5753642, //原本是0.2876821 \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 入門\u0026#34;, \u0026#34;like\u0026#34;: 2 } }, { \u0026#34;_score\u0026#34;: 0.49621537, //原本是0.16540512 \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;ES 最高難度\u0026#34;, \u0026#34;like\u0026#34;: 10 } } ] 有時候線性的計算 new_score = old_score * like值 的效果並不是那麼好，field_value_factor 中還支持 modifier、factor 參數，可以改變 like 值對 old_score 的影響\nmodifier 參數支持的值\nnone : new_score = old_score * like值 默認狀態就是 none，線性 log1p : new_score = old_score * log(1 + like值) 最常用，可以讓 like 值字段的評分曲線更平滑 log2p : new_score = old_score * log(2 + like值) ln : new_score = old_score * ln(like值) ln1p : new_score = old_score * ln(1 + like值) ln2p : new_score = old_score * ln(2 + like值) square : 計算平方 sqrt : 計算平方根 reciprocal : 計算倒數 factor 參數\nfactor 作為一個調節用的參數，沒有 modifier 那麼強大會改變整個曲線，他僅改變一些常量值，設置 factor \u0026gt; 1 會提昇效果，factor \u0026lt; 1 會降低效果 假設 modifier 是 log1p，那麼加入了 factor 的公式就是 new_score = old_score * log(1 + factor * like值) 對剛剛的例子加上 modifier、factor\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#3","date":"2018-07-05","objectID":"a484ab21c7f671c6c54ed0326066f0a3","title":"ElasticSearch - function_score（field_value_factor 具體實例）","url":"https://kucw.io/blog/2018/7/elasticsearch-function_score-field_value_factor/"},{"categories":["Elastic Search"],"content":" function_score 內容較多，此篇主要是介紹 function_score 的基本概念\n具體實例請參考以下連接\nElasticSearch - function_score（field_value_factor 具體實例）\nElasticSearch - function_score（weight 具體實例）\nElasticSearch - function_score（random_score 具體實例）\nElasticSearch - function_score（衰減函數 linear、exp、gauss 具體實例）\n在使用 ES 進行全文搜索時，搜索結果默認會以文檔的相關度進行排序，而這個 \u0026ldquo;文檔的相關度\u0026rdquo;，是可以透過 function_score 自己定義的，也就是說我們可以透過使用 function_score，來控制 \u0026ldquo;怎麼樣的文檔相關度更高\u0026rdquo; 這件事\nfunction_score 是專門用於處理文檔 _score 的 DSL，它允許爲每個主查詢 query 匹配的文檔應用加強函數， 以達到改變原始查詢評分 score 的目的 function_score 會在主查詢 query 結束後對每一個匹配的文檔進行一系列的重打分操作，能夠對多個字段一起進行綜合評估，且能夠使用 filter 將結果劃分爲多個子集（每個特性一個filter），並爲每個子集使用不同的加強函數 function_score 提供了幾種加強 _score 計算的函數\nweight : 設置一個簡單而不被規範化的權重提升值 weight 加強函數 和 boost 參數 類似，可以用於任何查詢，不過有一點差別是 weight 不會被 Lucene normalize 成難以理解的浮點數，而是直接被應用（boost 會被 normalize） 例如當 weight 爲 2 時，最終結果爲 new_score = old_score * 2 field_value_factor : 將某個字段的值乘上 old_score 像是將 字段 shareCount 或是 字段 likeCount 作爲考慮因素，new_score = old_score * 那個文檔的 likeCount 的值 random_score : 爲每個用戶都使用一個不同的隨機評分對結果排序，但對某一具體用戶來說，看到的順序始終是一致的 衰減函數 (linear、exp、guass) : 以某個字段的值為基準，距離某個值越近得分越高 script_score : 當需求超出以上範圍時，可以用自定義腳本完全控制評分計算，不過因為還要額外維護腳本不好維護，因此盡量使用 ES 提供的評分函數，需求真的無法滿足再使用 script_score function_scroe其他輔助的參數\nboost_mode : 決定 old_score 和 加強score 如何合併 multiply（默認） : new_score = old_score * 加強score sum : new_score = old_score + 加強score min : old_score 和 加強 score 取較小值，new_score = min(old_score, 加強score) max : old_score 和 加強 score 取較大值，new_score = max(old_score, 加強score) replace : 加強 score 直接替換掉 old_score，new_score = 加強score score_mode : 決定 functions 裡面的加強 score 們怎麼合併，會先合併加強 score 們成一個總加強 score，再使用總加強 score 去和 old_score 做合併，換言之就是會先執行 score_mode，再執行 boost_mode multiply（默認） sum avg first : 使用首個函數（可以有filter，也可以沒有）的結果作為最終結果 max min max_boost : 限制加強函數的最大效果，就是限制加強 score 最大能多少，但要注意不會限制 old_score 如果加強 score 超過了 max_boost 限制的值，會把加強 score 的值設成 max_boost 的值 假設加強 score 是5，而 max_boost 是2，因為加強 score 超出了 max_boost 的限制，所以 max_boost 就會把加強 score 改為2 簡單的說，就是 加強score = min(加強score, max_boost) function_score 查詢模板\n如果要使用 function_score 改變分數，要使用 function_score 查詢\n簡單的說，就是在一個 function_score 內部的 query 的全文搜索得到的 _score 基礎上，給他加上其他字段的評分標準，就能夠得到把 \u0026ldquo;全文搜索 + 其他字段\u0026rdquo; 綜合起來評分的效果\n單個加強函數的查詢模板\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { //主查詢，查詢完後這裡自己會有一個評分，就是 old_score \u0026#34;query\u0026#34;: {.....}, //在 old_score 的基礎上，給他加強其他字段的評分 //這裡會產生一個加強 score，如果只有一個加強 function 時，直接將加強函數名寫在 query 下面就可以了 \u0026#34;field_value_factor\u0026#34;: {...}, //指定用哪種方式結合 old_score 和加強 score 成為 new_score \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34;, //限制加強 score 的最高分，但是不會限制 old_score \u0026#34;max_boost\u0026#34;: 1.5 } } } 多個加強函數的查詢模板\n如果有多個加強函數，那就要使用functions來包含這些加強函數們，functions是一個數組，裡面放著的是將要被使用的加強函數列表\n可以為functions裡的加強函數指定一個filter，這樣做的話，只有在文檔滿足此filter的要求，此filter的加強函數才會應用到文擋上，也可以不指定filter，這樣的話此加強函數就會應用到全部的文擋上\n一個文檔可以一次滿足多條加強函數和多個filter，如果一次滿足多個，那麼就會產生多個加強score，因此ES會使用score_mode定義的方式來合併這些加強score們，得到一個總加強score，得到總加強score之後，才會再使用boost_mode定義的方式去和old_score做合併\n像是下面的例子，field_value_factor和gauss這兩個加強函數會應用到所有文檔上，而weight只會應用到滿足filter的文檔上，假設有個文檔滿足了filter的條件，那他就會得到3個加強score，這3個加強score會使用sum的方式合併成一個總加強score，然後才和old_score使用multiply的方式合併\nGET mytest/doc/_search { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { //主查詢，查詢完後這裡自己會有一個評分，就是 ol","date":"2018-07-04","objectID":"888dbbd2d9809b99f59dc306c461355b","title":"ElasticSearch - function_score 簡介","url":"https://kucw.io/blog/2018/7/elasticsearch-function_score/"},{"categories":["Elastic Search"],"content":" ES 為了避免深分頁，不允許使用分頁（from \u0026amp; size）查詢 10000 條以後的數據，因此如果要查詢第 10000 條以後的數據，要使用 ES 提供的 scroll 游標 來查詢\n原因是因為假設取的頁數較大時（深分頁），如請求第 20 頁，ES 不得不取出所有分片上的第 1 頁到第 20 頁的所有文檔，並做排序，最終再取出 from 後的 size 條結果作爲最終的返回值 假設你有 16 個分片，則需要在 coordinate node 彙總到 shards * (from + size) 條記錄，即需要 16 * (20 + 10) 記錄後做一次全局排序 所以，當索引非常非常大(千萬或億)，是無法使用 from + size 做深分頁的，分頁越深則越容易 Out Of Memory，即使你運氣很好沒有發生 Out Of Memory，也會非常消耗 CPU 和內存資源 因此 ES 使用 index.max_result_window:10000 作爲保護措施 ，即默認 from + size 不能超過 10000，雖然這個參數可以動態修改，也可以在配置文件配置，但是最好不要這麼做，應該改用 ES 提供的 scroll 方法來取得數據 scroll 游標原理\n可以把 scroll 理解爲關係型數據庫裏的 cursor，因此，scroll 並不適合用來做實時搜索，而更適用於後台批處理任務，比如群發 scroll 具體分爲初始化和遍歷兩步 初始化時將所有符合搜索條件的搜索結果緩存起來，可以想象成快照 在遍歷時，從這個快照裏取數據 也就是說，在初始化後對索引插入、刪除、更新數據都不會影響遍歷結果 游標可以增加性能的原因，是因為如果做深分頁，每次搜索都必須重新排序，非常浪費，使用 scroll 就是一次把要用的數據都排完了，分批取出，因此比使用 from + size 還好 具體實例\n初始化\n請求\n注意要在URL中的search後加上 scroll=1m，不能寫在 request body 中，其中 1m 表示這個游標要保持開啟 1 分鐘\n可以指定 size 大小，就是每次回傳幾筆數據，當回傳到沒有數據時，仍會返回 200 成功，只是 hits 裡的 hits 會是空 list\n在初始化時除了回傳 _scroll_id，也會回傳前 100 筆（假設 size = 100）的數據\nrequest body 和一般搜索一樣，因此可以說在初始化的過程中，除了加上 scroll 設置游標開啟時間之外，其他的都跟一般的搜尋沒有兩樣（要設置查詢條件，也會回傳前 size 筆的數據）\nGET my_index/_search?scroll=1m { \u0026#34;query\u0026#34;:{ \u0026#34;range\u0026#34;:{ \u0026#34;createTime\u0026#34;: { \u0026#34;gte\u0026#34;: 1522229999999 } } }, \u0026#34;size\u0026#34;: 1000 } 返回結果\n{ \u0026#34;_scroll_id\u0026#34;: \u0026#34;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAfv5-FjNOamF0Mk1aUUhpUnU5ZWNMaHJocWcAAAAAAH7-gBYzTmphdDJNWlFIaVJ1OWVjTGhyaHFnAAAAAAB-_n8WM05qYXQyTVpRSGlSdTllY0xocmhxZwAAAAAAdsJxFmVkZTBJalJWUmp5UmI3V0FYc2lQbVEAAAAAAHbCcBZlZGUwSWpSVlJqeVJiN1dBWHNpUG1R\u0026#34;, \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 5, \u0026#34;successful\u0026#34;: 5, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 84, \u0026#34;max_score\u0026#34;: 1, \u0026#34;hits\u0026#34;: [ { \u0026#34;_index\u0026#34;: \u0026#34;video1522821719\u0026#34;, \u0026#34;_type\u0026#34;: \u0026#34;doc\u0026#34;, \u0026#34;_id\u0026#34;: \u0026#34;84056\u0026#34;, \u0026#34;_score\u0026#34;: 1, \u0026#34;_source\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;三个院子\u0026#34;, \u0026#34;createTime\u0026#34;: 1522239744000 } } ....99 data ] } } 遍歷數據\n請求\n使用初始化返回的 _scroll_id 來進行請求，每一次請求都會繼續返回初始化中未讀完數據，並且會返回一個 _scroll_id，這個 _scroll_id 可能會改變，因此每一次請求應該帶上上一次請求返回的 _scroll_id\n要注意返回的是 _scroll_id，但是放在請求裡的是 scroll_id，兩者拼寫上有不同\n且每次發送 scroll 請求時，都要再重新刷新這個 scroll 的開啟時間，以防不小心超時導致數據取得不完整\nGET _search/scroll?scroll=1m { \u0026#34;scroll_id\u0026#34;: \u0026#34;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAdsMqFmVkZTBJalJWUmp5UmI3V0FYc2lQbVEAAAAAAHbDKRZlZGUwSWpSVlJqeVJiN1dBWHNpUG1RAAAAAABpX2sWclBEekhiRVpSRktHWXFudnVaQ3dIQQAAAAAAaV9qFnJQRHpIYkVaUkZLR1lxbnZ1WkN3SEEAAAAAAGlfaRZyUER6SGJFWlJGS0dZcW52dVpDd0hB\u0026#34; } 返回結果\n如果沒有數據了，就會回傳空的 hits，可以用這個判斷是否遍歷完成了數據\n{ \u0026#34;_scroll_id\u0026#34;: \u0026#34;DnF1ZXJ5VGhlbkZldGNoBQAAAAAAdsMqFmVkZTBJalJWUmp5UmI3V0FYc2lQbVEAAAAAAHbDKRZlZGUwSWpSVlJqeVJiN1dBWHNpUG1RAAAAAABpX2sWclBEekhiRVpSRktHWXFudnVaQ3dIQQAAAAAAaV9qFnJQRHpIYkVaUkZLR1lxbnZ1WkN3SEEAAAAAAGlfaRZyUER6SGJFWlJGS0dZcW52dVpDd0hB\u0026#34;, \u0026#34;took\u0026#34;: 2, \u0026#34;timed_out\u0026#34;: false, \u0026#34;_shards\u0026#34;: { \u0026#34;total\u0026#34;: 5, \u0026#34;successful\u0026#34;: 5, \u0026#34;skipped\u0026#34;: 0, \u0026#34;failed\u0026#34;: 0 }, \u0026#34;hits\u0026#34;: { \u0026#34;total\u0026#34;: 84, \u0026#34;max_score\u0026#34;: null, \u0026#34;hits\u0026#34;: [] } } 優化scroll查詢\n在一般場景下，scroll 通常用來取得需要排序過後的大筆數據，但是有時候數據之間的排序性對我們而言是沒有關係的，只要所有數據都能取出來就好，這時能夠對 scroll 進行優化\n初始化\n使用 _doc","date":"2018-06-26","objectID":"52ac56105de41a22c86f3aa0e662d094","title":"ElasticSearch - 解決 ES 的深分頁問題（游標 scroll）","url":"https://kucw.io/blog/2018/6/elasticsearch-scroll/"},{"categories":["Elastic Search"],"content":" 由於在 ES 中，所有單個文檔的增刪改都是原子性的操作，因此將相關的實體數據都儲存在同一個文檔是很好的，且由於所有信息都在一個文檔中，因此當我們查詢時就沒有必要像 mysql 一樣去關聯很多張表，只要搜一遍文檔就可以查出所有需要的數據，查詢效率非常高\n因此除了基本數據類型之外，ES 也支持使用複雜的數據類型，像是數組、內部對象，而要使用內部對象的話，需要使用 nested 來定義索引，使文檔內可以包含一個內部對象\n為什麼不用 object 而要使用 nested 來定義索引的原因是，object 類型會使得內部對象的關聯性丟失\n這是因為 Lucene 底層其實沒有內部對象的概念，所以 ES 會利用簡單的列表儲存字段名和值，將 object 類型的對象層次攤平，再傳給 Lucene\n假設 user 類型是 object，當插入一筆新的數據時，ES 會將他轉換為下面的內部文檔，其中可以看見 alice 和 white 的關聯性丟失了\nPUT mytest/doc/1 { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34;, \u0026#34;user\u0026#34;: [ { \u0026#34;first\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Smith\u0026#34; }, { \u0026#34;first\u0026#34;: \u0026#34;Alice\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;White\u0026#34; } ] } 轉換後的內部文檔 { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34;, \u0026#34;user.first\u0026#34;: [ \u0026#34;alice\u0026#34;, \u0026#34;john\u0026#34; ], \u0026#34;user.last\u0026#34;: [ \u0026#34;smith\u0026#34;, \u0026#34;white\u0026#34; ] } 理論上從插入的數據來看，應該搜索 \u0026ldquo;first 為 Alice 且 last 為 White\u0026rdquo; 時，這個文檔才算符合條件被搜出來，其他的條件都不算符合，但是因為 ES 把 object 類型的對象攤平了，所以實際上如果搜索 \u0026ldquo;first 為 Alice 且 last 為 Smith\u0026rdquo;，這個文檔也會當作符合的文檔被搜出來，但這樣就違反我們的意願了，我們希望內部對象自己的關聯性還是存在的\n因此在使用內部對象時，要改使用 nested 類型來取代 object 類型 (因為 nested 類型不會被攤平，下面說明)\nnested 類型就是為了解決 object 類型在對象數組上丟失關聯性的問題的，如果將字段設置為 nested 類型，那個每一個嵌套對象都會被索引為一個 \u0026ldquo;隱藏的獨立文檔\u0026rdquo;\n其本質上就是將數組中的每個對象作為分離出來的隱藏文檔進行索引，因此這也意味著每個嵌套對象可以獨立於其他對象被查詢\n假設將上面的例子的 user 改為 nested 類型，經過 ES 轉換後的文檔如下\n//嵌套文檔1 { \u0026#34;user.first\u0026#34;: [ \u0026#34;alice\u0026#34; ], \u0026#34;user.last\u0026#34;: [ \u0026#34;white\u0026#34; ] } //嵌套文檔2 { \u0026#34;user.first\u0026#34;: [ \u0026#34;john\u0026#34; ], \u0026#34;user.last\u0026#34;: [ \u0026#34;smith\u0026#34; ] } //根文檔，或者也可以稱為父文檔 { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34; } 在獨立索引每一個嵌套對象後，對象中每個字段的相關性得以保留，因此我們查詢時，也僅返回那些真正符合條件的文檔\n不僅如此，由於嵌套文檔直接儲存在文檔內部，因此查詢時嵌套文檔和根文檔的聯合成本很低，速度和單獨儲存幾乎一樣\n但是要注意，查詢的時候返回的是整個文檔，而不是嵌套文檔本身，並且如果要增刪改一個嵌套對象，必須把整個文檔重新索引才可以\n具體實例\n索引準備\n定義一個 nested 類型的 mapping，user 是一個內部對象，裡面包含了 first、last 和 age，因為 user 設置了 nested 類型，因此 user 對象會被索引在獨立的嵌套文檔中\nPUT mytest { \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { //group是正常的keyword字段 \u0026#34;group\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, //user是一個nested類型，表示他底下還會包含子對象 \u0026#34;user\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;nested\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;first\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;last\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;age\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; } } } } } } } 插入兩筆數據\nPOST mytest/doc { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;Taylor\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Swift\u0026#34;, \u0026#34;age\u0026#34;: 30 } } POST mytest/doc { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34;, \u0026#34;user\u0026#34;: [ { \u0026#34;first\u0026#34;: \u0026#34;Amy\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;White\u0026#34;, \u0026#34;age\u0026#34;: 18 }, { \u0026#34;first\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34;: 22 } ] } 因此在ES中存在的文檔如下\n\u0026#34;hits\u0026#34;: [ { \u0026#34;_source\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34;, \u0026#34;user\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;Taylor\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Swift\u0026#34;, \u0026#34;age\u0026#34;: 30 } } }, { \u0026#34;_source\u0026#34;: { \u0026#34;group\u0026#34;: \u0026#34;fans\u0026#34;, \u0026#34;user\u0026#34;: [ { \u0026#34;first\u0026#34;: \u0026#34;Amy\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;White\u0026#34;, \u0026#34;age\u0026#34;: 18 }, { \u0026#34;first\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;age\u0026#34;: 22 } ] } } ] 嵌套對象查詢 nested\n由於嵌套對象被索引在獨立的隱藏文檔中，因此我們無法直接使用一般的 query 去查詢他，我們必須改使用 \u0026ldquo;nested查詢\u0026r","date":"2018-06-22","objectID":"564da7aa802946689d07487ae1b60f02","title":"ElasticSearch - 嵌套對象 nested","url":"https://kucw.io/blog/2018/6/elasticsearch-nested/"},{"categories":["Elastic Search"],"content":" ES 中的 term 和 match 牽扯到了分詞器、mapping、倒排索引等，如果不熟悉相關知識，請先看 ElasticSearch - index mapping（5.x以上） 這篇文章 term 是直接把 field 拿去查詢倒排索引中確切的 term match 會先對 field 進行分詞操作，然後再去倒排索引中查詢 具體實例 假設有一個字段 nickname，存放的類型是 text，因此當新增一筆文檔時，內容會被分詞器分詞，然後才儲存進倒排索引 假設插入了一筆文檔，其中 \u0026quot;nickname\u0026quot;: \u0026quot;1 hello\u0026quot;，分詞過後變為 1、hello，因此倒排索引中儲存的是兩筆索引 1 和 hello，而不是一筆索引 1 hello 使用 match 做查詢 \u0026quot;match\u0026quot;: \u0026quot;1-hello\u0026quot; : 成功，因為 match 把 1-hello 分詞成 1、hello，而 1 和 hello 都存在在倒排索引中，所以不只能夠查到，_score 還很高 \u0026quot;match\u0026quot;: \u0026quot;1\u0026quot; : 成功，1 被分詞過後還是 1，而 1 存在在倒排索引中 \u0026quot;match\u0026quot;: \u0026quot;hello how r u\u0026quot; : 成功，match 把 hello how r u 分詞成 hello、how、r、u，而 hello 存在在倒排索引中 使用 term 做查詢 \u0026quot;term\u0026quot;: \u0026quot;2222-hello : 失敗，倒排索引只有 1、hello，並沒有 2222-hello \u0026quot;term\u0026quot;: \u0026quot;hello\u0026quot; : 成功，因為倒排索引中有 hello \u0026quot;term\u0026quot;: \u0026quot;1 hello\u0026quot; : 失敗，雖然 1 hello 和我們當時插入的數據一模一樣，但是因為倒排索引在建立索引時把原始的數據分詞了才儲存進索引，裡面存的是 1 和 hello，並沒有存放 1 hello，因此查詢失敗 ","date":"2018-06-20","objectID":"a8dc66b6c959cdc18053669f36564c1d","title":"ElasticSearch - term 和 match 的差別","url":"https://kucw.io/blog/2018/6/elasticsearch-term-match/"},{"categories":["Java"],"content":" Java 基本內置 annotation\n@Override\n@Override用在方法上，表示這個方法重寫了父方法，如toString() 如果父方法沒有這個方法，那麼就無法編譯過 如果實現接口，需要在每個實現方法都加上@Override，說明這是要實現那個接口的方法，而不是自己新創的方法 @Deprecated\n@Deprecated 表示這個方法已經過期，不建議開發者使用 暗示在將來某個不確定的版本，就有可能會被取消掉 @SuppressWarnings\n@SuppressWarnings是抑制警告的意思，這個注解主要的用處就是忽略警告信息\n常見的警告值\ndeprecation : 使用了不贊成使用的類或方法時的警告 unused : 某個變量被定義，但是沒有被使用 path : 在類路徑、源文件路徑等中有不存在的路徑時的警告 具體實例\npublic class Test { //定義了一個不建議使用了方法 @Deprecated public void sayHello() { System.out.println(\u0026#34;Hello\u0026#34;); } //聲明以下的方法，自動忽略deprecation和unused的警告，不要在console上報出warn //以下的方法使用了被Deprecated的方法sayHello //也沒有使用到一個被定義的變量 i //但是因為設置了警告忽略所以不會有warning @SuppressWarnings({\u0026#34;deprecation\u0026#34;, \u0026#34;unused\u0026#34;}) public static void main(String[] args) { int i; Test test = new Test(); test.sayHello(); } } @FunctionallInterface\nJava 1.8新增的注解，用於約定函數式接口\n函數式接口存在的意義，主要是配合Lambda表達式來使用\n如果接口中只允許有一個public的抽象方法，該接口就稱為函數式接口 (不過此接口可以包含多個default方法或是多個static方法) 能夠包含多個default和static方法的原因是因為在使用函數式編程時，為了能夠使用lambda表達式，所以每個接口的實現類只允許實現一個方法 而default方法和static方法不允許實現類實現，所以就不會影響lambda表達式，因此就可以存在在函數式接口裡 //匿名實現類 new Function\u0026lt;Integer, String\u0026gt; 實現了Function接口 List\u0026lt;Integer\u0026gt; intList = Lists.newArrayList(1, 2, 3); List\u0026lt;String\u0026gt; stringList = Lists.transform(intList, new com.google.common.base.Function\u0026lt;Integer, String\u0026gt;() { @Override public String apply(Integer input) { return input + \u0026#34;aa\u0026#34;; } }); //可以轉換成下面的lambda表達式 List\u0026lt;Integer\u0026gt; intList = Lists.newArrayList(1, 2, 3); List\u0026lt;String\u0026gt; stringList = Lists.transform(intList, input -\u0026gt; input + \u0026#34;aa\u0026#34;); 使用 @FunctionallInterface 自定義函數式接口\n//自定義一個函數式接口，並且只有一個public的抽象方法 @FunctionalInterface public interface MyFunction { public String myApply(); } //定義一個類，讓這個類去使用 MyFunction public class Hello { void say(MyFunction myFunction) { System.out.println(myFunction.myApply()); } } public class MainTest { public static void main(String[] args) { Hello hello = new Hello(); //輸出 Hello World hello.say(new MyFunction() { @Override public String myApply() { return \u0026#34;Hello World\u0026#34;; } }); //可以轉換為以下的lambda表達式，同樣輸出 Hello World hello.say(() -\u0026gt; \u0026#34;Hello World\u0026#34;); } } 自定義新的 annotation\n使用元注解(meta annotation)來設定一個注解的作用，可以說他是 \u0026ldquo;自定義注解\u0026rdquo; 的注解\n元注解的種類\n@Target : 表示這個注解可以放在什麼位置上，也就是可以修飾 ElementType.TYPE : 能修飾類、接口、枚舉、注解 ElementType.FIELD : 能修飾字段、枚舉的常量 ElementType.METHOD : 能修飾方法 ElementType.PARAMETER : 能修飾方法參數 ElementType.CONSTRUCTOR : 能修飾構造函數 ElementType.LOCAL_VARIABLE : 能修飾局部變量 ElementType.ANNOTATION_TYPE : 能修飾注解 (元注解就是此種) ElementType.PACKAGE : 能修飾包 @Retention : 表示這個注解的生命週期 可選的值有三種 RetentionPolicy.SOURCE 表示此注解只在源代碼中有效果，不會編譯進class文件 @Override就是這種注解 RetentionPolicy.CLASS 表示此注解除了在源代碼有效果，也會編譯進class文件，但是在運行期是無效果的 @Retention的默認值，即是當沒有指定@Retention的時候，就會是這種類型 RetentionPolicy.RUNTIME 表示此注解從源代碼到運行期一直存在 程序可以透過反射獲取這個注解的信息 @Inherited : 表示該注解有繼承性，即是子類可以拿到繼承父類上的注解信息 @Documetned : 在用javadoc命令生成API文檔後，文檔裡會出現該注解說明 @Repeatable (java1.8新增) 當沒有使用@Repeatable修飾的時候，注解在同一個位置只能出現一次，如果寫重複的兩次就會報錯 但是使用@Repeatable之後，就能夠在同一個地方使用多次 使用@Repeatable的時機通常在想要取得一組資訊時 像是一個User裡面可能有id、name屬性，我們想要以User為單位來取 但是因為注解裡面無法使用自定義對象 所以只能透過@Repeatable這種方法來達成目標 注解參數的數據類型\n支持的類型 所有基本數據類型 (int, float, boolean, byte, double, char, long, short) String類型 Class類型 enum類型 Annotation類型 以上所有類型的數組 不支","date":"2018-06-11","objectID":"324743321e2b73965ca72d956eeb80d0","title":"Java - annotation 的使用","url":"https://kucw.io/blog/2018/6/java-annotation/"},{"categories":["Elastic Search"],"content":" ElasticSearch 的 analysis 實際上是將三個功能封裝在一起，這三個功能按照順序執行，而這三個功能都是能自定義的 字符過濾器 (char_filter) 首先，字符串按順序通過每個字符過濾器，他們的任務是在分詞前整理字符串 一個字符過濾器可以用來去掉HTML，或者將\u0026amp;轉化成and 分詞器 (tokenizer) 其次，字符串被分詞器分爲單個的詞條，一個簡單的分詞器遇到空格和標點的時候，可能會將文本拆分成詞條 Hello how are you?會被ES預設的分詞器standard分成hello、how、are、you Token 過濾器 (filter) 最後，詞條按順序通過每個 token 過濾器，這個過程可能會改變詞條(Quick -\u0026gt; quick)、刪除詞條(a、an、and、the\u0026hellip;)、增加詞條(jump和leap這種同義詞) 自定義 analysis 格式 PUT mytest { \u0026#34;setting\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { 自定義的字符過濾器 }, \u0026#34;tokenizer\u0026#34;: { 自定義的分詞器 }, \u0026#34;filter\u0026#34;: { 自定義的token過濾器 }, \u0026#34;analyzer\u0026#34;: { 自定義的分析器，可以將上面的char_filter、tokenizer、filter用不同的組合拼起來，形成不同的分析器 } } } } 具體實例 PUT mytest { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;\u0026amp;_to_and\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [\u0026#34;\u0026amp;=\u0026gt; and \u0026#34;] }, \u0026#34;xxx\u0026#34;: {....}, \u0026#34;yyy\u0026#34;: {....} }, \u0026#34;filter\u0026#34;: { \u0026#34;my_stopwords\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: [\u0026#34;the\u0026#34;, \u0026#34;a\u0026#34;] } }, \u0026#34;analyzer\u0026#34;: { //自定義分析器，將想要的char_filter、tokenizer、filter給加載進來 //數組順序很重要，因為是照順序執行，先執行htmp_strip，再執行\u0026amp;_to_and，然後才去執行tokenizer \u0026#34;my_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;htmp_strip\u0026#34;, \u0026#34;\u0026amp;_to_and\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [\u0026#34;lowercase\u0026#34;, \u0026#34;my_stopwords\u0026#34;] } } } }, \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;nickname\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;my_analyzer\u0026#34; //使用自定義的分析器 } } } } } 控制分析器 search_analyzer、analyzer 分析器主要有兩種情況會被使用，一種是插入文檔時，將text類型的字段做分詞然後插入倒排索引，第二種就是在查詢時，先對要查詢的text類型的輸入做分詞，再去倒排索引搜索\n如果想要讓 索引 和 查詢 時使用不同的分詞器，ElasticSearch也是能支持的，只需要在字段上加上search_analyzer參數\n在索引時，只會去看字段有沒有定義analyzer，有定義的話就用定義的，沒定義就用ES預設的 在查詢時，會先去看字段有沒有定義search_analyzer，如果沒有定義，就去看有沒有analyzer，再沒有定義，才會去使用ES預設的 具體實例\nPUT mytest { \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;nickname\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, //索引時使用standard分詞器 \u0026#34;search_analyzer\u0026#34;: \u0026#34;simple\u0026#34; //查詢時使用simple分詞器 }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; //索引和查詢都使用standard分詞器 } } } } } ","date":"2018-06-09","objectID":"ba5fcbfaafba32a55f95f60c0714df03","title":"ElasticSearch - 自定義 analysis","url":"https://kucw.io/blog/2018/6/elasticsearch-analysis/"},{"categories":["Elastic Search"],"content":" Elasticsearch支持的基本類型\n字符串 : text, keyword text : 存儲數據的時候，會自動分詞，並生成索引 keyword : 存儲數據的時候，不會分詞，而是直接整個詞拿去建索引 整數 : byte, short, integer, long 浮點數 : float, double 布爾型 : boolean 日期 : date 自定義 mapping\nindex : 設置此字段能不能被查詢，就是決定要不要將這個字段放進倒排索引裡\n若 index 設置為 true（默認是 true），則表示這個這個字段會被放進倒排索引裡，如果是 text 就是分詞過後放進索引，如果是 keyword、integer\u0026hellip;就直接整段放進索引裡 若 index 設置為 false，則表示這個字段不放進倒排索引裡，因此不能查詢這個字段（因為他不存在於倒排索引裡） 通常這種被設成 false 的字段，可以想像成是屬於一種附屬的字段，就是不能被 match、term 查詢，但是當該文檔被其他搜索條件搜出來時，他可以附帶的一起被找出來，因為他們同屬於同一個文檔 analyzer : 主要用在 text 類型的字段上，就是設定要使用哪種分詞器來建立索引\n可以使用內建的分詞器，或是使用自定義的分詞器 可以使用 /_analyze 測試分析器具體會將句子分詞成什麼樣子，它能幫助我們理解 Elasticsearch 索引內部發生了什麼\nGET _analyze { \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Text to analyze\u0026#34; } 具體實例\nmapping\nPUT mytest { \u0026#34;mappings\u0026#34;: { \u0026#34;doc\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;index\u0026#34;: false }, \u0026#34;uid\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34; }, \u0026#34;nickname\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } } } } 搜索 uid 時，name 會一起被找出來\nGET mytest/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;uid\u0026#34;: 1 } } } { \u0026#34;uid\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;1-hello\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;1-nickname\u0026#34; } 搜索 name，會報 error\nGET mytest/_search { \u0026#34;query\u0026#34;: { \u0026#34;term\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;1-hello\u0026#34; } } } \u0026#34;error\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;illegal_argument_exception\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;Cannot search on field [name] since it is not indexed.\u0026#34; } 更新 mapping\n當首次創建一個 index 的時候，可以指定類型的 mapping，但假設後來想要增加一個新的映射字段，可以使用 /_mapping 把新的字段加進 mapping 映射裡\n可以增加一個新的映射，但是不能修改存在的映射，原因是因為這個映射可能有文檔去用，如果改了映射的類型，可能會導致 index 的數據出錯，因此只能新加字段進去，不能修改 具體實例\n在 mytest 映射中的 doc 類型增加一個新的名爲 tag 的 keyword\nPUT mytest/_mapping/doc { \u0026#34;properties\u0026#34;: { \u0026#34;tag\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } 在 ES 中更新 mapping（新增一個字段）會產生的問題\n雖然 ES 支持更新映射在 mapping 中新增字段，但是這樣會造成一個問題，就是舊的文檔並不會自動更新產生新的字段\n假設舊文檔有 name、nickname 這兩個字段，並且已經插入了兩筆數據\n{ \u0026#34;name\u0026#34;: \u0026#34;Jackson\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;jack\u0026#34; } { \u0026#34;name\u0026#34;: \u0026#34;Amy\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;a\u0026#34; } 此時插入一個新的字段 sex，並且插入一筆新的數據\n{ \u0026#34;name\u0026#34;: \u0026#34;NewYork\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;new\u0026#34;, \u0026#34;sex\u0026#34;: 1 } 執行 \u0026quot;match_all\u0026quot;: {} 查詢時，結果如下\n由於 Jackson 和 Amy 並沒有 sex 這個字段，所以雖然執行 match_all 搜索時能被搜出來，但是搜索 \u0026quot;term\u0026quot;: { \u0026quot;sex\u0026quot;: 1 } 的話，就搜索不出來\n{ \u0026#34;name\u0026#34;: \u0026#34;Jackson\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;jack\u0026#34; } { \u0026#34;name\u0026#34;: \u0026#34;Amy\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;a\u0026#34; } { \u0026#34;name\u0026#34;: \u0026#34;NewYork\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;new\u0026#34;, \u0026#34;sex\u0026#34;: 1 } 我們希望的狀況是，增加一個新字段，舊文檔應該也要設置一個預設值，像是 sex=0，而不是整個字段消失，如此在查詢時邏輯才不會有問題\n解決舊文檔沒有新字段這個問題主要有3種方法\n新建一個 index，使用 scroll + bulk insert 將數據從舊的索引批量插入到新索引中\n可以在 java 裡自定義舊文檔預設值的邏輯 新建一個 index，使用 ES 提供的 reindex API 將數據從舊的索引 reindex 到新索引\n需要使用 ES 的腳本 script，在 reindex 的請求裡定義舊文檔預設值的邏輯\nPOST _reindex { \u0026#34;source\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;mytest\u0026#34; }, \u0026#34;dest\u0026#34;: { \u0026#34;index\u0026#34;: \u0026#34;mytest2\u0026#34; }, \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: \u0026#34;if (ctx._source.new == null) {ctx._source.new = params.new}\u0026#34;, \u0026#34;params\u0026#34;: { \u0026#34;new\u0026#34;: \u0026#34;good\u0026#34; }, \u0026#34;lang\u0026","date":"2018-06-07","objectID":"742397e1b9a33b03c8681a1369dd22bd","title":"ElasticSearch - index mapping（5.x以上）","url":"https://kucw.io/blog/2018/6/elasticsearch-index-mapping/"}]